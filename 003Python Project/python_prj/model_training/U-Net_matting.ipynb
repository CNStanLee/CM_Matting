{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All code integrated: U-Net model, image preprocessing, model inference, and post-processing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image as PILImage\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# 设置设备\n",
    "#device = torch.device(\"cuda\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# U-Net Components\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# U-Net Model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes=1, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024 // 2)\n",
    "        self.up1 = Up(1024, 512 // 2, bilinear)\n",
    "        self.up2 = Up(512, 256 // 2, bilinear)\n",
    "        self.up3 = Up(256, 128 // 2, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss, need inputs between 0 and 1.\"\"\"\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        assert input.size() == target.size(), \"Input sizes must be equal.\"\n",
    "        assert input.dim() == 4, \"Input must be a 4D Tensor.\"\n",
    "        # Convert to float to ensure full precision during division\n",
    "        input = torch.sigmoid(input).float()\n",
    "        target = target.float()\n",
    "        \n",
    "        numerator = 2 * torch.sum(input * target, dim=(2, 3))\n",
    "        denominator = torch.sum(input + target, dim=(2, 3))\n",
    "        \n",
    "        dice_score = (numerator + self.smooth) / (denominator + self.smooth)\n",
    "        dice_loss = 1 - dice_score\n",
    "        return dice_loss.mean()\n",
    "\n",
    "# Load Pre-trained Model\n",
    "def load_pretrained_model(model_path='model_best.pth', device='cpu'):\n",
    "    # 确保UNet模型已经被定义在这个函数的外部\n",
    "    model = UNet(n_channels=3, n_classes=1)\n",
    "    # 加载预训练的权重，确保指定了map_location来适配你的设备\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    # 将模型移动到指定的设备\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# Image Preprocessing\n",
    "def preprocess_image(image_path):\n",
    "    # 直接从文件路径加载图片\n",
    "    image = PILImage.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((320, 320)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = transform(image)\n",
    "    input_tensor = input_tensor.unsqueeze(0)  # 增加一个批次维度\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inference Function\n",
    "def model_inference(model, input_tensor, device):\n",
    "    input_tensor = input_tensor.to(device)  # 确保输入张量在正确的设备上\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Do not compute gradient for inference\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "        output = model(input_tensor)  # Forward pass\n",
    "    return output\n",
    "\n",
    "def inference_and_get_alphamap(image_path, device, model_path='model_best.pth'):\n",
    "    model = load_pretrained_model(model_path, device)  # 加载模型和权重\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "\n",
    "    # 图像预处理\n",
    "    original_image = PILImage.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = preprocess_image(image_path)\n",
    "    input_tensor = input_tensor.to(device)  # 确保张量在正确的设备上\n",
    "\n",
    "    # 模型推理\n",
    "    with torch.no_grad():  # 不计算梯度\n",
    "        output = model(input_tensor)  # 前向传播\n",
    "    \n",
    "    # 将输出应用Sigmoid函数转换成概率，进一步得到Alpha Map\n",
    "    alpha_map = torch.sigmoid(output).squeeze().cpu().numpy()\n",
    "    alpha_map = np.clip((alpha_map * 255).astype(np.uint8), 0, 255)  # 转换为0-255的整数\n",
    "\n",
    "    # 调整Alpha Map的尺寸以匹配原始图像的尺寸\n",
    "    original_width, original_height = original_image.size\n",
    "    alpha_map_resized = cv2.resize(alpha_map, (original_width, original_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return alpha_map_resized\n",
    "\n",
    "\n",
    "# 保存Alpha Map的函数\n",
    "def save_alpha_map(alpha_map, save_path='alpha_map.png'):\n",
    "    PILImage.fromarray(alpha_map).save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Custom Dataset Class for Image Segmentation\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        image = PILImage.open(image_path).convert(\"RGB\")\n",
    "        mask = PILImage.open(mask_path).convert(\"L\")  # 加载为灰度图\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "            mask = transforms.ToTensor()(mask)\n",
    "            mask = torch.unsqueeze(mask, 0)  # 确保mask是单通道\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((320, 320)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Function to match image files with their corresponding mask files\n",
    "def get_matching_files(images_dir, masks_dir, image_ext=\".jpg\", mask_ext=\".png\"):\n",
    "    image_files = sorted(glob.glob(os.path.join(images_dir, f\"*{image_ext}\")))\n",
    "    mask_files = sorted(glob.glob(os.path.join(masks_dir, f\"*{mask_ext}\")))\n",
    "    \n",
    "    matched_images = []\n",
    "    matched_masks = []\n",
    "\n",
    "    image_basenames = {os.path.splitext(os.path.basename(image))[0]: image for image in image_files}\n",
    "    mask_basenames = {os.path.splitext(os.path.basename(mask))[0]: mask for mask in mask_files}\n",
    "\n",
    "    total_images = len(image_basenames)\n",
    "    print(f\"Total images found: {total_images}\")\n",
    "\n",
    "    for basename, image_path in image_basenames.items():\n",
    "        if basename in mask_basenames:\n",
    "            matched_images.append(image_path)\n",
    "            matched_masks.append(mask_basenames[basename])\n",
    "            # processed += 1\n",
    "            # print(f\"Processed {processed}/{total_images} images.\")\n",
    "    \n",
    "    print(f\"Total matched images: {len(matched_images)}\")\n",
    "    return matched_images, matched_masks\n",
    "\n",
    "# Training Function with Visualization\n",
    "def train_and_evaluate_with_saving(model, train_loader, val_loader, optimizer, criterion, device, epochs, model_path='model_best.pth'):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_loss = 0\n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Print the loss every 10 batches\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "                \n",
    "        # Average training loss\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Average validation loss\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Average Train Loss: {train_loss:.4f}, Average Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Save the model if it's the best so far\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f'Saved new best model at epoch {epoch+1} with Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Plot training and validation losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images found: 16184\n",
      "Total matched images: 16184\n",
      "Epoch [1/10], Batch [10/3237], Loss: 0.24892538785934448\n",
      "Epoch [1/10], Batch [20/3237], Loss: 0.38272804021835327\n",
      "Epoch [1/10], Batch [30/3237], Loss: 0.5762755274772644\n",
      "Epoch [1/10], Batch [40/3237], Loss: 0.4387231767177582\n",
      "Epoch [1/10], Batch [50/3237], Loss: 0.24416230618953705\n",
      "Epoch [1/10], Batch [60/3237], Loss: 0.34871843457221985\n",
      "Epoch [1/10], Batch [70/3237], Loss: 0.26286765933036804\n",
      "Epoch [1/10], Batch [80/3237], Loss: 0.4446031153202057\n",
      "Epoch [1/10], Batch [90/3237], Loss: 0.26755619049072266\n",
      "Epoch [1/10], Batch [100/3237], Loss: 0.26144835352897644\n",
      "Epoch [1/10], Batch [110/3237], Loss: 0.24987636506557465\n",
      "Epoch [1/10], Batch [120/3237], Loss: 0.31937098503112793\n",
      "Epoch [1/10], Batch [130/3237], Loss: 0.49647364020347595\n",
      "Epoch [1/10], Batch [140/3237], Loss: 0.49276259541511536\n",
      "Epoch [1/10], Batch [150/3237], Loss: 0.2959432601928711\n",
      "Epoch [1/10], Batch [160/3237], Loss: 0.25589701533317566\n",
      "Epoch [1/10], Batch [170/3237], Loss: 0.3904447853565216\n",
      "Epoch [1/10], Batch [180/3237], Loss: 0.3087094724178314\n",
      "Epoch [1/10], Batch [190/3237], Loss: 0.31559446454048157\n",
      "Epoch [1/10], Batch [200/3237], Loss: 0.5425508618354797\n",
      "Epoch [1/10], Batch [210/3237], Loss: 0.2119264155626297\n",
      "Epoch [1/10], Batch [220/3237], Loss: 0.3002750277519226\n",
      "Epoch [1/10], Batch [230/3237], Loss: 0.37974169850349426\n",
      "Epoch [1/10], Batch [240/3237], Loss: 0.4076331853866577\n",
      "Epoch [1/10], Batch [250/3237], Loss: 0.3706745505332947\n",
      "Epoch [1/10], Batch [260/3237], Loss: 0.20537273585796356\n",
      "Epoch [1/10], Batch [270/3237], Loss: 0.28381067514419556\n",
      "Epoch [1/10], Batch [280/3237], Loss: 0.2248973846435547\n",
      "Epoch [1/10], Batch [290/3237], Loss: 0.37703800201416016\n",
      "Epoch [1/10], Batch [300/3237], Loss: 0.320780485868454\n",
      "Epoch [1/10], Batch [310/3237], Loss: 0.37686893343925476\n",
      "Epoch [1/10], Batch [320/3237], Loss: 0.35777145624160767\n",
      "Epoch [1/10], Batch [330/3237], Loss: 0.29921355843544006\n",
      "Epoch [1/10], Batch [340/3237], Loss: 0.20335978269577026\n",
      "Epoch [1/10], Batch [350/3237], Loss: 0.3862245976924896\n",
      "Epoch [1/10], Batch [360/3237], Loss: 0.2594793140888214\n",
      "Epoch [1/10], Batch [370/3237], Loss: 0.25563955307006836\n",
      "Epoch [1/10], Batch [380/3237], Loss: 0.20445118844509125\n",
      "Epoch [1/10], Batch [390/3237], Loss: 0.2893725037574768\n",
      "Epoch [1/10], Batch [400/3237], Loss: 0.31588640809059143\n",
      "Epoch [1/10], Batch [410/3237], Loss: 0.5652227401733398\n",
      "Epoch [1/10], Batch [420/3237], Loss: 0.29940634965896606\n",
      "Epoch [1/10], Batch [430/3237], Loss: 0.2617250084877014\n",
      "Epoch [1/10], Batch [440/3237], Loss: 0.3485303521156311\n",
      "Epoch [1/10], Batch [450/3237], Loss: 0.22674697637557983\n",
      "Epoch [1/10], Batch [460/3237], Loss: 0.6943879723548889\n",
      "Epoch [1/10], Batch [470/3237], Loss: 0.3329554796218872\n",
      "Epoch [1/10], Batch [480/3237], Loss: 0.3186853229999542\n",
      "Epoch [1/10], Batch [490/3237], Loss: 0.24085287749767303\n",
      "Epoch [1/10], Batch [500/3237], Loss: 0.17975132167339325\n",
      "Epoch [1/10], Batch [510/3237], Loss: 0.4592348039150238\n",
      "Epoch [1/10], Batch [520/3237], Loss: 0.2234601527452469\n",
      "Epoch [1/10], Batch [530/3237], Loss: 0.2677760124206543\n",
      "Epoch [1/10], Batch [540/3237], Loss: 0.3526712656021118\n",
      "Epoch [1/10], Batch [550/3237], Loss: 0.380522757768631\n",
      "Epoch [1/10], Batch [560/3237], Loss: 0.35845211148262024\n",
      "Epoch [1/10], Batch [570/3237], Loss: 0.3419058620929718\n",
      "Epoch [1/10], Batch [580/3237], Loss: 0.335590660572052\n",
      "Epoch [1/10], Batch [590/3237], Loss: 0.32099851965904236\n",
      "Epoch [1/10], Batch [600/3237], Loss: 0.35376909375190735\n",
      "Epoch [1/10], Batch [610/3237], Loss: 0.44272860884666443\n",
      "Epoch [1/10], Batch [620/3237], Loss: 0.2221093475818634\n",
      "Epoch [1/10], Batch [630/3237], Loss: 0.23337693512439728\n",
      "Epoch [1/10], Batch [640/3237], Loss: 0.3270362913608551\n",
      "Epoch [1/10], Batch [650/3237], Loss: 0.2029877007007599\n",
      "Epoch [1/10], Batch [660/3237], Loss: 0.2635355591773987\n",
      "Epoch [1/10], Batch [670/3237], Loss: 0.2564801871776581\n",
      "Epoch [1/10], Batch [680/3237], Loss: 0.41871875524520874\n",
      "Epoch [1/10], Batch [690/3237], Loss: 0.2505418658256531\n",
      "Epoch [1/10], Batch [700/3237], Loss: 0.30969852209091187\n",
      "Epoch [1/10], Batch [710/3237], Loss: 0.12661224603652954\n",
      "Epoch [1/10], Batch [720/3237], Loss: 0.2550404965877533\n",
      "Epoch [1/10], Batch [730/3237], Loss: 0.2852187156677246\n",
      "Epoch [1/10], Batch [740/3237], Loss: 0.2920774221420288\n",
      "Epoch [1/10], Batch [750/3237], Loss: 0.20092372596263885\n",
      "Epoch [1/10], Batch [760/3237], Loss: 0.4063553214073181\n",
      "Epoch [1/10], Batch [770/3237], Loss: 0.22343558073043823\n",
      "Epoch [1/10], Batch [780/3237], Loss: 0.31458941102027893\n",
      "Epoch [1/10], Batch [790/3237], Loss: 0.3992335796356201\n",
      "Epoch [1/10], Batch [800/3237], Loss: 0.2722412943840027\n",
      "Epoch [1/10], Batch [810/3237], Loss: 0.34339943528175354\n",
      "Epoch [1/10], Batch [820/3237], Loss: 0.18448960781097412\n",
      "Epoch [1/10], Batch [830/3237], Loss: 0.31739678978919983\n",
      "Epoch [1/10], Batch [840/3237], Loss: 0.5577684640884399\n",
      "Epoch [1/10], Batch [850/3237], Loss: 0.21350520849227905\n",
      "Epoch [1/10], Batch [860/3237], Loss: 0.23520001769065857\n",
      "Epoch [1/10], Batch [870/3237], Loss: 0.1914467215538025\n",
      "Epoch [1/10], Batch [880/3237], Loss: 0.280910462141037\n",
      "Epoch [1/10], Batch [890/3237], Loss: 0.22528719902038574\n",
      "Epoch [1/10], Batch [900/3237], Loss: 0.3505481779575348\n",
      "Epoch [1/10], Batch [910/3237], Loss: 0.3406970500946045\n",
      "Epoch [1/10], Batch [920/3237], Loss: 0.30147039890289307\n",
      "Epoch [1/10], Batch [930/3237], Loss: 0.5407969951629639\n",
      "Epoch [1/10], Batch [940/3237], Loss: 0.21508139371871948\n",
      "Epoch [1/10], Batch [950/3237], Loss: 0.2822197377681732\n",
      "Epoch [1/10], Batch [960/3237], Loss: 0.235315203666687\n",
      "Epoch [1/10], Batch [970/3237], Loss: 0.14883305132389069\n",
      "Epoch [1/10], Batch [980/3237], Loss: 0.454110711812973\n",
      "Epoch [1/10], Batch [990/3237], Loss: 0.3809054493904114\n",
      "Epoch [1/10], Batch [1000/3237], Loss: 0.34206289052963257\n",
      "Epoch [1/10], Batch [1010/3237], Loss: 0.26995909214019775\n",
      "Epoch [1/10], Batch [1020/3237], Loss: 0.23749759793281555\n",
      "Epoch [1/10], Batch [1030/3237], Loss: 0.1904265582561493\n",
      "Epoch [1/10], Batch [1040/3237], Loss: 0.5447044968605042\n",
      "Epoch [1/10], Batch [1050/3237], Loss: 0.37961775064468384\n",
      "Epoch [1/10], Batch [1060/3237], Loss: 0.37655189633369446\n",
      "Epoch [1/10], Batch [1070/3237], Loss: 0.15064726769924164\n",
      "Epoch [1/10], Batch [1080/3237], Loss: 0.22054332494735718\n",
      "Epoch [1/10], Batch [1090/3237], Loss: 0.3486599624156952\n",
      "Epoch [1/10], Batch [1100/3237], Loss: 0.4314640462398529\n",
      "Epoch [1/10], Batch [1110/3237], Loss: 0.18219108879566193\n",
      "Epoch [1/10], Batch [1120/3237], Loss: 0.7482532858848572\n",
      "Epoch [1/10], Batch [1130/3237], Loss: 0.39972877502441406\n",
      "Epoch [1/10], Batch [1140/3237], Loss: 0.37009552121162415\n",
      "Epoch [1/10], Batch [1150/3237], Loss: 0.18621759116649628\n",
      "Epoch [1/10], Batch [1160/3237], Loss: 0.3108017146587372\n",
      "Epoch [1/10], Batch [1170/3237], Loss: 0.16354671120643616\n",
      "Epoch [1/10], Batch [1180/3237], Loss: 0.37230560183525085\n",
      "Epoch [1/10], Batch [1190/3237], Loss: 0.1980208158493042\n",
      "Epoch [1/10], Batch [1200/3237], Loss: 0.19884634017944336\n",
      "Epoch [1/10], Batch [1210/3237], Loss: 0.19037076830863953\n",
      "Epoch [1/10], Batch [1220/3237], Loss: 0.22080127894878387\n",
      "Epoch [1/10], Batch [1230/3237], Loss: 0.24136340618133545\n",
      "Epoch [1/10], Batch [1240/3237], Loss: 0.20914095640182495\n",
      "Epoch [1/10], Batch [1250/3237], Loss: 0.23671619594097137\n",
      "Epoch [1/10], Batch [1260/3237], Loss: 0.18734632432460785\n",
      "Epoch [1/10], Batch [1270/3237], Loss: 0.21985818445682526\n",
      "Epoch [1/10], Batch [1280/3237], Loss: 0.2503603398799896\n",
      "Epoch [1/10], Batch [1290/3237], Loss: 0.18636462092399597\n",
      "Epoch [1/10], Batch [1300/3237], Loss: 0.3472658097743988\n",
      "Epoch [1/10], Batch [1310/3237], Loss: 0.23162582516670227\n",
      "Epoch [1/10], Batch [1320/3237], Loss: 0.3049580454826355\n",
      "Epoch [1/10], Batch [1330/3237], Loss: 0.2739158272743225\n",
      "Epoch [1/10], Batch [1340/3237], Loss: 0.25396496057510376\n",
      "Epoch [1/10], Batch [1350/3237], Loss: 0.1834290325641632\n",
      "Epoch [1/10], Batch [1360/3237], Loss: 0.24481536448001862\n",
      "Epoch [1/10], Batch [1370/3237], Loss: 0.2074887752532959\n",
      "Epoch [1/10], Batch [1380/3237], Loss: 0.3084968030452728\n",
      "Epoch [1/10], Batch [1390/3237], Loss: 0.2924000024795532\n",
      "Epoch [1/10], Batch [1400/3237], Loss: 0.184433251619339\n",
      "Epoch [1/10], Batch [1410/3237], Loss: 0.21776080131530762\n",
      "Epoch [1/10], Batch [1420/3237], Loss: 0.20519420504570007\n",
      "Epoch [1/10], Batch [1430/3237], Loss: 0.27341601252555847\n",
      "Epoch [1/10], Batch [1440/3237], Loss: 0.28173720836639404\n",
      "Epoch [1/10], Batch [1450/3237], Loss: 0.26583683490753174\n",
      "Epoch [1/10], Batch [1460/3237], Loss: 0.26431262493133545\n",
      "Epoch [1/10], Batch [1470/3237], Loss: 0.2510644197463989\n",
      "Epoch [1/10], Batch [1480/3237], Loss: 0.2609351873397827\n",
      "Epoch [1/10], Batch [1490/3237], Loss: 0.1817338466644287\n",
      "Epoch [1/10], Batch [1500/3237], Loss: 0.32063326239585876\n",
      "Epoch [1/10], Batch [1510/3237], Loss: 0.14960381388664246\n",
      "Epoch [1/10], Batch [1520/3237], Loss: 0.2302711457014084\n",
      "Epoch [1/10], Batch [1530/3237], Loss: 0.35441628098487854\n",
      "Epoch [1/10], Batch [1540/3237], Loss: 0.30186548829078674\n",
      "Epoch [1/10], Batch [1550/3237], Loss: 0.191877081990242\n",
      "Epoch [1/10], Batch [1560/3237], Loss: 0.23225024342536926\n",
      "Epoch [1/10], Batch [1570/3237], Loss: 0.31635716557502747\n",
      "Epoch [1/10], Batch [1580/3237], Loss: 0.1665104329586029\n",
      "Epoch [1/10], Batch [1590/3237], Loss: 0.4697768986225128\n",
      "Epoch [1/10], Batch [1600/3237], Loss: 0.2566952109336853\n",
      "Epoch [1/10], Batch [1610/3237], Loss: 0.2700020670890808\n",
      "Epoch [1/10], Batch [1620/3237], Loss: 0.22308465838432312\n",
      "Epoch [1/10], Batch [1630/3237], Loss: 0.17556647956371307\n",
      "Epoch [1/10], Batch [1640/3237], Loss: 0.3315839171409607\n",
      "Epoch [1/10], Batch [1650/3237], Loss: 0.42192110419273376\n",
      "Epoch [1/10], Batch [1660/3237], Loss: 0.2908278703689575\n",
      "Epoch [1/10], Batch [1670/3237], Loss: 0.26793089509010315\n",
      "Epoch [1/10], Batch [1680/3237], Loss: 0.30774855613708496\n",
      "Epoch [1/10], Batch [1690/3237], Loss: 0.2379419058561325\n",
      "Epoch [1/10], Batch [1700/3237], Loss: 0.2277798056602478\n",
      "Epoch [1/10], Batch [1710/3237], Loss: 0.31014516949653625\n",
      "Epoch [1/10], Batch [1720/3237], Loss: 0.3060885965824127\n",
      "Epoch [1/10], Batch [1730/3237], Loss: 0.24802355468273163\n",
      "Epoch [1/10], Batch [1740/3237], Loss: 0.25693175196647644\n",
      "Epoch [1/10], Batch [1750/3237], Loss: 0.3327254354953766\n",
      "Epoch [1/10], Batch [1760/3237], Loss: 0.19091227650642395\n",
      "Epoch [1/10], Batch [1770/3237], Loss: 0.29368388652801514\n",
      "Epoch [1/10], Batch [1780/3237], Loss: 0.17995135486125946\n",
      "Epoch [1/10], Batch [1790/3237], Loss: 0.1745787411928177\n",
      "Epoch [1/10], Batch [1800/3237], Loss: 0.16594630479812622\n",
      "Epoch [1/10], Batch [1810/3237], Loss: 0.2417856901884079\n",
      "Epoch [1/10], Batch [1820/3237], Loss: 0.23925456404685974\n",
      "Epoch [1/10], Batch [1830/3237], Loss: 0.5753854513168335\n",
      "Epoch [1/10], Batch [1840/3237], Loss: 0.22575831413269043\n",
      "Epoch [1/10], Batch [1850/3237], Loss: 0.21655791997909546\n",
      "Epoch [1/10], Batch [1860/3237], Loss: 0.2848784625530243\n",
      "Epoch [1/10], Batch [1870/3237], Loss: 0.34600749611854553\n",
      "Epoch [1/10], Batch [1880/3237], Loss: 0.17250904440879822\n",
      "Epoch [1/10], Batch [1890/3237], Loss: 0.12141667306423187\n",
      "Epoch [1/10], Batch [1900/3237], Loss: 0.2540414333343506\n",
      "Epoch [1/10], Batch [1910/3237], Loss: 0.2337457537651062\n",
      "Epoch [1/10], Batch [1920/3237], Loss: 0.23113660514354706\n",
      "Epoch [1/10], Batch [1930/3237], Loss: 0.21335364878177643\n",
      "Epoch [1/10], Batch [1940/3237], Loss: 0.2333158254623413\n",
      "Epoch [1/10], Batch [1950/3237], Loss: 0.24024856090545654\n",
      "Epoch [1/10], Batch [1960/3237], Loss: 0.2979409992694855\n",
      "Epoch [1/10], Batch [1970/3237], Loss: 0.3163151443004608\n",
      "Epoch [1/10], Batch [1980/3237], Loss: 0.38391417264938354\n",
      "Epoch [1/10], Batch [1990/3237], Loss: 0.31359726190567017\n",
      "Epoch [1/10], Batch [2000/3237], Loss: 0.20777609944343567\n",
      "Epoch [1/10], Batch [2010/3237], Loss: 0.323210209608078\n",
      "Epoch [1/10], Batch [2020/3237], Loss: 0.24689540266990662\n",
      "Epoch [1/10], Batch [2030/3237], Loss: 0.3243177831172943\n",
      "Epoch [1/10], Batch [2040/3237], Loss: 0.246242493391037\n",
      "Epoch [1/10], Batch [2050/3237], Loss: 0.20571406185626984\n",
      "Epoch [1/10], Batch [2060/3237], Loss: 0.27192041277885437\n",
      "Epoch [1/10], Batch [2070/3237], Loss: 0.17906326055526733\n",
      "Epoch [1/10], Batch [2080/3237], Loss: 0.26505613327026367\n",
      "Epoch [1/10], Batch [2090/3237], Loss: 0.17180627584457397\n",
      "Epoch [1/10], Batch [2100/3237], Loss: 0.17562688887119293\n",
      "Epoch [1/10], Batch [2110/3237], Loss: 0.20441831648349762\n",
      "Epoch [1/10], Batch [2120/3237], Loss: 0.35798758268356323\n",
      "Epoch [1/10], Batch [2130/3237], Loss: 0.14125362038612366\n",
      "Epoch [1/10], Batch [2140/3237], Loss: 0.19374074041843414\n",
      "Epoch [1/10], Batch [2150/3237], Loss: 0.2109987437725067\n",
      "Epoch [1/10], Batch [2160/3237], Loss: 0.24723029136657715\n",
      "Epoch [1/10], Batch [2170/3237], Loss: 0.25311487913131714\n",
      "Epoch [1/10], Batch [2180/3237], Loss: 0.2622624337673187\n",
      "Epoch [1/10], Batch [2190/3237], Loss: 0.46170303225517273\n",
      "Epoch [1/10], Batch [2200/3237], Loss: 0.27752813696861267\n",
      "Epoch [1/10], Batch [2210/3237], Loss: 0.1744459867477417\n",
      "Epoch [1/10], Batch [2220/3237], Loss: 0.3216768205165863\n",
      "Epoch [1/10], Batch [2230/3237], Loss: 0.31723856925964355\n",
      "Epoch [1/10], Batch [2240/3237], Loss: 0.4138389527797699\n",
      "Epoch [1/10], Batch [2250/3237], Loss: 0.32773634791374207\n",
      "Epoch [1/10], Batch [2260/3237], Loss: 0.1536920964717865\n",
      "Epoch [1/10], Batch [2270/3237], Loss: 0.1388729065656662\n",
      "Epoch [1/10], Batch [2280/3237], Loss: 0.28955402970314026\n",
      "Epoch [1/10], Batch [2290/3237], Loss: 0.20553112030029297\n",
      "Epoch [1/10], Batch [2300/3237], Loss: 0.2068479061126709\n",
      "Epoch [1/10], Batch [2310/3237], Loss: 0.1714850217103958\n",
      "Epoch [1/10], Batch [2320/3237], Loss: 0.22482383251190186\n",
      "Epoch [1/10], Batch [2330/3237], Loss: 0.1599942296743393\n",
      "Epoch [1/10], Batch [2340/3237], Loss: 0.23500415682792664\n",
      "Epoch [1/10], Batch [2350/3237], Loss: 0.3190862536430359\n",
      "Epoch [1/10], Batch [2360/3237], Loss: 0.4327635169029236\n",
      "Epoch [1/10], Batch [2370/3237], Loss: 0.23039771616458893\n",
      "Epoch [1/10], Batch [2380/3237], Loss: 0.33321189880371094\n",
      "Epoch [1/10], Batch [2390/3237], Loss: 0.16704429686069489\n",
      "Epoch [1/10], Batch [2400/3237], Loss: 0.3021303415298462\n",
      "Epoch [1/10], Batch [2410/3237], Loss: 0.15199503302574158\n",
      "Epoch [1/10], Batch [2420/3237], Loss: 0.23685820400714874\n",
      "Epoch [1/10], Batch [2430/3237], Loss: 0.2012852430343628\n",
      "Epoch [1/10], Batch [2440/3237], Loss: 0.22668413817882538\n",
      "Epoch [1/10], Batch [2450/3237], Loss: 0.2197062224149704\n",
      "Epoch [1/10], Batch [2460/3237], Loss: 0.18929938971996307\n",
      "Epoch [1/10], Batch [2470/3237], Loss: 0.14505727589130402\n",
      "Epoch [1/10], Batch [2480/3237], Loss: 0.23248107731342316\n",
      "Epoch [1/10], Batch [2490/3237], Loss: 0.277944415807724\n",
      "Epoch [1/10], Batch [2500/3237], Loss: 0.23016826808452606\n",
      "Epoch [1/10], Batch [2510/3237], Loss: 0.2968052625656128\n",
      "Epoch [1/10], Batch [2520/3237], Loss: 0.15943892300128937\n",
      "Epoch [1/10], Batch [2530/3237], Loss: 0.26973819732666016\n",
      "Epoch [1/10], Batch [2540/3237], Loss: 0.21771258115768433\n",
      "Epoch [1/10], Batch [2550/3237], Loss: 0.21457791328430176\n",
      "Epoch [1/10], Batch [2560/3237], Loss: 0.2457006722688675\n",
      "Epoch [1/10], Batch [2570/3237], Loss: 0.1718224734067917\n",
      "Epoch [1/10], Batch [2580/3237], Loss: 0.22720105946063995\n",
      "Epoch [1/10], Batch [2590/3237], Loss: 0.20129722356796265\n",
      "Epoch [1/10], Batch [2600/3237], Loss: 0.22976092994213104\n",
      "Epoch [1/10], Batch [2610/3237], Loss: 0.4108152687549591\n",
      "Epoch [1/10], Batch [2620/3237], Loss: 0.305121511220932\n",
      "Epoch [1/10], Batch [2630/3237], Loss: 0.36450743675231934\n",
      "Epoch [1/10], Batch [2640/3237], Loss: 0.14570075273513794\n",
      "Epoch [1/10], Batch [2650/3237], Loss: 0.15664780139923096\n",
      "Epoch [1/10], Batch [2660/3237], Loss: 0.12182857096195221\n",
      "Epoch [1/10], Batch [2670/3237], Loss: 0.28121280670166016\n",
      "Epoch [1/10], Batch [2680/3237], Loss: 0.1431218981742859\n",
      "Epoch [1/10], Batch [2690/3237], Loss: 0.2908630073070526\n",
      "Epoch [1/10], Batch [2700/3237], Loss: 0.3073119521141052\n",
      "Epoch [1/10], Batch [2710/3237], Loss: 0.3141586184501648\n",
      "Epoch [1/10], Batch [2720/3237], Loss: 0.2806647717952728\n",
      "Epoch [1/10], Batch [2730/3237], Loss: 0.2648799419403076\n",
      "Epoch [1/10], Batch [2740/3237], Loss: 0.17721614241600037\n",
      "Epoch [1/10], Batch [2750/3237], Loss: 0.24864275753498077\n",
      "Epoch [1/10], Batch [2760/3237], Loss: 0.23606853187084198\n",
      "Epoch [1/10], Batch [2770/3237], Loss: 0.23436343669891357\n",
      "Epoch [1/10], Batch [2780/3237], Loss: 0.21333912014961243\n",
      "Epoch [1/10], Batch [2790/3237], Loss: 0.16171979904174805\n",
      "Epoch [1/10], Batch [2800/3237], Loss: 0.14695483446121216\n",
      "Epoch [1/10], Batch [2810/3237], Loss: 0.2874244153499603\n",
      "Epoch [1/10], Batch [2820/3237], Loss: 0.16271229088306427\n",
      "Epoch [1/10], Batch [2830/3237], Loss: 0.20758526027202606\n",
      "Epoch [1/10], Batch [2840/3237], Loss: 0.7934158444404602\n",
      "Epoch [1/10], Batch [2850/3237], Loss: 0.3594512939453125\n",
      "Epoch [1/10], Batch [2860/3237], Loss: 0.10665766149759293\n",
      "Epoch [1/10], Batch [2870/3237], Loss: 0.316866397857666\n",
      "Epoch [1/10], Batch [2880/3237], Loss: 0.31789398193359375\n",
      "Epoch [1/10], Batch [2890/3237], Loss: 0.2540223002433777\n",
      "Epoch [1/10], Batch [2900/3237], Loss: 0.3605897128582001\n",
      "Epoch [1/10], Batch [2910/3237], Loss: 0.2501629889011383\n",
      "Epoch [1/10], Batch [2920/3237], Loss: 0.4523330330848694\n",
      "Epoch [1/10], Batch [2930/3237], Loss: 0.16671603918075562\n",
      "Epoch [1/10], Batch [2940/3237], Loss: 0.27221542596817017\n",
      "Epoch [1/10], Batch [2950/3237], Loss: 0.19659243524074554\n",
      "Epoch [1/10], Batch [2960/3237], Loss: 0.2761628329753876\n",
      "Epoch [1/10], Batch [2970/3237], Loss: 0.21667712926864624\n",
      "Epoch [1/10], Batch [2980/3237], Loss: 0.21089690923690796\n",
      "Epoch [1/10], Batch [2990/3237], Loss: 0.3554559648036957\n",
      "Epoch [1/10], Batch [3000/3237], Loss: 0.13228389620780945\n",
      "Epoch [1/10], Batch [3010/3237], Loss: 0.2073991745710373\n",
      "Epoch [1/10], Batch [3020/3237], Loss: 0.1866760551929474\n",
      "Epoch [1/10], Batch [3030/3237], Loss: 0.09559442102909088\n",
      "Epoch [1/10], Batch [3040/3237], Loss: 0.10877389460802078\n",
      "Epoch [1/10], Batch [3050/3237], Loss: 0.2433338463306427\n",
      "Epoch [1/10], Batch [3060/3237], Loss: 0.20531803369522095\n",
      "Epoch [1/10], Batch [3070/3237], Loss: 0.24241222441196442\n",
      "Epoch [1/10], Batch [3080/3237], Loss: 0.10580983757972717\n",
      "Epoch [1/10], Batch [3090/3237], Loss: 0.14172199368476868\n",
      "Epoch [1/10], Batch [3100/3237], Loss: 0.2024722695350647\n",
      "Epoch [1/10], Batch [3110/3237], Loss: 0.3120685815811157\n",
      "Epoch [1/10], Batch [3120/3237], Loss: 0.26845306158065796\n",
      "Epoch [1/10], Batch [3130/3237], Loss: 0.23239360749721527\n",
      "Epoch [1/10], Batch [3140/3237], Loss: 0.19496285915374756\n",
      "Epoch [1/10], Batch [3150/3237], Loss: 0.4110136330127716\n",
      "Epoch [1/10], Batch [3160/3237], Loss: 0.44525137543678284\n",
      "Epoch [1/10], Batch [3170/3237], Loss: 0.20938389003276825\n",
      "Epoch [1/10], Batch [3180/3237], Loss: 0.25602251291275024\n",
      "Epoch [1/10], Batch [3190/3237], Loss: 0.11853179335594177\n",
      "Epoch [1/10], Batch [3200/3237], Loss: 0.2078063189983368\n",
      "Epoch [1/10], Batch [3210/3237], Loss: 0.27287495136260986\n",
      "Epoch [1/10], Batch [3220/3237], Loss: 0.17360110580921173\n",
      "Epoch [1/10], Batch [3230/3237], Loss: 0.18474708497524261\n",
      "Epoch 1, Average Train Loss: 0.2765, Average Val Loss: 0.2386\n",
      "Saved new best model at epoch 1 with Val Loss: 0.2386\n",
      "Epoch [2/10], Batch [10/3237], Loss: 0.2819899320602417\n",
      "Epoch [2/10], Batch [20/3237], Loss: 0.2285454273223877\n",
      "Epoch [2/10], Batch [30/3237], Loss: 0.21546931564807892\n",
      "Epoch [2/10], Batch [40/3237], Loss: 0.13869498670101166\n",
      "Epoch [2/10], Batch [50/3237], Loss: 0.16269950568675995\n",
      "Epoch [2/10], Batch [60/3237], Loss: 0.2885674238204956\n",
      "Epoch [2/10], Batch [70/3237], Loss: 0.20859012007713318\n",
      "Epoch [2/10], Batch [80/3237], Loss: 0.1404961496591568\n",
      "Epoch [2/10], Batch [90/3237], Loss: 0.1364915817975998\n",
      "Epoch [2/10], Batch [100/3237], Loss: 0.2948671579360962\n",
      "Epoch [2/10], Batch [110/3237], Loss: 0.22338412702083588\n",
      "Epoch [2/10], Batch [120/3237], Loss: 0.36538854241371155\n",
      "Epoch [2/10], Batch [130/3237], Loss: 0.1946832686662674\n",
      "Epoch [2/10], Batch [140/3237], Loss: 0.1815933883190155\n",
      "Epoch [2/10], Batch [150/3237], Loss: 0.14058980345726013\n",
      "Epoch [2/10], Batch [160/3237], Loss: 0.31609007716178894\n",
      "Epoch [2/10], Batch [170/3237], Loss: 0.14525508880615234\n",
      "Epoch [2/10], Batch [180/3237], Loss: 0.2709972560405731\n",
      "Epoch [2/10], Batch [190/3237], Loss: 0.4356739819049835\n",
      "Epoch [2/10], Batch [200/3237], Loss: 0.3586549162864685\n",
      "Epoch [2/10], Batch [210/3237], Loss: 0.32079118490219116\n",
      "Epoch [2/10], Batch [220/3237], Loss: 0.143533393740654\n",
      "Epoch [2/10], Batch [230/3237], Loss: 0.3465435802936554\n",
      "Epoch [2/10], Batch [240/3237], Loss: 0.17546504735946655\n",
      "Epoch [2/10], Batch [250/3237], Loss: 0.19025157392024994\n",
      "Epoch [2/10], Batch [260/3237], Loss: 0.18705031275749207\n",
      "Epoch [2/10], Batch [270/3237], Loss: 0.29021942615509033\n",
      "Epoch [2/10], Batch [280/3237], Loss: 0.156199112534523\n",
      "Epoch [2/10], Batch [290/3237], Loss: 0.2409157007932663\n",
      "Epoch [2/10], Batch [300/3237], Loss: 0.40370553731918335\n",
      "Epoch [2/10], Batch [310/3237], Loss: 0.2024257928133011\n",
      "Epoch [2/10], Batch [320/3237], Loss: 0.20009009540081024\n",
      "Epoch [2/10], Batch [330/3237], Loss: 0.23360413312911987\n",
      "Epoch [2/10], Batch [340/3237], Loss: 0.3360285460948944\n",
      "Epoch [2/10], Batch [350/3237], Loss: 0.73431396484375\n",
      "Epoch [2/10], Batch [360/3237], Loss: 0.29331138730049133\n",
      "Epoch [2/10], Batch [370/3237], Loss: 0.19411158561706543\n",
      "Epoch [2/10], Batch [380/3237], Loss: 0.22469264268875122\n",
      "Epoch [2/10], Batch [390/3237], Loss: 0.19443297386169434\n",
      "Epoch [2/10], Batch [400/3237], Loss: 0.2643757164478302\n",
      "Epoch [2/10], Batch [410/3237], Loss: 0.18387623131275177\n",
      "Epoch [2/10], Batch [420/3237], Loss: 0.14803174138069153\n",
      "Epoch [2/10], Batch [430/3237], Loss: 0.15676864981651306\n",
      "Epoch [2/10], Batch [440/3237], Loss: 0.3126934766769409\n",
      "Epoch [2/10], Batch [450/3237], Loss: 0.1676202416419983\n",
      "Epoch [2/10], Batch [460/3237], Loss: 0.39030882716178894\n",
      "Epoch [2/10], Batch [470/3237], Loss: 0.22510753571987152\n",
      "Epoch [2/10], Batch [480/3237], Loss: 0.1565435230731964\n",
      "Epoch [2/10], Batch [490/3237], Loss: 0.2957529127597809\n",
      "Epoch [2/10], Batch [500/3237], Loss: 0.21796409785747528\n",
      "Epoch [2/10], Batch [510/3237], Loss: 0.26190754771232605\n",
      "Epoch [2/10], Batch [520/3237], Loss: 0.1419675052165985\n",
      "Epoch [2/10], Batch [530/3237], Loss: 0.2993721663951874\n",
      "Epoch [2/10], Batch [540/3237], Loss: 0.23884418606758118\n",
      "Epoch [2/10], Batch [550/3237], Loss: 0.13128957152366638\n",
      "Epoch [2/10], Batch [560/3237], Loss: 0.37937238812446594\n",
      "Epoch [2/10], Batch [570/3237], Loss: 0.11198568344116211\n",
      "Epoch [2/10], Batch [580/3237], Loss: 0.24341508746147156\n",
      "Epoch [2/10], Batch [590/3237], Loss: 0.12565681338310242\n",
      "Epoch [2/10], Batch [600/3237], Loss: 0.13818776607513428\n",
      "Epoch [2/10], Batch [610/3237], Loss: 0.16271835565567017\n",
      "Epoch [2/10], Batch [620/3237], Loss: 0.24920974671840668\n",
      "Epoch [2/10], Batch [630/3237], Loss: 0.3297642469406128\n",
      "Epoch [2/10], Batch [640/3237], Loss: 0.29190316796302795\n",
      "Epoch [2/10], Batch [650/3237], Loss: 0.1634177714586258\n",
      "Epoch [2/10], Batch [660/3237], Loss: 0.2909230887889862\n",
      "Epoch [2/10], Batch [670/3237], Loss: 0.22305472195148468\n",
      "Epoch [2/10], Batch [680/3237], Loss: 0.13399869203567505\n",
      "Epoch [2/10], Batch [690/3237], Loss: 0.3403222858905792\n",
      "Epoch [2/10], Batch [700/3237], Loss: 0.21284453570842743\n",
      "Epoch [2/10], Batch [710/3237], Loss: 0.33720511198043823\n",
      "Epoch [2/10], Batch [720/3237], Loss: 0.28543633222579956\n",
      "Epoch [2/10], Batch [730/3237], Loss: 0.17071759700775146\n",
      "Epoch [2/10], Batch [740/3237], Loss: 0.19704513251781464\n",
      "Epoch [2/10], Batch [750/3237], Loss: 0.20046305656433105\n",
      "Epoch [2/10], Batch [760/3237], Loss: 0.3024865686893463\n",
      "Epoch [2/10], Batch [770/3237], Loss: 0.16077035665512085\n",
      "Epoch [2/10], Batch [780/3237], Loss: 0.18632598221302032\n",
      "Epoch [2/10], Batch [790/3237], Loss: 0.4394526481628418\n",
      "Epoch [2/10], Batch [800/3237], Loss: 0.32110923528671265\n",
      "Epoch [2/10], Batch [810/3237], Loss: 0.1692521572113037\n",
      "Epoch [2/10], Batch [820/3237], Loss: 0.17959174513816833\n",
      "Epoch [2/10], Batch [830/3237], Loss: 0.2135223150253296\n",
      "Epoch [2/10], Batch [840/3237], Loss: 0.14866074919700623\n",
      "Epoch [2/10], Batch [850/3237], Loss: 0.1045260801911354\n",
      "Epoch [2/10], Batch [860/3237], Loss: 0.11934676021337509\n",
      "Epoch [2/10], Batch [870/3237], Loss: 0.2966880202293396\n",
      "Epoch [2/10], Batch [880/3237], Loss: 0.2809426486492157\n",
      "Epoch [2/10], Batch [890/3237], Loss: 0.3795456290245056\n",
      "Epoch [2/10], Batch [900/3237], Loss: 0.1523653119802475\n",
      "Epoch [2/10], Batch [910/3237], Loss: 0.2572554349899292\n",
      "Epoch [2/10], Batch [920/3237], Loss: 0.22875234484672546\n",
      "Epoch [2/10], Batch [930/3237], Loss: 0.18621650338172913\n",
      "Epoch [2/10], Batch [940/3237], Loss: 0.23706041276454926\n",
      "Epoch [2/10], Batch [950/3237], Loss: 0.2681628465652466\n",
      "Epoch [2/10], Batch [960/3237], Loss: 0.17997422814369202\n",
      "Epoch [2/10], Batch [970/3237], Loss: 0.38567817211151123\n",
      "Epoch [2/10], Batch [980/3237], Loss: 0.22209931910037994\n",
      "Epoch [2/10], Batch [990/3237], Loss: 0.3272976577281952\n",
      "Epoch [2/10], Batch [1000/3237], Loss: 0.22968678176403046\n",
      "Epoch [2/10], Batch [1010/3237], Loss: 0.13701923191547394\n",
      "Epoch [2/10], Batch [1020/3237], Loss: 0.2156076431274414\n",
      "Epoch [2/10], Batch [1030/3237], Loss: 0.3056504428386688\n",
      "Epoch [2/10], Batch [1040/3237], Loss: 0.1591826230287552\n",
      "Epoch [2/10], Batch [1050/3237], Loss: 0.22230304777622223\n",
      "Epoch [2/10], Batch [1060/3237], Loss: 0.13526447117328644\n",
      "Epoch [2/10], Batch [1070/3237], Loss: 0.15156875550746918\n",
      "Epoch [2/10], Batch [1080/3237], Loss: 0.2554680109024048\n",
      "Epoch [2/10], Batch [1090/3237], Loss: 0.1916598081588745\n",
      "Epoch [2/10], Batch [1100/3237], Loss: 0.6026034951210022\n",
      "Epoch [2/10], Batch [1110/3237], Loss: 0.15227225422859192\n",
      "Epoch [2/10], Batch [1120/3237], Loss: 0.22816762328147888\n",
      "Epoch [2/10], Batch [1130/3237], Loss: 0.33912888169288635\n",
      "Epoch [2/10], Batch [1140/3237], Loss: 0.2237604707479477\n",
      "Epoch [2/10], Batch [1150/3237], Loss: 0.21439921855926514\n",
      "Epoch [2/10], Batch [1160/3237], Loss: 0.40648385882377625\n",
      "Epoch [2/10], Batch [1170/3237], Loss: 0.25728750228881836\n",
      "Epoch [2/10], Batch [1180/3237], Loss: 0.28704166412353516\n",
      "Epoch [2/10], Batch [1190/3237], Loss: 0.24934257566928864\n",
      "Epoch [2/10], Batch [1200/3237], Loss: 0.2287357598543167\n",
      "Epoch [2/10], Batch [1210/3237], Loss: 0.187810018658638\n",
      "Epoch [2/10], Batch [1220/3237], Loss: 0.32274332642555237\n",
      "Epoch [2/10], Batch [1230/3237], Loss: 0.19546808302402496\n",
      "Epoch [2/10], Batch [1240/3237], Loss: 0.19205649197101593\n",
      "Epoch [2/10], Batch [1250/3237], Loss: 0.3031875491142273\n",
      "Epoch [2/10], Batch [1260/3237], Loss: 0.3285362124443054\n",
      "Epoch [2/10], Batch [1270/3237], Loss: 0.21598802506923676\n",
      "Epoch [2/10], Batch [1280/3237], Loss: 0.14397500455379486\n",
      "Epoch [2/10], Batch [1290/3237], Loss: 0.14376087486743927\n",
      "Epoch [2/10], Batch [1300/3237], Loss: 0.11349523067474365\n",
      "Epoch [2/10], Batch [1310/3237], Loss: 0.2315853387117386\n",
      "Epoch [2/10], Batch [1320/3237], Loss: 0.17566756904125214\n",
      "Epoch [2/10], Batch [1330/3237], Loss: 0.24353241920471191\n",
      "Epoch [2/10], Batch [1340/3237], Loss: 0.15562006831169128\n",
      "Epoch [2/10], Batch [1350/3237], Loss: 0.12361674755811691\n",
      "Epoch [2/10], Batch [1360/3237], Loss: 0.13848966360092163\n",
      "Epoch [2/10], Batch [1370/3237], Loss: 0.2965947091579437\n",
      "Epoch [2/10], Batch [1380/3237], Loss: 0.15425226092338562\n",
      "Epoch [2/10], Batch [1390/3237], Loss: 0.33459338545799255\n",
      "Epoch [2/10], Batch [1400/3237], Loss: 0.26749905943870544\n",
      "Epoch [2/10], Batch [1410/3237], Loss: 0.3339748978614807\n",
      "Epoch [2/10], Batch [1420/3237], Loss: 0.35391464829444885\n",
      "Epoch [2/10], Batch [1430/3237], Loss: 0.22460952401161194\n",
      "Epoch [2/10], Batch [1440/3237], Loss: 0.11039430648088455\n",
      "Epoch [2/10], Batch [1450/3237], Loss: 0.29297029972076416\n",
      "Epoch [2/10], Batch [1460/3237], Loss: 0.18122179806232452\n",
      "Epoch [2/10], Batch [1470/3237], Loss: 0.43056467175483704\n",
      "Epoch [2/10], Batch [1480/3237], Loss: 0.13532625138759613\n",
      "Epoch [2/10], Batch [1490/3237], Loss: 0.33517688512802124\n",
      "Epoch [2/10], Batch [1500/3237], Loss: 0.17376521229743958\n",
      "Epoch [2/10], Batch [1510/3237], Loss: 0.31569576263427734\n",
      "Epoch [2/10], Batch [1520/3237], Loss: 0.26180094480514526\n",
      "Epoch [2/10], Batch [1530/3237], Loss: 0.17921951413154602\n",
      "Epoch [2/10], Batch [1540/3237], Loss: 0.15662433207035065\n",
      "Epoch [2/10], Batch [1550/3237], Loss: 0.2333461344242096\n",
      "Epoch [2/10], Batch [1560/3237], Loss: 0.1613868623971939\n",
      "Epoch [2/10], Batch [1570/3237], Loss: 0.25651270151138306\n",
      "Epoch [2/10], Batch [1580/3237], Loss: 0.368305504322052\n",
      "Epoch [2/10], Batch [1590/3237], Loss: 0.3097832500934601\n",
      "Epoch [2/10], Batch [1600/3237], Loss: 0.3001401722431183\n",
      "Epoch [2/10], Batch [1610/3237], Loss: 0.28006526827812195\n",
      "Epoch [2/10], Batch [1620/3237], Loss: 0.1381223201751709\n",
      "Epoch [2/10], Batch [1630/3237], Loss: 0.24381589889526367\n",
      "Epoch [2/10], Batch [1640/3237], Loss: 0.2746637165546417\n",
      "Epoch [2/10], Batch [1650/3237], Loss: 0.28046852350234985\n",
      "Epoch [2/10], Batch [1660/3237], Loss: 0.22447215020656586\n",
      "Epoch [2/10], Batch [1670/3237], Loss: 0.1370535045862198\n",
      "Epoch [2/10], Batch [1680/3237], Loss: 0.25271254777908325\n",
      "Epoch [2/10], Batch [1690/3237], Loss: 0.22876358032226562\n",
      "Epoch [2/10], Batch [1700/3237], Loss: 0.12843114137649536\n",
      "Epoch [2/10], Batch [1710/3237], Loss: 0.0995839312672615\n",
      "Epoch [2/10], Batch [1720/3237], Loss: 0.16275864839553833\n",
      "Epoch [2/10], Batch [1730/3237], Loss: 0.27188950777053833\n",
      "Epoch [2/10], Batch [1740/3237], Loss: 0.13663649559020996\n",
      "Epoch [2/10], Batch [1750/3237], Loss: 0.1323549449443817\n",
      "Epoch [2/10], Batch [1760/3237], Loss: 0.21893535554409027\n",
      "Epoch [2/10], Batch [1770/3237], Loss: 0.1615370661020279\n",
      "Epoch [2/10], Batch [1780/3237], Loss: 0.36822745203971863\n",
      "Epoch [2/10], Batch [1790/3237], Loss: 0.4285520911216736\n",
      "Epoch [2/10], Batch [1800/3237], Loss: 0.29416194558143616\n",
      "Epoch [2/10], Batch [1810/3237], Loss: 0.18292376399040222\n",
      "Epoch [2/10], Batch [1820/3237], Loss: 0.17736393213272095\n",
      "Epoch [2/10], Batch [1830/3237], Loss: 0.10211098194122314\n",
      "Epoch [2/10], Batch [1840/3237], Loss: 0.24415969848632812\n",
      "Epoch [2/10], Batch [1850/3237], Loss: 0.34999677538871765\n",
      "Epoch [2/10], Batch [1860/3237], Loss: 0.19633619487285614\n",
      "Epoch [2/10], Batch [1870/3237], Loss: 0.1992182433605194\n",
      "Epoch [2/10], Batch [1880/3237], Loss: 0.18351508677005768\n",
      "Epoch [2/10], Batch [1890/3237], Loss: 0.22370639443397522\n",
      "Epoch [2/10], Batch [1900/3237], Loss: 0.4088824391365051\n",
      "Epoch [2/10], Batch [1910/3237], Loss: 0.1985224485397339\n",
      "Epoch [2/10], Batch [1920/3237], Loss: 0.16827619075775146\n",
      "Epoch [2/10], Batch [1930/3237], Loss: 0.2524016797542572\n",
      "Epoch [2/10], Batch [1940/3237], Loss: 0.09672002494335175\n",
      "Epoch [2/10], Batch [1950/3237], Loss: 0.48378947377204895\n",
      "Epoch [2/10], Batch [1960/3237], Loss: 0.2100379467010498\n",
      "Epoch [2/10], Batch [1970/3237], Loss: 0.1195041611790657\n",
      "Epoch [2/10], Batch [1980/3237], Loss: 0.3276755213737488\n",
      "Epoch [2/10], Batch [1990/3237], Loss: 0.14718833565711975\n",
      "Epoch [2/10], Batch [2000/3237], Loss: 0.3159741461277008\n",
      "Epoch [2/10], Batch [2010/3237], Loss: 0.3210095167160034\n",
      "Epoch [2/10], Batch [2020/3237], Loss: 0.19528397917747498\n",
      "Epoch [2/10], Batch [2030/3237], Loss: 0.36377668380737305\n",
      "Epoch [2/10], Batch [2040/3237], Loss: 0.19285209476947784\n",
      "Epoch [2/10], Batch [2050/3237], Loss: 0.7831643223762512\n",
      "Epoch [2/10], Batch [2060/3237], Loss: 0.26103702187538147\n",
      "Epoch [2/10], Batch [2070/3237], Loss: 0.3300708830356598\n",
      "Epoch [2/10], Batch [2080/3237], Loss: 0.2957709729671478\n",
      "Epoch [2/10], Batch [2090/3237], Loss: 0.1266406774520874\n",
      "Epoch [2/10], Batch [2100/3237], Loss: 0.4101068079471588\n",
      "Epoch [2/10], Batch [2110/3237], Loss: 0.20719094574451447\n",
      "Epoch [2/10], Batch [2120/3237], Loss: 0.214128777384758\n",
      "Epoch [2/10], Batch [2130/3237], Loss: 0.17892783880233765\n",
      "Epoch [2/10], Batch [2140/3237], Loss: 0.10449410229921341\n",
      "Epoch [2/10], Batch [2150/3237], Loss: 0.177671879529953\n",
      "Epoch [2/10], Batch [2160/3237], Loss: 0.2545577883720398\n",
      "Epoch [2/10], Batch [2170/3237], Loss: 0.17363031208515167\n",
      "Epoch [2/10], Batch [2180/3237], Loss: 0.10046631842851639\n",
      "Epoch [2/10], Batch [2190/3237], Loss: 0.21318534016609192\n",
      "Epoch [2/10], Batch [2200/3237], Loss: 0.16949553787708282\n",
      "Epoch [2/10], Batch [2210/3237], Loss: 0.13344863057136536\n",
      "Epoch [2/10], Batch [2220/3237], Loss: 0.1698589324951172\n",
      "Epoch [2/10], Batch [2230/3237], Loss: 0.1932942122220993\n",
      "Epoch [2/10], Batch [2240/3237], Loss: 0.31298956274986267\n",
      "Epoch [2/10], Batch [2250/3237], Loss: 0.25906509160995483\n",
      "Epoch [2/10], Batch [2260/3237], Loss: 0.18580791354179382\n",
      "Epoch [2/10], Batch [2270/3237], Loss: 0.19026295840740204\n",
      "Epoch [2/10], Batch [2280/3237], Loss: 0.16951632499694824\n",
      "Epoch [2/10], Batch [2290/3237], Loss: 0.1968802958726883\n",
      "Epoch [2/10], Batch [2300/3237], Loss: 0.1535862386226654\n",
      "Epoch [2/10], Batch [2310/3237], Loss: 0.20159080624580383\n",
      "Epoch [2/10], Batch [2320/3237], Loss: 0.10791970789432526\n",
      "Epoch [2/10], Batch [2330/3237], Loss: 0.2058255672454834\n",
      "Epoch [2/10], Batch [2340/3237], Loss: 0.11319301277399063\n",
      "Epoch [2/10], Batch [2350/3237], Loss: 0.2328474372625351\n",
      "Epoch [2/10], Batch [2360/3237], Loss: 0.15093912184238434\n",
      "Epoch [2/10], Batch [2370/3237], Loss: 0.23808643221855164\n",
      "Epoch [2/10], Batch [2380/3237], Loss: 0.20062947273254395\n",
      "Epoch [2/10], Batch [2390/3237], Loss: 0.18908408284187317\n",
      "Epoch [2/10], Batch [2400/3237], Loss: 0.12738116085529327\n",
      "Epoch [2/10], Batch [2410/3237], Loss: 0.12616103887557983\n",
      "Epoch [2/10], Batch [2420/3237], Loss: 0.13492748141288757\n",
      "Epoch [2/10], Batch [2430/3237], Loss: 0.19079367816448212\n",
      "Epoch [2/10], Batch [2440/3237], Loss: 0.19270694255828857\n",
      "Epoch [2/10], Batch [2450/3237], Loss: 0.18030346930027008\n",
      "Epoch [2/10], Batch [2460/3237], Loss: 0.38506513833999634\n",
      "Epoch [2/10], Batch [2470/3237], Loss: 0.2679428160190582\n",
      "Epoch [2/10], Batch [2480/3237], Loss: 0.18765850365161896\n",
      "Epoch [2/10], Batch [2490/3237], Loss: 0.25041308999061584\n",
      "Epoch [2/10], Batch [2500/3237], Loss: 0.3109838664531708\n",
      "Epoch [2/10], Batch [2510/3237], Loss: 0.45640143752098083\n",
      "Epoch [2/10], Batch [2520/3237], Loss: 0.3975515365600586\n",
      "Epoch [2/10], Batch [2530/3237], Loss: 0.15478460490703583\n",
      "Epoch [2/10], Batch [2540/3237], Loss: 0.13395963609218597\n",
      "Epoch [2/10], Batch [2550/3237], Loss: 0.20316550135612488\n",
      "Epoch [2/10], Batch [2560/3237], Loss: 0.14660204946994781\n",
      "Epoch [2/10], Batch [2570/3237], Loss: 0.1983949989080429\n",
      "Epoch [2/10], Batch [2580/3237], Loss: 0.12819452583789825\n",
      "Epoch [2/10], Batch [2590/3237], Loss: 0.2781371772289276\n",
      "Epoch [2/10], Batch [2600/3237], Loss: 0.20550590753555298\n",
      "Epoch [2/10], Batch [2610/3237], Loss: 0.29128673672676086\n",
      "Epoch [2/10], Batch [2620/3237], Loss: 0.19846177101135254\n",
      "Epoch [2/10], Batch [2630/3237], Loss: 0.17717483639717102\n",
      "Epoch [2/10], Batch [2640/3237], Loss: 0.1874854564666748\n",
      "Epoch [2/10], Batch [2650/3237], Loss: 0.0691603571176529\n",
      "Epoch [2/10], Batch [2660/3237], Loss: 0.21164461970329285\n",
      "Epoch [2/10], Batch [2670/3237], Loss: 0.2592402994632721\n",
      "Epoch [2/10], Batch [2680/3237], Loss: 0.15732717514038086\n",
      "Epoch [2/10], Batch [2690/3237], Loss: 0.14819373190402985\n",
      "Epoch [2/10], Batch [2700/3237], Loss: 0.26516193151474\n",
      "Epoch [2/10], Batch [2710/3237], Loss: 0.31723833084106445\n",
      "Epoch [2/10], Batch [2720/3237], Loss: 0.2664761245250702\n",
      "Epoch [2/10], Batch [2730/3237], Loss: 0.20741663873195648\n",
      "Epoch [2/10], Batch [2740/3237], Loss: 0.13817854225635529\n",
      "Epoch [2/10], Batch [2750/3237], Loss: 0.23866526782512665\n",
      "Epoch [2/10], Batch [2760/3237], Loss: 0.2380606234073639\n",
      "Epoch [2/10], Batch [2770/3237], Loss: 0.14068607985973358\n",
      "Epoch [2/10], Batch [2780/3237], Loss: 0.2019771933555603\n",
      "Epoch [2/10], Batch [2790/3237], Loss: 0.2118385285139084\n",
      "Epoch [2/10], Batch [2800/3237], Loss: 0.19664351642131805\n",
      "Epoch [2/10], Batch [2810/3237], Loss: 0.3284391760826111\n",
      "Epoch [2/10], Batch [2820/3237], Loss: 0.26402756571769714\n",
      "Epoch [2/10], Batch [2830/3237], Loss: 0.17686639726161957\n",
      "Epoch [2/10], Batch [2840/3237], Loss: 0.16302688419818878\n",
      "Epoch [2/10], Batch [2850/3237], Loss: 0.3542920649051666\n",
      "Epoch [2/10], Batch [2860/3237], Loss: 0.22948066890239716\n",
      "Epoch [2/10], Batch [2870/3237], Loss: 0.13954982161521912\n",
      "Epoch [2/10], Batch [2880/3237], Loss: 0.2692396938800812\n",
      "Epoch [2/10], Batch [2890/3237], Loss: 0.23257698118686676\n",
      "Epoch [2/10], Batch [2900/3237], Loss: 0.15267030894756317\n",
      "Epoch [2/10], Batch [2910/3237], Loss: 0.16430038213729858\n",
      "Epoch [2/10], Batch [2920/3237], Loss: 0.2802327573299408\n",
      "Epoch [2/10], Batch [2930/3237], Loss: 0.1518016755580902\n",
      "Epoch [2/10], Batch [2940/3237], Loss: 0.17590706050395966\n",
      "Epoch [2/10], Batch [2950/3237], Loss: 0.24797402322292328\n",
      "Epoch [2/10], Batch [2960/3237], Loss: 0.08774509280920029\n",
      "Epoch [2/10], Batch [2970/3237], Loss: 0.3659295439720154\n",
      "Epoch [2/10], Batch [2980/3237], Loss: 0.19414874911308289\n",
      "Epoch [2/10], Batch [2990/3237], Loss: 0.22016479074954987\n",
      "Epoch [2/10], Batch [3000/3237], Loss: 0.3034043610095978\n",
      "Epoch [2/10], Batch [3010/3237], Loss: 0.11193990707397461\n",
      "Epoch [2/10], Batch [3020/3237], Loss: 0.1208328977227211\n",
      "Epoch [2/10], Batch [3030/3237], Loss: 0.1752021312713623\n",
      "Epoch [2/10], Batch [3040/3237], Loss: 0.09643489867448807\n",
      "Epoch [2/10], Batch [3050/3237], Loss: 0.14685235917568207\n",
      "Epoch [2/10], Batch [3060/3237], Loss: 0.29754266142845154\n",
      "Epoch [2/10], Batch [3070/3237], Loss: 0.2890971601009369\n",
      "Epoch [2/10], Batch [3080/3237], Loss: 0.36078742146492004\n",
      "Epoch [2/10], Batch [3090/3237], Loss: 0.2834996283054352\n",
      "Epoch [2/10], Batch [3100/3237], Loss: 0.09089259803295135\n",
      "Epoch [2/10], Batch [3110/3237], Loss: 0.3026413023471832\n",
      "Epoch [2/10], Batch [3120/3237], Loss: 0.23979972302913666\n",
      "Epoch [2/10], Batch [3130/3237], Loss: 0.18280012905597687\n",
      "Epoch [2/10], Batch [3140/3237], Loss: 0.40628767013549805\n",
      "Epoch [2/10], Batch [3150/3237], Loss: 0.18413077294826508\n",
      "Epoch [2/10], Batch [3160/3237], Loss: 0.15989360213279724\n",
      "Epoch [2/10], Batch [3170/3237], Loss: 0.3182201683521271\n",
      "Epoch [2/10], Batch [3180/3237], Loss: 0.260505348443985\n",
      "Epoch [2/10], Batch [3190/3237], Loss: 0.20895667374134064\n",
      "Epoch [2/10], Batch [3200/3237], Loss: 0.2670281231403351\n",
      "Epoch [2/10], Batch [3210/3237], Loss: 0.29364341497421265\n",
      "Epoch [2/10], Batch [3220/3237], Loss: 0.21031294763088226\n",
      "Epoch [2/10], Batch [3230/3237], Loss: 0.28294944763183594\n",
      "Epoch 2, Average Train Loss: 0.2253, Average Val Loss: 0.2150\n",
      "Saved new best model at epoch 2 with Val Loss: 0.2150\n",
      "Epoch [3/10], Batch [10/3237], Loss: 0.1463708132505417\n",
      "Epoch [3/10], Batch [20/3237], Loss: 0.19563506543636322\n",
      "Epoch [3/10], Batch [30/3237], Loss: 0.49340304732322693\n",
      "Epoch [3/10], Batch [40/3237], Loss: 0.3076949119567871\n",
      "Epoch [3/10], Batch [50/3237], Loss: 0.18453270196914673\n",
      "Epoch [3/10], Batch [60/3237], Loss: 0.18972140550613403\n",
      "Epoch [3/10], Batch [70/3237], Loss: 0.20826148986816406\n",
      "Epoch [3/10], Batch [80/3237], Loss: 0.11790041625499725\n",
      "Epoch [3/10], Batch [90/3237], Loss: 0.11481549590826035\n",
      "Epoch [3/10], Batch [100/3237], Loss: 0.2479497492313385\n",
      "Epoch [3/10], Batch [110/3237], Loss: 0.1399526745080948\n",
      "Epoch [3/10], Batch [120/3237], Loss: 0.22004859149456024\n",
      "Epoch [3/10], Batch [130/3237], Loss: 0.25795596837997437\n",
      "Epoch [3/10], Batch [140/3237], Loss: 0.1921711564064026\n",
      "Epoch [3/10], Batch [150/3237], Loss: 0.13493670523166656\n",
      "Epoch [3/10], Batch [160/3237], Loss: 0.3135526776313782\n",
      "Epoch [3/10], Batch [170/3237], Loss: 0.15877334773540497\n",
      "Epoch [3/10], Batch [180/3237], Loss: 0.33693864941596985\n",
      "Epoch [3/10], Batch [190/3237], Loss: 0.160478875041008\n",
      "Epoch [3/10], Batch [200/3237], Loss: 0.3655790388584137\n",
      "Epoch [3/10], Batch [210/3237], Loss: 0.18816325068473816\n",
      "Epoch [3/10], Batch [220/3237], Loss: 0.21133677661418915\n",
      "Epoch [3/10], Batch [230/3237], Loss: 0.16516739130020142\n",
      "Epoch [3/10], Batch [240/3237], Loss: 0.4409734308719635\n",
      "Epoch [3/10], Batch [250/3237], Loss: 0.27090469002723694\n",
      "Epoch [3/10], Batch [260/3237], Loss: 0.11339365690946579\n",
      "Epoch [3/10], Batch [270/3237], Loss: 0.23070509731769562\n",
      "Epoch [3/10], Batch [280/3237], Loss: 0.27697068452835083\n",
      "Epoch [3/10], Batch [290/3237], Loss: 0.15831083059310913\n",
      "Epoch [3/10], Batch [300/3237], Loss: 0.11978140473365784\n",
      "Epoch [3/10], Batch [310/3237], Loss: 0.12534873187541962\n",
      "Epoch [3/10], Batch [320/3237], Loss: 0.2597905099391937\n",
      "Epoch [3/10], Batch [330/3237], Loss: 0.407475084066391\n",
      "Epoch [3/10], Batch [340/3237], Loss: 0.21717023849487305\n",
      "Epoch [3/10], Batch [350/3237], Loss: 0.3294539451599121\n",
      "Epoch [3/10], Batch [360/3237], Loss: 0.19918552041053772\n",
      "Epoch [3/10], Batch [370/3237], Loss: 0.2644672393798828\n",
      "Epoch [3/10], Batch [380/3237], Loss: 0.4030129909515381\n",
      "Epoch [3/10], Batch [390/3237], Loss: 0.2275022715330124\n",
      "Epoch [3/10], Batch [400/3237], Loss: 0.20803222060203552\n",
      "Epoch [3/10], Batch [410/3237], Loss: 0.24688641726970673\n",
      "Epoch [3/10], Batch [420/3237], Loss: 0.30102595686912537\n",
      "Epoch [3/10], Batch [430/3237], Loss: 0.27206605672836304\n",
      "Epoch [3/10], Batch [440/3237], Loss: 0.15673719346523285\n",
      "Epoch [3/10], Batch [450/3237], Loss: 0.11529328674077988\n",
      "Epoch [3/10], Batch [460/3237], Loss: 0.4611285924911499\n",
      "Epoch [3/10], Batch [470/3237], Loss: 0.18391364812850952\n",
      "Epoch [3/10], Batch [480/3237], Loss: 0.12635007500648499\n",
      "Epoch [3/10], Batch [490/3237], Loss: 0.30142974853515625\n",
      "Epoch [3/10], Batch [500/3237], Loss: 0.2981351315975189\n",
      "Epoch [3/10], Batch [510/3237], Loss: 0.1325739622116089\n",
      "Epoch [3/10], Batch [520/3237], Loss: 0.1854163259267807\n",
      "Epoch [3/10], Batch [530/3237], Loss: 0.12015029788017273\n",
      "Epoch [3/10], Batch [540/3237], Loss: 0.19510841369628906\n",
      "Epoch [3/10], Batch [550/3237], Loss: 0.16516828536987305\n",
      "Epoch [3/10], Batch [560/3237], Loss: 0.09732901304960251\n",
      "Epoch [3/10], Batch [570/3237], Loss: 0.22448323667049408\n",
      "Epoch [3/10], Batch [580/3237], Loss: 0.2752402126789093\n",
      "Epoch [3/10], Batch [590/3237], Loss: 0.17481227219104767\n",
      "Epoch [3/10], Batch [600/3237], Loss: 0.20092254877090454\n",
      "Epoch [3/10], Batch [610/3237], Loss: 0.20327815413475037\n",
      "Epoch [3/10], Batch [620/3237], Loss: 0.2570193409919739\n",
      "Epoch [3/10], Batch [630/3237], Loss: 0.28160738945007324\n",
      "Epoch [3/10], Batch [640/3237], Loss: 0.3339694142341614\n",
      "Epoch [3/10], Batch [650/3237], Loss: 0.20618972182273865\n",
      "Epoch [3/10], Batch [660/3237], Loss: 0.0946732684969902\n",
      "Epoch [3/10], Batch [670/3237], Loss: 0.1285138577222824\n",
      "Epoch [3/10], Batch [680/3237], Loss: 0.19331206381320953\n",
      "Epoch [3/10], Batch [690/3237], Loss: 0.16515037417411804\n",
      "Epoch [3/10], Batch [700/3237], Loss: 0.23383693397045135\n",
      "Epoch [3/10], Batch [710/3237], Loss: 0.10857664048671722\n",
      "Epoch [3/10], Batch [720/3237], Loss: 0.24708884954452515\n",
      "Epoch [3/10], Batch [730/3237], Loss: 0.12809716165065765\n",
      "Epoch [3/10], Batch [740/3237], Loss: 0.2687995731830597\n",
      "Epoch [3/10], Batch [750/3237], Loss: 0.08110305666923523\n",
      "Epoch [3/10], Batch [760/3237], Loss: 0.2270744889974594\n",
      "Epoch [3/10], Batch [770/3237], Loss: 0.22456485033035278\n",
      "Epoch [3/10], Batch [780/3237], Loss: 0.2856570780277252\n",
      "Epoch [3/10], Batch [790/3237], Loss: 0.34496769309043884\n",
      "Epoch [3/10], Batch [800/3237], Loss: 0.21390193700790405\n",
      "Epoch [3/10], Batch [810/3237], Loss: 0.23150303959846497\n",
      "Epoch [3/10], Batch [820/3237], Loss: 0.42555394768714905\n",
      "Epoch [3/10], Batch [830/3237], Loss: 0.14066150784492493\n",
      "Epoch [3/10], Batch [840/3237], Loss: 0.14673374593257904\n",
      "Epoch [3/10], Batch [850/3237], Loss: 0.24843014776706696\n",
      "Epoch [3/10], Batch [860/3237], Loss: 0.16137851774692535\n",
      "Epoch [3/10], Batch [870/3237], Loss: 0.14449693262577057\n",
      "Epoch [3/10], Batch [880/3237], Loss: 0.12932562828063965\n",
      "Epoch [3/10], Batch [890/3237], Loss: 0.15327993035316467\n",
      "Epoch [3/10], Batch [900/3237], Loss: 0.3351123332977295\n",
      "Epoch [3/10], Batch [910/3237], Loss: 0.2878335416316986\n",
      "Epoch [3/10], Batch [920/3237], Loss: 0.18150310218334198\n",
      "Epoch [3/10], Batch [930/3237], Loss: 0.08739123493432999\n",
      "Epoch [3/10], Batch [940/3237], Loss: 0.11881467700004578\n",
      "Epoch [3/10], Batch [950/3237], Loss: 0.14749456942081451\n",
      "Epoch [3/10], Batch [960/3237], Loss: 0.19157786667346954\n",
      "Epoch [3/10], Batch [970/3237], Loss: 0.09989091753959656\n",
      "Epoch [3/10], Batch [980/3237], Loss: 0.14958178997039795\n",
      "Epoch [3/10], Batch [990/3237], Loss: 0.11842454969882965\n",
      "Epoch [3/10], Batch [1000/3237], Loss: 0.14462876319885254\n",
      "Epoch [3/10], Batch [1010/3237], Loss: 0.30168861150741577\n",
      "Epoch [3/10], Batch [1020/3237], Loss: 0.1285470575094223\n",
      "Epoch [3/10], Batch [1030/3237], Loss: 0.1890232115983963\n",
      "Epoch [3/10], Batch [1040/3237], Loss: 0.10264430940151215\n",
      "Epoch [3/10], Batch [1050/3237], Loss: 0.22066253423690796\n",
      "Epoch [3/10], Batch [1060/3237], Loss: 0.16453681886196136\n",
      "Epoch [3/10], Batch [1070/3237], Loss: 0.21980398893356323\n",
      "Epoch [3/10], Batch [1080/3237], Loss: 0.1017785519361496\n",
      "Epoch [3/10], Batch [1090/3237], Loss: 0.14735634624958038\n",
      "Epoch [3/10], Batch [1100/3237], Loss: 0.24941465258598328\n",
      "Epoch [3/10], Batch [1110/3237], Loss: 0.4725878834724426\n",
      "Epoch [3/10], Batch [1120/3237], Loss: 0.23619712889194489\n",
      "Epoch [3/10], Batch [1130/3237], Loss: 0.209461972117424\n",
      "Epoch [3/10], Batch [1140/3237], Loss: 0.43923789262771606\n",
      "Epoch [3/10], Batch [1150/3237], Loss: 0.06455077230930328\n",
      "Epoch [3/10], Batch [1160/3237], Loss: 0.12667325139045715\n",
      "Epoch [3/10], Batch [1170/3237], Loss: 0.19208765029907227\n",
      "Epoch [3/10], Batch [1180/3237], Loss: 0.13804687559604645\n",
      "Epoch [3/10], Batch [1190/3237], Loss: 0.18178969621658325\n",
      "Epoch [3/10], Batch [1200/3237], Loss: 0.10392309725284576\n",
      "Epoch [3/10], Batch [1210/3237], Loss: 0.26817354559898376\n",
      "Epoch [3/10], Batch [1220/3237], Loss: 0.12331269681453705\n",
      "Epoch [3/10], Batch [1230/3237], Loss: 0.20458351075649261\n",
      "Epoch [3/10], Batch [1240/3237], Loss: 0.25590574741363525\n",
      "Epoch [3/10], Batch [1250/3237], Loss: 0.3133227527141571\n",
      "Epoch [3/10], Batch [1260/3237], Loss: 0.07507789134979248\n",
      "Epoch [3/10], Batch [1270/3237], Loss: 0.11757176369428635\n",
      "Epoch [3/10], Batch [1280/3237], Loss: 0.34319713711738586\n",
      "Epoch [3/10], Batch [1290/3237], Loss: 0.1617196649312973\n",
      "Epoch [3/10], Batch [1300/3237], Loss: 0.2222016155719757\n",
      "Epoch [3/10], Batch [1310/3237], Loss: 0.20023885369300842\n",
      "Epoch [3/10], Batch [1320/3237], Loss: 0.25146734714508057\n",
      "Epoch [3/10], Batch [1330/3237], Loss: 0.1763772964477539\n",
      "Epoch [3/10], Batch [1340/3237], Loss: 0.15207523107528687\n",
      "Epoch [3/10], Batch [1350/3237], Loss: 0.20025497674942017\n",
      "Epoch [3/10], Batch [1360/3237], Loss: 0.20307223498821259\n",
      "Epoch [3/10], Batch [1370/3237], Loss: 0.12461601197719574\n",
      "Epoch [3/10], Batch [1380/3237], Loss: 0.09181474149227142\n",
      "Epoch [3/10], Batch [1390/3237], Loss: 0.11221975833177567\n",
      "Epoch [3/10], Batch [1400/3237], Loss: 0.3306737542152405\n",
      "Epoch [3/10], Batch [1410/3237], Loss: 0.17847371101379395\n",
      "Epoch [3/10], Batch [1420/3237], Loss: 0.270349383354187\n",
      "Epoch [3/10], Batch [1430/3237], Loss: 0.40241706371307373\n",
      "Epoch [3/10], Batch [1440/3237], Loss: 0.266976922750473\n",
      "Epoch [3/10], Batch [1450/3237], Loss: 0.21960704028606415\n",
      "Epoch [3/10], Batch [1460/3237], Loss: 0.2153460681438446\n",
      "Epoch [3/10], Batch [1470/3237], Loss: 0.09711112827062607\n",
      "Epoch [3/10], Batch [1480/3237], Loss: 0.11367249488830566\n",
      "Epoch [3/10], Batch [1490/3237], Loss: 0.22822558879852295\n",
      "Epoch [3/10], Batch [1500/3237], Loss: 0.10212446749210358\n",
      "Epoch [3/10], Batch [1510/3237], Loss: 0.17085056006908417\n",
      "Epoch [3/10], Batch [1520/3237], Loss: 0.1581231951713562\n",
      "Epoch [3/10], Batch [1530/3237], Loss: 0.09325001388788223\n",
      "Epoch [3/10], Batch [1540/3237], Loss: 0.2687389552593231\n",
      "Epoch [3/10], Batch [1550/3237], Loss: 0.1622937172651291\n",
      "Epoch [3/10], Batch [1560/3237], Loss: 0.39556747674942017\n",
      "Epoch [3/10], Batch [1570/3237], Loss: 0.1397070735692978\n",
      "Epoch [3/10], Batch [1580/3237], Loss: 0.12155841290950775\n",
      "Epoch [3/10], Batch [1590/3237], Loss: 0.11280959844589233\n",
      "Epoch [3/10], Batch [1600/3237], Loss: 0.2831272780895233\n",
      "Epoch [3/10], Batch [1610/3237], Loss: 0.24442251026630402\n",
      "Epoch [3/10], Batch [1620/3237], Loss: 0.2690742313861847\n",
      "Epoch [3/10], Batch [1630/3237], Loss: 0.1815384477376938\n",
      "Epoch [3/10], Batch [1640/3237], Loss: 0.14380818605422974\n",
      "Epoch [3/10], Batch [1650/3237], Loss: 0.10674560070037842\n",
      "Epoch [3/10], Batch [1660/3237], Loss: 0.2802143096923828\n",
      "Epoch [3/10], Batch [1670/3237], Loss: 0.15768735110759735\n",
      "Epoch [3/10], Batch [1680/3237], Loss: 0.11371210962533951\n",
      "Epoch [3/10], Batch [1690/3237], Loss: 0.12407338619232178\n",
      "Epoch [3/10], Batch [1700/3237], Loss: 0.2116689682006836\n",
      "Epoch [3/10], Batch [1710/3237], Loss: 0.13036160171031952\n",
      "Epoch [3/10], Batch [1720/3237], Loss: 0.14961935579776764\n",
      "Epoch [3/10], Batch [1730/3237], Loss: 0.2923063337802887\n",
      "Epoch [3/10], Batch [1740/3237], Loss: 0.1974259912967682\n",
      "Epoch [3/10], Batch [1750/3237], Loss: 0.12714044749736786\n",
      "Epoch [3/10], Batch [1760/3237], Loss: 0.23359538614749908\n",
      "Epoch [3/10], Batch [1770/3237], Loss: 0.32690420746803284\n",
      "Epoch [3/10], Batch [1780/3237], Loss: 0.10431372374296188\n",
      "Epoch [3/10], Batch [1790/3237], Loss: 0.20201458036899567\n",
      "Epoch [3/10], Batch [1800/3237], Loss: 0.32136160135269165\n",
      "Epoch [3/10], Batch [1810/3237], Loss: 0.1481780707836151\n",
      "Epoch [3/10], Batch [1820/3237], Loss: 0.16251973807811737\n",
      "Epoch [3/10], Batch [1830/3237], Loss: 0.3226509690284729\n",
      "Epoch [3/10], Batch [1840/3237], Loss: 0.19746054708957672\n",
      "Epoch [3/10], Batch [1850/3237], Loss: 0.1488802433013916\n",
      "Epoch [3/10], Batch [1860/3237], Loss: 0.18071135878562927\n",
      "Epoch [3/10], Batch [1870/3237], Loss: 0.09930434077978134\n",
      "Epoch [3/10], Batch [1880/3237], Loss: 0.2554980516433716\n",
      "Epoch [3/10], Batch [1890/3237], Loss: 0.21540957689285278\n",
      "Epoch [3/10], Batch [1900/3237], Loss: 0.15962934494018555\n",
      "Epoch [3/10], Batch [1910/3237], Loss: 0.23938754200935364\n",
      "Epoch [3/10], Batch [1920/3237], Loss: 0.22256411612033844\n",
      "Epoch [3/10], Batch [1930/3237], Loss: 0.264005184173584\n",
      "Epoch [3/10], Batch [1940/3237], Loss: 0.24619129300117493\n",
      "Epoch [3/10], Batch [1950/3237], Loss: 0.17889012396335602\n",
      "Epoch [3/10], Batch [1960/3237], Loss: 0.22694765031337738\n",
      "Epoch [3/10], Batch [1970/3237], Loss: 0.2712419331073761\n",
      "Epoch [3/10], Batch [1980/3237], Loss: 0.13129059970378876\n",
      "Epoch [3/10], Batch [1990/3237], Loss: 0.22360816597938538\n",
      "Epoch [3/10], Batch [2000/3237], Loss: 0.27525007724761963\n",
      "Epoch [3/10], Batch [2010/3237], Loss: 0.20497263967990875\n",
      "Epoch [3/10], Batch [2020/3237], Loss: 0.10376103967428207\n",
      "Epoch [3/10], Batch [2030/3237], Loss: 0.311353474855423\n",
      "Epoch [3/10], Batch [2040/3237], Loss: 0.16255134344100952\n",
      "Epoch [3/10], Batch [2050/3237], Loss: 0.19455105066299438\n",
      "Epoch [3/10], Batch [2060/3237], Loss: 0.2003968060016632\n",
      "Epoch [3/10], Batch [2070/3237], Loss: 0.2076985090970993\n",
      "Epoch [3/10], Batch [2080/3237], Loss: 0.31074264645576477\n",
      "Epoch [3/10], Batch [2090/3237], Loss: 0.1631222814321518\n",
      "Epoch [3/10], Batch [2100/3237], Loss: 0.3115892708301544\n",
      "Epoch [3/10], Batch [2110/3237], Loss: 0.2649354934692383\n",
      "Epoch [3/10], Batch [2120/3237], Loss: 0.25705382227897644\n",
      "Epoch [3/10], Batch [2130/3237], Loss: 0.27015483379364014\n",
      "Epoch [3/10], Batch [2140/3237], Loss: 0.14364807307720184\n",
      "Epoch [3/10], Batch [2150/3237], Loss: 0.15533748269081116\n",
      "Epoch [3/10], Batch [2160/3237], Loss: 0.10421846061944962\n",
      "Epoch [3/10], Batch [2170/3237], Loss: 0.14453166723251343\n",
      "Epoch [3/10], Batch [2180/3237], Loss: 0.15991945564746857\n",
      "Epoch [3/10], Batch [2190/3237], Loss: 0.1734517216682434\n",
      "Epoch [3/10], Batch [2200/3237], Loss: 0.22385536134243011\n",
      "Epoch [3/10], Batch [2210/3237], Loss: 0.10332715511322021\n",
      "Epoch [3/10], Batch [2220/3237], Loss: 0.18215984106063843\n",
      "Epoch [3/10], Batch [2230/3237], Loss: 0.2753128111362457\n",
      "Epoch [3/10], Batch [2240/3237], Loss: 0.15725602209568024\n",
      "Epoch [3/10], Batch [2250/3237], Loss: 0.1669841706752777\n",
      "Epoch [3/10], Batch [2260/3237], Loss: 0.26905757188796997\n",
      "Epoch [3/10], Batch [2270/3237], Loss: 0.18357208371162415\n",
      "Epoch [3/10], Batch [2280/3237], Loss: 0.14998213946819305\n",
      "Epoch [3/10], Batch [2290/3237], Loss: 0.21790453791618347\n",
      "Epoch [3/10], Batch [2300/3237], Loss: 0.26389628648757935\n",
      "Epoch [3/10], Batch [2310/3237], Loss: 0.1619555950164795\n",
      "Epoch [3/10], Batch [2320/3237], Loss: 0.34003347158432007\n",
      "Epoch [3/10], Batch [2330/3237], Loss: 0.14764925837516785\n",
      "Epoch [3/10], Batch [2340/3237], Loss: 0.13484422862529755\n",
      "Epoch [3/10], Batch [2350/3237], Loss: 0.1716710478067398\n",
      "Epoch [3/10], Batch [2360/3237], Loss: 0.22009199857711792\n",
      "Epoch [3/10], Batch [2370/3237], Loss: 0.13425132632255554\n",
      "Epoch [3/10], Batch [2380/3237], Loss: 0.24816542863845825\n",
      "Epoch [3/10], Batch [2390/3237], Loss: 0.11672355234622955\n",
      "Epoch [3/10], Batch [2400/3237], Loss: 0.23108495771884918\n",
      "Epoch [3/10], Batch [2410/3237], Loss: 0.2251749336719513\n",
      "Epoch [3/10], Batch [2420/3237], Loss: 0.20576003193855286\n",
      "Epoch [3/10], Batch [2430/3237], Loss: 0.12157700955867767\n",
      "Epoch [3/10], Batch [2440/3237], Loss: 0.10033737868070602\n",
      "Epoch [3/10], Batch [2450/3237], Loss: 0.24425366520881653\n",
      "Epoch [3/10], Batch [2460/3237], Loss: 0.10647894442081451\n",
      "Epoch [3/10], Batch [2470/3237], Loss: 0.3471125066280365\n",
      "Epoch [3/10], Batch [2480/3237], Loss: 0.2254919409751892\n",
      "Epoch [3/10], Batch [2490/3237], Loss: 0.11959930509328842\n",
      "Epoch [3/10], Batch [2500/3237], Loss: 0.07806877046823502\n",
      "Epoch [3/10], Batch [2510/3237], Loss: 0.39636778831481934\n",
      "Epoch [3/10], Batch [2520/3237], Loss: 0.15300093591213226\n",
      "Epoch [3/10], Batch [2530/3237], Loss: 0.4041767120361328\n",
      "Epoch [3/10], Batch [2540/3237], Loss: 0.21666881442070007\n",
      "Epoch [3/10], Batch [2550/3237], Loss: 0.21342626214027405\n",
      "Epoch [3/10], Batch [2560/3237], Loss: 0.20324525237083435\n",
      "Epoch [3/10], Batch [2570/3237], Loss: 0.13668930530548096\n",
      "Epoch [3/10], Batch [2580/3237], Loss: 0.22117923200130463\n",
      "Epoch [3/10], Batch [2590/3237], Loss: 0.1820475161075592\n",
      "Epoch [3/10], Batch [2600/3237], Loss: 0.08939185738563538\n",
      "Epoch [3/10], Batch [2610/3237], Loss: 0.22128669917583466\n",
      "Epoch [3/10], Batch [2620/3237], Loss: 0.16073814034461975\n",
      "Epoch [3/10], Batch [2630/3237], Loss: 0.2203567773103714\n",
      "Epoch [3/10], Batch [2640/3237], Loss: 0.1558355838060379\n",
      "Epoch [3/10], Batch [2650/3237], Loss: 0.10683497786521912\n",
      "Epoch [3/10], Batch [2660/3237], Loss: 0.1345851570367813\n",
      "Epoch [3/10], Batch [2670/3237], Loss: 0.4724932014942169\n",
      "Epoch [3/10], Batch [2680/3237], Loss: 0.14941033720970154\n",
      "Epoch [3/10], Batch [2690/3237], Loss: 0.08076613396406174\n",
      "Epoch [3/10], Batch [2700/3237], Loss: 0.0888829231262207\n",
      "Epoch [3/10], Batch [2710/3237], Loss: 0.17904523015022278\n",
      "Epoch [3/10], Batch [2720/3237], Loss: 0.16617713868618011\n",
      "Epoch [3/10], Batch [2730/3237], Loss: 0.25411075353622437\n",
      "Epoch [3/10], Batch [2740/3237], Loss: 0.2017313688993454\n",
      "Epoch [3/10], Batch [2750/3237], Loss: 0.1863165646791458\n",
      "Epoch [3/10], Batch [2760/3237], Loss: 0.2024201601743698\n",
      "Epoch [3/10], Batch [2770/3237], Loss: 0.15271347761154175\n",
      "Epoch [3/10], Batch [2780/3237], Loss: 0.1453166902065277\n",
      "Epoch [3/10], Batch [2790/3237], Loss: 0.12987487018108368\n",
      "Epoch [3/10], Batch [2800/3237], Loss: 0.13704551756381989\n",
      "Epoch [3/10], Batch [2810/3237], Loss: 0.14253386855125427\n",
      "Epoch [3/10], Batch [2820/3237], Loss: 0.17135249078273773\n",
      "Epoch [3/10], Batch [2830/3237], Loss: 0.158824160695076\n",
      "Epoch [3/10], Batch [2840/3237], Loss: 0.11183055490255356\n",
      "Epoch [3/10], Batch [2850/3237], Loss: 0.1745484173297882\n",
      "Epoch [3/10], Batch [2860/3237], Loss: 0.40928274393081665\n",
      "Epoch [3/10], Batch [2870/3237], Loss: 0.1677194982767105\n",
      "Epoch [3/10], Batch [2880/3237], Loss: 0.16695819795131683\n",
      "Epoch [3/10], Batch [2890/3237], Loss: 0.24454724788665771\n",
      "Epoch [3/10], Batch [2900/3237], Loss: 0.12563456594944\n",
      "Epoch [3/10], Batch [2910/3237], Loss: 0.430176317691803\n",
      "Epoch [3/10], Batch [2920/3237], Loss: 0.15232960879802704\n",
      "Epoch [3/10], Batch [2930/3237], Loss: 0.08268792927265167\n",
      "Epoch [3/10], Batch [2940/3237], Loss: 0.19862960278987885\n",
      "Epoch [3/10], Batch [2950/3237], Loss: 0.2179090976715088\n",
      "Epoch [3/10], Batch [2960/3237], Loss: 0.2104681134223938\n",
      "Epoch [3/10], Batch [2970/3237], Loss: 0.09883547574281693\n",
      "Epoch [3/10], Batch [2980/3237], Loss: 0.2198813408613205\n",
      "Epoch [3/10], Batch [2990/3237], Loss: 0.24468111991882324\n",
      "Epoch [3/10], Batch [3000/3237], Loss: 0.1246330589056015\n",
      "Epoch [3/10], Batch [3010/3237], Loss: 0.22050738334655762\n",
      "Epoch [3/10], Batch [3020/3237], Loss: 0.23390568792819977\n",
      "Epoch [3/10], Batch [3030/3237], Loss: 0.1375957727432251\n",
      "Epoch [3/10], Batch [3040/3237], Loss: 0.26071760058403015\n",
      "Epoch [3/10], Batch [3050/3237], Loss: 0.16697055101394653\n",
      "Epoch [3/10], Batch [3060/3237], Loss: 0.1576634794473648\n",
      "Epoch [3/10], Batch [3070/3237], Loss: 0.19016388058662415\n",
      "Epoch [3/10], Batch [3080/3237], Loss: 0.1987193077802658\n",
      "Epoch [3/10], Batch [3090/3237], Loss: 0.19601000845432281\n",
      "Epoch [3/10], Batch [3100/3237], Loss: 0.17956164479255676\n",
      "Epoch [3/10], Batch [3110/3237], Loss: 0.15344546735286713\n",
      "Epoch [3/10], Batch [3120/3237], Loss: 0.2899634838104248\n",
      "Epoch [3/10], Batch [3130/3237], Loss: 0.23704910278320312\n",
      "Epoch [3/10], Batch [3140/3237], Loss: 0.23401187360286713\n",
      "Epoch [3/10], Batch [3150/3237], Loss: 0.16333423554897308\n",
      "Epoch [3/10], Batch [3160/3237], Loss: 0.19568683207035065\n",
      "Epoch [3/10], Batch [3170/3237], Loss: 0.12303391844034195\n",
      "Epoch [3/10], Batch [3180/3237], Loss: 0.21124893426895142\n",
      "Epoch [3/10], Batch [3190/3237], Loss: 0.10352756083011627\n",
      "Epoch [3/10], Batch [3200/3237], Loss: 0.1482163369655609\n",
      "Epoch [3/10], Batch [3210/3237], Loss: 0.12738236784934998\n",
      "Epoch [3/10], Batch [3220/3237], Loss: 0.24450431764125824\n",
      "Epoch [3/10], Batch [3230/3237], Loss: 0.09823495894670486\n",
      "Epoch 3, Average Train Loss: 0.1999, Average Val Loss: 0.2028\n",
      "Saved new best model at epoch 3 with Val Loss: 0.2028\n",
      "Epoch [4/10], Batch [10/3237], Loss: 0.37704595923423767\n",
      "Epoch [4/10], Batch [20/3237], Loss: 0.08626232296228409\n",
      "Epoch [4/10], Batch [30/3237], Loss: 0.15715660154819489\n",
      "Epoch [4/10], Batch [40/3237], Loss: 0.1436408907175064\n",
      "Epoch [4/10], Batch [50/3237], Loss: 0.18029552698135376\n",
      "Epoch [4/10], Batch [60/3237], Loss: 0.14739163219928741\n",
      "Epoch [4/10], Batch [70/3237], Loss: 0.13703858852386475\n",
      "Epoch [4/10], Batch [80/3237], Loss: 0.18573135137557983\n",
      "Epoch [4/10], Batch [90/3237], Loss: 0.2440691888332367\n",
      "Epoch [4/10], Batch [100/3237], Loss: 0.2603330910205841\n",
      "Epoch [4/10], Batch [110/3237], Loss: 0.18258415162563324\n",
      "Epoch [4/10], Batch [120/3237], Loss: 0.11885548382997513\n",
      "Epoch [4/10], Batch [130/3237], Loss: 0.3448947072029114\n",
      "Epoch [4/10], Batch [140/3237], Loss: 0.25369346141815186\n",
      "Epoch [4/10], Batch [150/3237], Loss: 0.31594318151474\n",
      "Epoch [4/10], Batch [160/3237], Loss: 0.11707944422960281\n",
      "Epoch [4/10], Batch [170/3237], Loss: 0.17728275060653687\n",
      "Epoch [4/10], Batch [180/3237], Loss: 0.14010436832904816\n",
      "Epoch [4/10], Batch [190/3237], Loss: 0.0981660857796669\n",
      "Epoch [4/10], Batch [200/3237], Loss: 0.1896139234304428\n",
      "Epoch [4/10], Batch [210/3237], Loss: 0.2293756604194641\n",
      "Epoch [4/10], Batch [220/3237], Loss: 0.4138600528240204\n",
      "Epoch [4/10], Batch [230/3237], Loss: 0.129408061504364\n",
      "Epoch [4/10], Batch [240/3237], Loss: 0.1905328929424286\n",
      "Epoch [4/10], Batch [250/3237], Loss: 0.2508912682533264\n",
      "Epoch [4/10], Batch [260/3237], Loss: 0.1515144556760788\n",
      "Epoch [4/10], Batch [270/3237], Loss: 0.1851673126220703\n",
      "Epoch [4/10], Batch [280/3237], Loss: 0.1646098643541336\n",
      "Epoch [4/10], Batch [290/3237], Loss: 0.10216876119375229\n",
      "Epoch [4/10], Batch [300/3237], Loss: 0.23852433264255524\n",
      "Epoch [4/10], Batch [310/3237], Loss: 0.1571560949087143\n",
      "Epoch [4/10], Batch [320/3237], Loss: 0.11835722625255585\n",
      "Epoch [4/10], Batch [330/3237], Loss: 0.13070595264434814\n",
      "Epoch [4/10], Batch [340/3237], Loss: 0.13942138850688934\n",
      "Epoch [4/10], Batch [350/3237], Loss: 0.09567898511886597\n",
      "Epoch [4/10], Batch [360/3237], Loss: 0.13343000411987305\n",
      "Epoch [4/10], Batch [370/3237], Loss: 0.21298417448997498\n",
      "Epoch [4/10], Batch [380/3237], Loss: 0.11650007963180542\n",
      "Epoch [4/10], Batch [390/3237], Loss: 0.30509430170059204\n",
      "Epoch [4/10], Batch [400/3237], Loss: 0.15943190455436707\n",
      "Epoch [4/10], Batch [410/3237], Loss: 0.4387740194797516\n",
      "Epoch [4/10], Batch [420/3237], Loss: 0.15392863750457764\n",
      "Epoch [4/10], Batch [430/3237], Loss: 0.3267107307910919\n",
      "Epoch [4/10], Batch [440/3237], Loss: 0.11597388237714767\n",
      "Epoch [4/10], Batch [450/3237], Loss: 0.2500932514667511\n",
      "Epoch [4/10], Batch [460/3237], Loss: 0.2095181941986084\n",
      "Epoch [4/10], Batch [470/3237], Loss: 0.30498555302619934\n",
      "Epoch [4/10], Batch [480/3237], Loss: 0.10470029711723328\n",
      "Epoch [4/10], Batch [490/3237], Loss: 0.22740305960178375\n",
      "Epoch [4/10], Batch [500/3237], Loss: 0.14056405425071716\n",
      "Epoch [4/10], Batch [510/3237], Loss: 0.1546570360660553\n",
      "Epoch [4/10], Batch [520/3237], Loss: 0.1692623496055603\n",
      "Epoch [4/10], Batch [530/3237], Loss: 0.4521953761577606\n",
      "Epoch [4/10], Batch [540/3237], Loss: 0.2650202810764313\n",
      "Epoch [4/10], Batch [550/3237], Loss: 0.1170281395316124\n",
      "Epoch [4/10], Batch [560/3237], Loss: 0.1748320311307907\n",
      "Epoch [4/10], Batch [570/3237], Loss: 0.2482220083475113\n",
      "Epoch [4/10], Batch [580/3237], Loss: 0.2732361853122711\n",
      "Epoch [4/10], Batch [590/3237], Loss: 0.3350051939487457\n",
      "Epoch [4/10], Batch [600/3237], Loss: 0.29615387320518494\n",
      "Epoch [4/10], Batch [610/3237], Loss: 0.1348547786474228\n",
      "Epoch [4/10], Batch [620/3237], Loss: 0.11438997089862823\n",
      "Epoch [4/10], Batch [630/3237], Loss: 0.17563049495220184\n",
      "Epoch [4/10], Batch [640/3237], Loss: 0.2163650244474411\n",
      "Epoch [4/10], Batch [650/3237], Loss: 0.23467619717121124\n",
      "Epoch [4/10], Batch [660/3237], Loss: 0.3060596287250519\n",
      "Epoch [4/10], Batch [670/3237], Loss: 0.23135638236999512\n",
      "Epoch [4/10], Batch [680/3237], Loss: 0.1750829666852951\n",
      "Epoch [4/10], Batch [690/3237], Loss: 0.14721454679965973\n",
      "Epoch [4/10], Batch [700/3237], Loss: 0.34535372257232666\n",
      "Epoch [4/10], Batch [710/3237], Loss: 0.10230062901973724\n",
      "Epoch [4/10], Batch [720/3237], Loss: 0.1640738695859909\n",
      "Epoch [4/10], Batch [730/3237], Loss: 0.18579775094985962\n",
      "Epoch [4/10], Batch [740/3237], Loss: 0.12157033383846283\n",
      "Epoch [4/10], Batch [750/3237], Loss: 0.1362084597349167\n",
      "Epoch [4/10], Batch [760/3237], Loss: 0.18455131351947784\n",
      "Epoch [4/10], Batch [770/3237], Loss: 0.1149909719824791\n",
      "Epoch [4/10], Batch [780/3237], Loss: 0.19009071588516235\n",
      "Epoch [4/10], Batch [790/3237], Loss: 0.16941535472869873\n",
      "Epoch [4/10], Batch [800/3237], Loss: 0.09879804402589798\n",
      "Epoch [4/10], Batch [810/3237], Loss: 0.1441013067960739\n",
      "Epoch [4/10], Batch [820/3237], Loss: 0.10927008092403412\n",
      "Epoch [4/10], Batch [830/3237], Loss: 0.0866173505783081\n",
      "Epoch [4/10], Batch [840/3237], Loss: 0.12265430390834808\n",
      "Epoch [4/10], Batch [850/3237], Loss: 0.1272769272327423\n",
      "Epoch [4/10], Batch [860/3237], Loss: 0.15317286550998688\n",
      "Epoch [4/10], Batch [870/3237], Loss: 0.2951567769050598\n",
      "Epoch [4/10], Batch [880/3237], Loss: 0.28243929147720337\n",
      "Epoch [4/10], Batch [890/3237], Loss: 0.17998334765434265\n",
      "Epoch [4/10], Batch [900/3237], Loss: 0.12271255254745483\n",
      "Epoch [4/10], Batch [910/3237], Loss: 0.1803252249956131\n",
      "Epoch [4/10], Batch [920/3237], Loss: 0.13516198098659515\n",
      "Epoch [4/10], Batch [930/3237], Loss: 0.1864493489265442\n",
      "Epoch [4/10], Batch [940/3237], Loss: 0.6105789542198181\n",
      "Epoch [4/10], Batch [950/3237], Loss: 0.31431663036346436\n",
      "Epoch [4/10], Batch [960/3237], Loss: 0.14373861253261566\n",
      "Epoch [4/10], Batch [970/3237], Loss: 0.12457162886857986\n",
      "Epoch [4/10], Batch [980/3237], Loss: 0.15001192688941956\n",
      "Epoch [4/10], Batch [990/3237], Loss: 0.10521919280290604\n",
      "Epoch [4/10], Batch [1000/3237], Loss: 0.6970618367195129\n",
      "Epoch [4/10], Batch [1010/3237], Loss: 0.20568197965621948\n",
      "Epoch [4/10], Batch [1020/3237], Loss: 0.15267041325569153\n",
      "Epoch [4/10], Batch [1030/3237], Loss: 0.2656461298465729\n",
      "Epoch [4/10], Batch [1040/3237], Loss: 0.13220633566379547\n",
      "Epoch [4/10], Batch [1050/3237], Loss: 0.08226704597473145\n",
      "Epoch [4/10], Batch [1060/3237], Loss: 0.15007087588310242\n",
      "Epoch [4/10], Batch [1070/3237], Loss: 0.13742828369140625\n",
      "Epoch [4/10], Batch [1080/3237], Loss: 0.18286554515361786\n",
      "Epoch [4/10], Batch [1090/3237], Loss: 0.18825319409370422\n",
      "Epoch [4/10], Batch [1100/3237], Loss: 0.17118681967258453\n",
      "Epoch [4/10], Batch [1110/3237], Loss: 0.2282586544752121\n",
      "Epoch [4/10], Batch [1120/3237], Loss: 0.17468330264091492\n",
      "Epoch [4/10], Batch [1130/3237], Loss: 0.16232864558696747\n",
      "Epoch [4/10], Batch [1140/3237], Loss: 0.2209223359823227\n",
      "Epoch [4/10], Batch [1150/3237], Loss: 0.1272290199995041\n",
      "Epoch [4/10], Batch [1160/3237], Loss: 0.1378623992204666\n",
      "Epoch [4/10], Batch [1170/3237], Loss: 0.053512319922447205\n",
      "Epoch [4/10], Batch [1180/3237], Loss: 0.11131709069013596\n",
      "Epoch [4/10], Batch [1190/3237], Loss: 0.1265483796596527\n",
      "Epoch [4/10], Batch [1200/3237], Loss: 0.08019702136516571\n",
      "Epoch [4/10], Batch [1210/3237], Loss: 0.2802294194698334\n",
      "Epoch [4/10], Batch [1220/3237], Loss: 0.10759402811527252\n",
      "Epoch [4/10], Batch [1230/3237], Loss: 0.24723881483078003\n",
      "Epoch [4/10], Batch [1240/3237], Loss: 0.08457624167203903\n",
      "Epoch [4/10], Batch [1250/3237], Loss: 0.1271347999572754\n",
      "Epoch [4/10], Batch [1260/3237], Loss: 0.17587143182754517\n",
      "Epoch [4/10], Batch [1270/3237], Loss: 0.21572469174861908\n",
      "Epoch [4/10], Batch [1280/3237], Loss: 0.17794717848300934\n",
      "Epoch [4/10], Batch [1290/3237], Loss: 0.1700800359249115\n",
      "Epoch [4/10], Batch [1300/3237], Loss: 0.20529720187187195\n",
      "Epoch [4/10], Batch [1310/3237], Loss: 0.14959965646266937\n",
      "Epoch [4/10], Batch [1320/3237], Loss: 0.21441134810447693\n",
      "Epoch [4/10], Batch [1330/3237], Loss: 0.12577062845230103\n",
      "Epoch [4/10], Batch [1340/3237], Loss: 0.20693615078926086\n",
      "Epoch [4/10], Batch [1350/3237], Loss: 0.3287663161754608\n",
      "Epoch [4/10], Batch [1360/3237], Loss: 0.28667861223220825\n",
      "Epoch [4/10], Batch [1370/3237], Loss: 0.09837859123945236\n",
      "Epoch [4/10], Batch [1380/3237], Loss: 0.10860469937324524\n",
      "Epoch [4/10], Batch [1390/3237], Loss: 0.0949687734246254\n",
      "Epoch [4/10], Batch [1400/3237], Loss: 0.6024004220962524\n",
      "Epoch [4/10], Batch [1410/3237], Loss: 0.23280316591262817\n",
      "Epoch [4/10], Batch [1420/3237], Loss: 0.1849924474954605\n",
      "Epoch [4/10], Batch [1430/3237], Loss: 0.3127387464046478\n",
      "Epoch [4/10], Batch [1440/3237], Loss: 0.21865130960941315\n",
      "Epoch [4/10], Batch [1450/3237], Loss: 0.20088696479797363\n",
      "Epoch [4/10], Batch [1460/3237], Loss: 0.15495559573173523\n",
      "Epoch [4/10], Batch [1470/3237], Loss: 0.23438194394111633\n",
      "Epoch [4/10], Batch [1480/3237], Loss: 0.14669056236743927\n",
      "Epoch [4/10], Batch [1490/3237], Loss: 0.1556735783815384\n",
      "Epoch [4/10], Batch [1500/3237], Loss: 0.2214132696390152\n",
      "Epoch [4/10], Batch [1510/3237], Loss: 0.14700143039226532\n",
      "Epoch [4/10], Batch [1520/3237], Loss: 0.07847991585731506\n",
      "Epoch [4/10], Batch [1530/3237], Loss: 0.1008690595626831\n",
      "Epoch [4/10], Batch [1540/3237], Loss: 0.22493402659893036\n",
      "Epoch [4/10], Batch [1550/3237], Loss: 0.14343050122261047\n",
      "Epoch [4/10], Batch [1560/3237], Loss: 0.19804546236991882\n",
      "Epoch [4/10], Batch [1570/3237], Loss: 0.05366959422826767\n",
      "Epoch [4/10], Batch [1580/3237], Loss: 0.33110618591308594\n",
      "Epoch [4/10], Batch [1590/3237], Loss: 0.2349833846092224\n",
      "Epoch [4/10], Batch [1600/3237], Loss: 0.16418880224227905\n",
      "Epoch [4/10], Batch [1610/3237], Loss: 0.15650902688503265\n",
      "Epoch [4/10], Batch [1620/3237], Loss: 0.09005764871835709\n",
      "Epoch [4/10], Batch [1630/3237], Loss: 0.13555216789245605\n",
      "Epoch [4/10], Batch [1640/3237], Loss: 0.14200322329998016\n",
      "Epoch [4/10], Batch [1650/3237], Loss: 0.1691969484090805\n",
      "Epoch [4/10], Batch [1660/3237], Loss: 0.10530003905296326\n",
      "Epoch [4/10], Batch [1670/3237], Loss: 0.0801612064242363\n",
      "Epoch [4/10], Batch [1680/3237], Loss: 0.1990073323249817\n",
      "Epoch [4/10], Batch [1690/3237], Loss: 0.09191086888313293\n",
      "Epoch [4/10], Batch [1700/3237], Loss: 0.18088218569755554\n",
      "Epoch [4/10], Batch [1710/3237], Loss: 0.11411603540182114\n",
      "Epoch [4/10], Batch [1720/3237], Loss: 0.10397019237279892\n",
      "Epoch [4/10], Batch [1730/3237], Loss: 0.072129026055336\n",
      "Epoch [4/10], Batch [1740/3237], Loss: 0.38604286313056946\n",
      "Epoch [4/10], Batch [1750/3237], Loss: 0.3469904661178589\n",
      "Epoch [4/10], Batch [1760/3237], Loss: 0.06710047274827957\n",
      "Epoch [4/10], Batch [1770/3237], Loss: 0.2630201578140259\n",
      "Epoch [4/10], Batch [1780/3237], Loss: 0.11004070192575455\n",
      "Epoch [4/10], Batch [1790/3237], Loss: 0.43115341663360596\n",
      "Epoch [4/10], Batch [1800/3237], Loss: 0.17698527872562408\n",
      "Epoch [4/10], Batch [1810/3237], Loss: 0.09810472279787064\n",
      "Epoch [4/10], Batch [1820/3237], Loss: 0.17689795792102814\n",
      "Epoch [4/10], Batch [1830/3237], Loss: 0.1290973424911499\n",
      "Epoch [4/10], Batch [1840/3237], Loss: 0.07055668532848358\n",
      "Epoch [4/10], Batch [1850/3237], Loss: 0.1866346001625061\n",
      "Epoch [4/10], Batch [1860/3237], Loss: 0.18925826251506805\n",
      "Epoch [4/10], Batch [1870/3237], Loss: 0.10817836225032806\n",
      "Epoch [4/10], Batch [1880/3237], Loss: 0.19871582090854645\n",
      "Epoch [4/10], Batch [1890/3237], Loss: 0.16757051646709442\n",
      "Epoch [4/10], Batch [1900/3237], Loss: 0.06920745223760605\n",
      "Epoch [4/10], Batch [1910/3237], Loss: 0.07936342805624008\n",
      "Epoch [4/10], Batch [1920/3237], Loss: 0.22296512126922607\n",
      "Epoch [4/10], Batch [1930/3237], Loss: 0.09589104354381561\n",
      "Epoch [4/10], Batch [1940/3237], Loss: 0.12750448286533356\n",
      "Epoch [4/10], Batch [1950/3237], Loss: 0.1368628442287445\n",
      "Epoch [4/10], Batch [1960/3237], Loss: 0.26186686754226685\n",
      "Epoch [4/10], Batch [1970/3237], Loss: 0.31173282861709595\n",
      "Epoch [4/10], Batch [1980/3237], Loss: 0.2790314257144928\n",
      "Epoch [4/10], Batch [1990/3237], Loss: 0.11671056598424911\n",
      "Epoch [4/10], Batch [2000/3237], Loss: 0.1948680430650711\n",
      "Epoch [4/10], Batch [2010/3237], Loss: 0.23691217601299286\n",
      "Epoch [4/10], Batch [2020/3237], Loss: 0.6241012215614319\n",
      "Epoch [4/10], Batch [2030/3237], Loss: 0.14130786061286926\n",
      "Epoch [4/10], Batch [2040/3237], Loss: 0.20531050860881805\n",
      "Epoch [4/10], Batch [2050/3237], Loss: 0.13744057714939117\n",
      "Epoch [4/10], Batch [2060/3237], Loss: 0.15040409564971924\n",
      "Epoch [4/10], Batch [2070/3237], Loss: 0.16667209565639496\n",
      "Epoch [4/10], Batch [2080/3237], Loss: 0.20813605189323425\n",
      "Epoch [4/10], Batch [2090/3237], Loss: 0.1980961561203003\n",
      "Epoch [4/10], Batch [2100/3237], Loss: 0.40006357431411743\n",
      "Epoch [4/10], Batch [2110/3237], Loss: 0.22166430950164795\n",
      "Epoch [4/10], Batch [2120/3237], Loss: 0.11640611290931702\n",
      "Epoch [4/10], Batch [2130/3237], Loss: 0.17523325979709625\n",
      "Epoch [4/10], Batch [2140/3237], Loss: 0.1407797932624817\n",
      "Epoch [4/10], Batch [2150/3237], Loss: 0.1041334867477417\n",
      "Epoch [4/10], Batch [2160/3237], Loss: 0.21187061071395874\n",
      "Epoch [4/10], Batch [2170/3237], Loss: 0.0924069955945015\n",
      "Epoch [4/10], Batch [2180/3237], Loss: 0.1263078898191452\n",
      "Epoch [4/10], Batch [2190/3237], Loss: 0.10742408782243729\n",
      "Epoch [4/10], Batch [2200/3237], Loss: 0.2000993937253952\n",
      "Epoch [4/10], Batch [2210/3237], Loss: 0.09303945302963257\n",
      "Epoch [4/10], Batch [2220/3237], Loss: 0.13894277811050415\n",
      "Epoch [4/10], Batch [2230/3237], Loss: 0.08658432960510254\n",
      "Epoch [4/10], Batch [2240/3237], Loss: 0.2827777862548828\n",
      "Epoch [4/10], Batch [2250/3237], Loss: 0.06001077592372894\n",
      "Epoch [4/10], Batch [2260/3237], Loss: 0.22993838787078857\n",
      "Epoch [4/10], Batch [2270/3237], Loss: 0.11794620007276535\n",
      "Epoch [4/10], Batch [2280/3237], Loss: 0.1801401823759079\n",
      "Epoch [4/10], Batch [2290/3237], Loss: 0.10263565182685852\n",
      "Epoch [4/10], Batch [2300/3237], Loss: 0.14901208877563477\n",
      "Epoch [4/10], Batch [2310/3237], Loss: 0.13718222081661224\n",
      "Epoch [4/10], Batch [2320/3237], Loss: 0.20865537226200104\n",
      "Epoch [4/10], Batch [2330/3237], Loss: 0.18781442940235138\n",
      "Epoch [4/10], Batch [2340/3237], Loss: 0.11762450635433197\n",
      "Epoch [4/10], Batch [2350/3237], Loss: 0.23651213943958282\n",
      "Epoch [4/10], Batch [2360/3237], Loss: 0.11583232879638672\n",
      "Epoch [4/10], Batch [2370/3237], Loss: 0.17727838456630707\n",
      "Epoch [4/10], Batch [2380/3237], Loss: 0.11597403138875961\n",
      "Epoch [4/10], Batch [2390/3237], Loss: 0.2446756362915039\n",
      "Epoch [4/10], Batch [2400/3237], Loss: 0.31285202503204346\n",
      "Epoch [4/10], Batch [2410/3237], Loss: 0.19198007881641388\n",
      "Epoch [4/10], Batch [2420/3237], Loss: 0.14062188565731049\n",
      "Epoch [4/10], Batch [2430/3237], Loss: 0.27824708819389343\n",
      "Epoch [4/10], Batch [2440/3237], Loss: 0.36588096618652344\n",
      "Epoch [4/10], Batch [2450/3237], Loss: 0.13724607229232788\n",
      "Epoch [4/10], Batch [2460/3237], Loss: 0.22579875588417053\n",
      "Epoch [4/10], Batch [2470/3237], Loss: 0.18341143429279327\n",
      "Epoch [4/10], Batch [2480/3237], Loss: 0.1314951628446579\n",
      "Epoch [4/10], Batch [2490/3237], Loss: 0.17522309720516205\n",
      "Epoch [4/10], Batch [2500/3237], Loss: 0.20616981387138367\n",
      "Epoch [4/10], Batch [2510/3237], Loss: 0.14956460893154144\n",
      "Epoch [4/10], Batch [2520/3237], Loss: 0.22991469502449036\n",
      "Epoch [4/10], Batch [2530/3237], Loss: 0.2773604989051819\n",
      "Epoch [4/10], Batch [2540/3237], Loss: 0.14083491265773773\n",
      "Epoch [4/10], Batch [2550/3237], Loss: 0.23370769619941711\n",
      "Epoch [4/10], Batch [2560/3237], Loss: 0.14681029319763184\n",
      "Epoch [4/10], Batch [2570/3237], Loss: 0.32257822155952454\n",
      "Epoch [4/10], Batch [2580/3237], Loss: 0.06185726448893547\n",
      "Epoch [4/10], Batch [2590/3237], Loss: 0.2643435597419739\n",
      "Epoch [4/10], Batch [2600/3237], Loss: 0.11241675913333893\n",
      "Epoch [4/10], Batch [2610/3237], Loss: 0.20297107100486755\n",
      "Epoch [4/10], Batch [2620/3237], Loss: 0.16382509469985962\n",
      "Epoch [4/10], Batch [2630/3237], Loss: 0.20146553218364716\n",
      "Epoch [4/10], Batch [2640/3237], Loss: 0.1767846792936325\n",
      "Epoch [4/10], Batch [2650/3237], Loss: 0.21678809821605682\n",
      "Epoch [4/10], Batch [2660/3237], Loss: 0.20544740557670593\n",
      "Epoch [4/10], Batch [2670/3237], Loss: 0.10515756160020828\n",
      "Epoch [4/10], Batch [2680/3237], Loss: 0.2869891822338104\n",
      "Epoch [4/10], Batch [2690/3237], Loss: 0.14099057018756866\n",
      "Epoch [4/10], Batch [2700/3237], Loss: 0.1496346890926361\n",
      "Epoch [4/10], Batch [2710/3237], Loss: 0.26078441739082336\n",
      "Epoch [4/10], Batch [2720/3237], Loss: 0.19651012122631073\n",
      "Epoch [4/10], Batch [2730/3237], Loss: 0.25352048873901367\n",
      "Epoch [4/10], Batch [2740/3237], Loss: 0.4418873190879822\n",
      "Epoch [4/10], Batch [2750/3237], Loss: 0.21741734445095062\n",
      "Epoch [4/10], Batch [2760/3237], Loss: 0.15353736281394958\n",
      "Epoch [4/10], Batch [2770/3237], Loss: 0.24307170510292053\n",
      "Epoch [4/10], Batch [2780/3237], Loss: 0.161004900932312\n",
      "Epoch [4/10], Batch [2790/3237], Loss: 0.10422774404287338\n",
      "Epoch [4/10], Batch [2800/3237], Loss: 0.2410532385110855\n",
      "Epoch [4/10], Batch [2810/3237], Loss: 0.32112112641334534\n",
      "Epoch [4/10], Batch [2820/3237], Loss: 0.10579591244459152\n",
      "Epoch [4/10], Batch [2830/3237], Loss: 0.15400336682796478\n",
      "Epoch [4/10], Batch [2840/3237], Loss: 0.2723180055618286\n",
      "Epoch [4/10], Batch [2850/3237], Loss: 0.16063301265239716\n",
      "Epoch [4/10], Batch [2860/3237], Loss: 0.16740748286247253\n",
      "Epoch [4/10], Batch [2870/3237], Loss: 0.14658938348293304\n",
      "Epoch [4/10], Batch [2880/3237], Loss: 0.2997880280017853\n",
      "Epoch [4/10], Batch [2890/3237], Loss: 0.17046280205249786\n",
      "Epoch [4/10], Batch [2900/3237], Loss: 0.14329741895198822\n",
      "Epoch [4/10], Batch [2910/3237], Loss: 0.2132139503955841\n",
      "Epoch [4/10], Batch [2920/3237], Loss: 0.2647593319416046\n",
      "Epoch [4/10], Batch [2930/3237], Loss: 0.17656444013118744\n",
      "Epoch [4/10], Batch [2940/3237], Loss: 0.08618078380823135\n",
      "Epoch [4/10], Batch [2950/3237], Loss: 0.1525827944278717\n",
      "Epoch [4/10], Batch [2960/3237], Loss: 0.14726485311985016\n",
      "Epoch [4/10], Batch [2970/3237], Loss: 0.09903383255004883\n",
      "Epoch [4/10], Batch [2980/3237], Loss: 0.09641896933317184\n",
      "Epoch [4/10], Batch [2990/3237], Loss: 0.10766731202602386\n",
      "Epoch [4/10], Batch [3000/3237], Loss: 0.18328434228897095\n",
      "Epoch [4/10], Batch [3010/3237], Loss: 0.18475335836410522\n",
      "Epoch [4/10], Batch [3020/3237], Loss: 0.19113066792488098\n",
      "Epoch [4/10], Batch [3030/3237], Loss: 0.361307829618454\n",
      "Epoch [4/10], Batch [3040/3237], Loss: 0.08019394427537918\n",
      "Epoch [4/10], Batch [3050/3237], Loss: 0.14526543021202087\n",
      "Epoch [4/10], Batch [3060/3237], Loss: 0.13896335661411285\n",
      "Epoch [4/10], Batch [3070/3237], Loss: 0.37175536155700684\n",
      "Epoch [4/10], Batch [3080/3237], Loss: 0.147416889667511\n",
      "Epoch [4/10], Batch [3090/3237], Loss: 0.12195425480604172\n",
      "Epoch [4/10], Batch [3100/3237], Loss: 0.24781428277492523\n",
      "Epoch [4/10], Batch [3110/3237], Loss: 0.094321109354496\n",
      "Epoch [4/10], Batch [3120/3237], Loss: 0.11751025915145874\n",
      "Epoch [4/10], Batch [3130/3237], Loss: 0.138528972864151\n",
      "Epoch [4/10], Batch [3140/3237], Loss: 0.1547047197818756\n",
      "Epoch [4/10], Batch [3150/3237], Loss: 0.1330152302980423\n",
      "Epoch [4/10], Batch [3160/3237], Loss: 0.3788374662399292\n",
      "Epoch [4/10], Batch [3170/3237], Loss: 0.24266460537910461\n",
      "Epoch [4/10], Batch [3180/3237], Loss: 0.1910364180803299\n",
      "Epoch [4/10], Batch [3190/3237], Loss: 0.3026930093765259\n",
      "Epoch [4/10], Batch [3200/3237], Loss: 0.0814281702041626\n",
      "Epoch [4/10], Batch [3210/3237], Loss: 0.27044668793678284\n",
      "Epoch [4/10], Batch [3220/3237], Loss: 0.13895365595817566\n",
      "Epoch [4/10], Batch [3230/3237], Loss: 0.22733184695243835\n",
      "Epoch 4, Average Train Loss: 0.1866, Average Val Loss: 0.2041\n",
      "Epoch [5/10], Batch [10/3237], Loss: 0.1591917872428894\n",
      "Epoch [5/10], Batch [20/3237], Loss: 0.15019173920154572\n",
      "Epoch [5/10], Batch [30/3237], Loss: 0.19729042053222656\n",
      "Epoch [5/10], Batch [40/3237], Loss: 0.20696793496608734\n",
      "Epoch [5/10], Batch [50/3237], Loss: 0.1819954216480255\n",
      "Epoch [5/10], Batch [60/3237], Loss: 0.14924579858779907\n",
      "Epoch [5/10], Batch [70/3237], Loss: 0.20703719556331635\n",
      "Epoch [5/10], Batch [80/3237], Loss: 0.23189546167850494\n",
      "Epoch [5/10], Batch [90/3237], Loss: 0.24518126249313354\n",
      "Epoch [5/10], Batch [100/3237], Loss: 0.37007638812065125\n",
      "Epoch [5/10], Batch [110/3237], Loss: 0.1506849229335785\n",
      "Epoch [5/10], Batch [120/3237], Loss: 0.12368115782737732\n",
      "Epoch [5/10], Batch [130/3237], Loss: 0.17393726110458374\n",
      "Epoch [5/10], Batch [140/3237], Loss: 0.14648893475532532\n",
      "Epoch [5/10], Batch [150/3237], Loss: 0.14091266691684723\n",
      "Epoch [5/10], Batch [160/3237], Loss: 0.2065739780664444\n",
      "Epoch [5/10], Batch [170/3237], Loss: 0.30937302112579346\n",
      "Epoch [5/10], Batch [180/3237], Loss: 0.2722894549369812\n",
      "Epoch [5/10], Batch [190/3237], Loss: 0.16624240577220917\n",
      "Epoch [5/10], Batch [200/3237], Loss: 0.11701894551515579\n",
      "Epoch [5/10], Batch [210/3237], Loss: 0.15658697485923767\n",
      "Epoch [5/10], Batch [220/3237], Loss: 0.07851000875234604\n",
      "Epoch [5/10], Batch [230/3237], Loss: 0.2645121216773987\n",
      "Epoch [5/10], Batch [240/3237], Loss: 0.1282917857170105\n",
      "Epoch [5/10], Batch [250/3237], Loss: 0.30569252371788025\n",
      "Epoch [5/10], Batch [260/3237], Loss: 0.13978876173496246\n",
      "Epoch [5/10], Batch [270/3237], Loss: 0.3930360674858093\n",
      "Epoch [5/10], Batch [280/3237], Loss: 0.1305348426103592\n",
      "Epoch [5/10], Batch [290/3237], Loss: 0.17304469645023346\n",
      "Epoch [5/10], Batch [300/3237], Loss: 0.1693040430545807\n",
      "Epoch [5/10], Batch [310/3237], Loss: 0.18796634674072266\n",
      "Epoch [5/10], Batch [320/3237], Loss: 0.12095262110233307\n",
      "Epoch [5/10], Batch [330/3237], Loss: 0.17342351377010345\n",
      "Epoch [5/10], Batch [340/3237], Loss: 0.1155456155538559\n",
      "Epoch [5/10], Batch [350/3237], Loss: 0.14422902464866638\n",
      "Epoch [5/10], Batch [360/3237], Loss: 0.10266135632991791\n",
      "Epoch [5/10], Batch [370/3237], Loss: 0.1288074404001236\n",
      "Epoch [5/10], Batch [380/3237], Loss: 0.07805335521697998\n",
      "Epoch [5/10], Batch [390/3237], Loss: 0.1964506357908249\n",
      "Epoch [5/10], Batch [400/3237], Loss: 0.1083989292383194\n",
      "Epoch [5/10], Batch [410/3237], Loss: 0.14301735162734985\n",
      "Epoch [5/10], Batch [420/3237], Loss: 0.09487780183553696\n",
      "Epoch [5/10], Batch [430/3237], Loss: 0.11117040365934372\n",
      "Epoch [5/10], Batch [440/3237], Loss: 0.08932295441627502\n",
      "Epoch [5/10], Batch [450/3237], Loss: 0.16661711037158966\n",
      "Epoch [5/10], Batch [460/3237], Loss: 0.15429124236106873\n",
      "Epoch [5/10], Batch [470/3237], Loss: 0.1760036051273346\n",
      "Epoch [5/10], Batch [480/3237], Loss: 0.3834261894226074\n",
      "Epoch [5/10], Batch [490/3237], Loss: 0.20698924362659454\n",
      "Epoch [5/10], Batch [500/3237], Loss: 0.3901635408401489\n",
      "Epoch [5/10], Batch [510/3237], Loss: 0.25031912326812744\n",
      "Epoch [5/10], Batch [520/3237], Loss: 0.10220784693956375\n",
      "Epoch [5/10], Batch [530/3237], Loss: 0.1295446902513504\n",
      "Epoch [5/10], Batch [540/3237], Loss: 0.126054584980011\n",
      "Epoch [5/10], Batch [550/3237], Loss: 0.1410897821187973\n",
      "Epoch [5/10], Batch [560/3237], Loss: 0.10476337373256683\n",
      "Epoch [5/10], Batch [570/3237], Loss: 0.14438308775424957\n",
      "Epoch [5/10], Batch [580/3237], Loss: 0.22561100125312805\n",
      "Epoch [5/10], Batch [590/3237], Loss: 0.2304084300994873\n",
      "Epoch [5/10], Batch [600/3237], Loss: 0.1175667941570282\n",
      "Epoch [5/10], Batch [610/3237], Loss: 0.24267230927944183\n",
      "Epoch [5/10], Batch [620/3237], Loss: 0.10760433971881866\n",
      "Epoch [5/10], Batch [630/3237], Loss: 0.21310855448246002\n",
      "Epoch [5/10], Batch [640/3237], Loss: 0.13287650048732758\n",
      "Epoch [5/10], Batch [650/3237], Loss: 0.22085151076316833\n",
      "Epoch [5/10], Batch [660/3237], Loss: 0.1103648915886879\n",
      "Epoch [5/10], Batch [670/3237], Loss: 0.11652351915836334\n",
      "Epoch [5/10], Batch [680/3237], Loss: 0.3185821771621704\n",
      "Epoch [5/10], Batch [690/3237], Loss: 0.15306265652179718\n",
      "Epoch [5/10], Batch [700/3237], Loss: 0.06594432890415192\n",
      "Epoch [5/10], Batch [710/3237], Loss: 0.14604952931404114\n",
      "Epoch [5/10], Batch [720/3237], Loss: 0.13958211243152618\n",
      "Epoch [5/10], Batch [730/3237], Loss: 0.29395073652267456\n",
      "Epoch [5/10], Batch [740/3237], Loss: 0.09444012492895126\n",
      "Epoch [5/10], Batch [750/3237], Loss: 0.16918620467185974\n",
      "Epoch [5/10], Batch [760/3237], Loss: 0.1058320626616478\n",
      "Epoch [5/10], Batch [770/3237], Loss: 0.05209016799926758\n",
      "Epoch [5/10], Batch [780/3237], Loss: 0.20056262612342834\n",
      "Epoch [5/10], Batch [790/3237], Loss: 0.5314384698867798\n",
      "Epoch [5/10], Batch [800/3237], Loss: 0.06781899929046631\n",
      "Epoch [5/10], Batch [810/3237], Loss: 0.20345687866210938\n",
      "Epoch [5/10], Batch [820/3237], Loss: 0.21122291684150696\n",
      "Epoch [5/10], Batch [830/3237], Loss: 0.08415227383375168\n",
      "Epoch [5/10], Batch [840/3237], Loss: 0.13003186881542206\n",
      "Epoch [5/10], Batch [850/3237], Loss: 0.10090303421020508\n",
      "Epoch [5/10], Batch [860/3237], Loss: 0.1729785054922104\n",
      "Epoch [5/10], Batch [870/3237], Loss: 0.20894168317317963\n",
      "Epoch [5/10], Batch [880/3237], Loss: 0.15230792760849\n",
      "Epoch [5/10], Batch [890/3237], Loss: 0.11187624931335449\n",
      "Epoch [5/10], Batch [900/3237], Loss: 0.20869120955467224\n",
      "Epoch [5/10], Batch [910/3237], Loss: 0.31395870447158813\n",
      "Epoch [5/10], Batch [920/3237], Loss: 0.10703445225954056\n",
      "Epoch [5/10], Batch [930/3237], Loss: 0.25702211260795593\n",
      "Epoch [5/10], Batch [940/3237], Loss: 0.23387441039085388\n",
      "Epoch [5/10], Batch [950/3237], Loss: 0.1587952822446823\n",
      "Epoch [5/10], Batch [960/3237], Loss: 0.15080226957798004\n",
      "Epoch [5/10], Batch [970/3237], Loss: 0.14842161536216736\n",
      "Epoch [5/10], Batch [980/3237], Loss: 0.08798817545175552\n",
      "Epoch [5/10], Batch [990/3237], Loss: 0.3875797688961029\n",
      "Epoch [5/10], Batch [1000/3237], Loss: 0.258975088596344\n",
      "Epoch [5/10], Batch [1010/3237], Loss: 0.11533772945404053\n",
      "Epoch [5/10], Batch [1020/3237], Loss: 0.09593949466943741\n",
      "Epoch [5/10], Batch [1030/3237], Loss: 0.16196197271347046\n",
      "Epoch [5/10], Batch [1040/3237], Loss: 0.11791294068098068\n",
      "Epoch [5/10], Batch [1050/3237], Loss: 0.19808770716190338\n",
      "Epoch [5/10], Batch [1060/3237], Loss: 0.18656112253665924\n",
      "Epoch [5/10], Batch [1070/3237], Loss: 0.35488182306289673\n",
      "Epoch [5/10], Batch [1080/3237], Loss: 0.29022136330604553\n",
      "Epoch [5/10], Batch [1090/3237], Loss: 0.10947003960609436\n",
      "Epoch [5/10], Batch [1100/3237], Loss: 0.18024799227714539\n",
      "Epoch [5/10], Batch [1110/3237], Loss: 0.11167088896036148\n",
      "Epoch [5/10], Batch [1120/3237], Loss: 0.04340846464037895\n",
      "Epoch [5/10], Batch [1130/3237], Loss: 0.13426408171653748\n",
      "Epoch [5/10], Batch [1140/3237], Loss: 0.14758436381816864\n",
      "Epoch [5/10], Batch [1150/3237], Loss: 0.21969519555568695\n",
      "Epoch [5/10], Batch [1160/3237], Loss: 0.13140439987182617\n",
      "Epoch [5/10], Batch [1170/3237], Loss: 0.11269472539424896\n",
      "Epoch [5/10], Batch [1180/3237], Loss: 0.18115760385990143\n",
      "Epoch [5/10], Batch [1190/3237], Loss: 0.08510194718837738\n",
      "Epoch [5/10], Batch [1200/3237], Loss: 0.24996337294578552\n",
      "Epoch [5/10], Batch [1210/3237], Loss: 0.15344029664993286\n",
      "Epoch [5/10], Batch [1220/3237], Loss: 0.12289078533649445\n",
      "Epoch [5/10], Batch [1230/3237], Loss: 0.3021032214164734\n",
      "Epoch [5/10], Batch [1240/3237], Loss: 0.12164587527513504\n",
      "Epoch [5/10], Batch [1250/3237], Loss: 0.21931079030036926\n",
      "Epoch [5/10], Batch [1260/3237], Loss: 0.18470318615436554\n",
      "Epoch [5/10], Batch [1270/3237], Loss: 0.18540683388710022\n",
      "Epoch [5/10], Batch [1280/3237], Loss: 0.11705464124679565\n",
      "Epoch [5/10], Batch [1290/3237], Loss: 0.47271156311035156\n",
      "Epoch [5/10], Batch [1300/3237], Loss: 0.15780867636203766\n",
      "Epoch [5/10], Batch [1310/3237], Loss: 0.33301088213920593\n",
      "Epoch [5/10], Batch [1320/3237], Loss: 0.1320193111896515\n",
      "Epoch [5/10], Batch [1330/3237], Loss: 0.12119615077972412\n",
      "Epoch [5/10], Batch [1340/3237], Loss: 0.20143912732601166\n",
      "Epoch [5/10], Batch [1350/3237], Loss: 0.12316804379224777\n",
      "Epoch [5/10], Batch [1360/3237], Loss: 0.13417740166187286\n",
      "Epoch [5/10], Batch [1370/3237], Loss: 0.21720093488693237\n",
      "Epoch [5/10], Batch [1380/3237], Loss: 0.11000394821166992\n",
      "Epoch [5/10], Batch [1390/3237], Loss: 0.08688875287771225\n",
      "Epoch [5/10], Batch [1400/3237], Loss: 0.22324338555335999\n",
      "Epoch [5/10], Batch [1410/3237], Loss: 0.4199061393737793\n",
      "Epoch [5/10], Batch [1420/3237], Loss: 0.07455926388502121\n",
      "Epoch [5/10], Batch [1430/3237], Loss: 0.09039592742919922\n",
      "Epoch [5/10], Batch [1440/3237], Loss: 0.13396184146404266\n",
      "Epoch [5/10], Batch [1450/3237], Loss: 0.1725013107061386\n",
      "Epoch [5/10], Batch [1460/3237], Loss: 0.10954601317644119\n",
      "Epoch [5/10], Batch [1470/3237], Loss: 0.1787717044353485\n",
      "Epoch [5/10], Batch [1480/3237], Loss: 0.16102004051208496\n",
      "Epoch [5/10], Batch [1490/3237], Loss: 0.08829263597726822\n",
      "Epoch [5/10], Batch [1500/3237], Loss: 0.08186081051826477\n",
      "Epoch [5/10], Batch [1510/3237], Loss: 0.13873903453350067\n",
      "Epoch [5/10], Batch [1520/3237], Loss: 0.15573950111865997\n",
      "Epoch [5/10], Batch [1530/3237], Loss: 0.11125257611274719\n",
      "Epoch [5/10], Batch [1540/3237], Loss: 0.13757364451885223\n",
      "Epoch [5/10], Batch [1550/3237], Loss: 0.08273057639598846\n",
      "Epoch [5/10], Batch [1560/3237], Loss: 0.17220517992973328\n",
      "Epoch [5/10], Batch [1570/3237], Loss: 0.1707940697669983\n",
      "Epoch [5/10], Batch [1580/3237], Loss: 0.05859817937016487\n",
      "Epoch [5/10], Batch [1590/3237], Loss: 0.09238819777965546\n",
      "Epoch [5/10], Batch [1600/3237], Loss: 0.10254944860935211\n",
      "Epoch [5/10], Batch [1610/3237], Loss: 0.20886383950710297\n",
      "Epoch [5/10], Batch [1620/3237], Loss: 0.23278960585594177\n",
      "Epoch [5/10], Batch [1630/3237], Loss: 0.08115892112255096\n",
      "Epoch [5/10], Batch [1640/3237], Loss: 0.09743402153253555\n",
      "Epoch [5/10], Batch [1650/3237], Loss: 0.2089887261390686\n",
      "Epoch [5/10], Batch [1660/3237], Loss: 0.08805013447999954\n",
      "Epoch [5/10], Batch [1670/3237], Loss: 0.13163241744041443\n",
      "Epoch [5/10], Batch [1680/3237], Loss: 0.2858184278011322\n",
      "Epoch [5/10], Batch [1690/3237], Loss: 0.10673180222511292\n",
      "Epoch [5/10], Batch [1700/3237], Loss: 0.293012410402298\n",
      "Epoch [5/10], Batch [1710/3237], Loss: 0.2016606479883194\n",
      "Epoch [5/10], Batch [1720/3237], Loss: 0.11147720366716385\n",
      "Epoch [5/10], Batch [1730/3237], Loss: 0.20876087248325348\n",
      "Epoch [5/10], Batch [1740/3237], Loss: 0.23825672268867493\n",
      "Epoch [5/10], Batch [1750/3237], Loss: 0.1440279483795166\n",
      "Epoch [5/10], Batch [1760/3237], Loss: 0.11125057190656662\n",
      "Epoch [5/10], Batch [1770/3237], Loss: 0.1349654346704483\n",
      "Epoch [5/10], Batch [1780/3237], Loss: 0.25840243697166443\n",
      "Epoch [5/10], Batch [1790/3237], Loss: 0.05578719079494476\n",
      "Epoch [5/10], Batch [1800/3237], Loss: 0.06310661882162094\n",
      "Epoch [5/10], Batch [1810/3237], Loss: 0.10851699858903885\n",
      "Epoch [5/10], Batch [1820/3237], Loss: 0.3444821238517761\n",
      "Epoch [5/10], Batch [1830/3237], Loss: 0.2583313584327698\n",
      "Epoch [5/10], Batch [1840/3237], Loss: 0.14959076046943665\n",
      "Epoch [5/10], Batch [1850/3237], Loss: 0.12359487265348434\n",
      "Epoch [5/10], Batch [1860/3237], Loss: 0.11707098037004471\n",
      "Epoch [5/10], Batch [1870/3237], Loss: 0.10868171602487564\n",
      "Epoch [5/10], Batch [1880/3237], Loss: 0.3294331431388855\n",
      "Epoch [5/10], Batch [1890/3237], Loss: 0.2570693790912628\n",
      "Epoch [5/10], Batch [1900/3237], Loss: 0.26897546648979187\n",
      "Epoch [5/10], Batch [1910/3237], Loss: 0.2202150821685791\n",
      "Epoch [5/10], Batch [1920/3237], Loss: 0.2677849233150482\n",
      "Epoch [5/10], Batch [1930/3237], Loss: 0.16867682337760925\n",
      "Epoch [5/10], Batch [1940/3237], Loss: 0.16073812544345856\n",
      "Epoch [5/10], Batch [1950/3237], Loss: 0.22516077756881714\n",
      "Epoch [5/10], Batch [1960/3237], Loss: 0.3527306616306305\n",
      "Epoch [5/10], Batch [1970/3237], Loss: 0.16049429774284363\n",
      "Epoch [5/10], Batch [1980/3237], Loss: 0.11966197937726974\n",
      "Epoch [5/10], Batch [1990/3237], Loss: 0.2712285816669464\n",
      "Epoch [5/10], Batch [2000/3237], Loss: 0.10706514120101929\n",
      "Epoch [5/10], Batch [2010/3237], Loss: 0.24095958471298218\n",
      "Epoch [5/10], Batch [2020/3237], Loss: 0.30720254778862\n",
      "Epoch [5/10], Batch [2030/3237], Loss: 0.21397088468074799\n",
      "Epoch [5/10], Batch [2040/3237], Loss: 0.1234147921204567\n",
      "Epoch [5/10], Batch [2050/3237], Loss: 0.23915892839431763\n",
      "Epoch [5/10], Batch [2060/3237], Loss: 0.1783960461616516\n",
      "Epoch [5/10], Batch [2070/3237], Loss: 0.3782888650894165\n",
      "Epoch [5/10], Batch [2080/3237], Loss: 0.10287948697805405\n",
      "Epoch [5/10], Batch [2090/3237], Loss: 0.29179805517196655\n",
      "Epoch [5/10], Batch [2100/3237], Loss: 0.1888170838356018\n",
      "Epoch [5/10], Batch [2110/3237], Loss: 0.24872657656669617\n",
      "Epoch [5/10], Batch [2120/3237], Loss: 0.20336289703845978\n",
      "Epoch [5/10], Batch [2130/3237], Loss: 0.14693686366081238\n",
      "Epoch [5/10], Batch [2140/3237], Loss: 0.2558642029762268\n",
      "Epoch [5/10], Batch [2150/3237], Loss: 0.09312639385461807\n",
      "Epoch [5/10], Batch [2160/3237], Loss: 0.15621981024742126\n",
      "Epoch [5/10], Batch [2170/3237], Loss: 0.1379581242799759\n",
      "Epoch [5/10], Batch [2180/3237], Loss: 0.26991698145866394\n",
      "Epoch [5/10], Batch [2190/3237], Loss: 0.20385092496871948\n",
      "Epoch [5/10], Batch [2200/3237], Loss: 0.13296708464622498\n",
      "Epoch [5/10], Batch [2210/3237], Loss: 0.13843168318271637\n",
      "Epoch [5/10], Batch [2220/3237], Loss: 0.10906042158603668\n",
      "Epoch [5/10], Batch [2230/3237], Loss: 0.10501163452863693\n",
      "Epoch [5/10], Batch [2240/3237], Loss: 0.11555812507867813\n",
      "Epoch [5/10], Batch [2250/3237], Loss: 0.2376173585653305\n",
      "Epoch [5/10], Batch [2260/3237], Loss: 0.22935836017131805\n",
      "Epoch [5/10], Batch [2270/3237], Loss: 0.12574999034404755\n",
      "Epoch [5/10], Batch [2280/3237], Loss: 0.12442810088396072\n",
      "Epoch [5/10], Batch [2290/3237], Loss: 0.11049795895814896\n",
      "Epoch [5/10], Batch [2300/3237], Loss: 0.13848571479320526\n",
      "Epoch [5/10], Batch [2310/3237], Loss: 0.19847087562084198\n",
      "Epoch [5/10], Batch [2320/3237], Loss: 0.34606292843818665\n",
      "Epoch [5/10], Batch [2330/3237], Loss: 0.21136124432086945\n",
      "Epoch [5/10], Batch [2340/3237], Loss: 0.1838640421628952\n",
      "Epoch [5/10], Batch [2350/3237], Loss: 0.19621391594409943\n",
      "Epoch [5/10], Batch [2360/3237], Loss: 0.1779455840587616\n",
      "Epoch [5/10], Batch [2370/3237], Loss: 0.2720348536968231\n",
      "Epoch [5/10], Batch [2380/3237], Loss: 0.11040821671485901\n",
      "Epoch [5/10], Batch [2390/3237], Loss: 0.24785616993904114\n",
      "Epoch [5/10], Batch [2400/3237], Loss: 0.1411089152097702\n",
      "Epoch [5/10], Batch [2410/3237], Loss: 0.1334218531847\n",
      "Epoch [5/10], Batch [2420/3237], Loss: 0.17664329707622528\n",
      "Epoch [5/10], Batch [2430/3237], Loss: 0.13966411352157593\n",
      "Epoch [5/10], Batch [2440/3237], Loss: 0.19245339930057526\n",
      "Epoch [5/10], Batch [2450/3237], Loss: 0.07862938940525055\n",
      "Epoch [5/10], Batch [2460/3237], Loss: 0.11502305418252945\n",
      "Epoch [5/10], Batch [2470/3237], Loss: 0.21070966124534607\n",
      "Epoch [5/10], Batch [2480/3237], Loss: 0.13307799398899078\n",
      "Epoch [5/10], Batch [2490/3237], Loss: 0.19692771136760712\n",
      "Epoch [5/10], Batch [2500/3237], Loss: 0.10548818111419678\n",
      "Epoch [5/10], Batch [2510/3237], Loss: 0.15842819213867188\n",
      "Epoch [5/10], Batch [2520/3237], Loss: 0.08395276218652725\n",
      "Epoch [5/10], Batch [2530/3237], Loss: 0.14433179795742035\n",
      "Epoch [5/10], Batch [2540/3237], Loss: 0.2679000794887543\n",
      "Epoch [5/10], Batch [2550/3237], Loss: 0.10183490812778473\n",
      "Epoch [5/10], Batch [2560/3237], Loss: 0.16324512660503387\n",
      "Epoch [5/10], Batch [2570/3237], Loss: 0.10673614591360092\n",
      "Epoch [5/10], Batch [2580/3237], Loss: 0.19303010404109955\n",
      "Epoch [5/10], Batch [2590/3237], Loss: 0.12345816940069199\n",
      "Epoch [5/10], Batch [2600/3237], Loss: 0.28018444776535034\n",
      "Epoch [5/10], Batch [2610/3237], Loss: 0.17353427410125732\n",
      "Epoch [5/10], Batch [2620/3237], Loss: 0.10846509039402008\n",
      "Epoch [5/10], Batch [2630/3237], Loss: 0.23352359235286713\n",
      "Epoch [5/10], Batch [2640/3237], Loss: 0.20999158918857574\n",
      "Epoch [5/10], Batch [2650/3237], Loss: 0.21944281458854675\n",
      "Epoch [5/10], Batch [2660/3237], Loss: 0.07712991535663605\n",
      "Epoch [5/10], Batch [2670/3237], Loss: 0.09098411351442337\n",
      "Epoch [5/10], Batch [2680/3237], Loss: 0.15757966041564941\n",
      "Epoch [5/10], Batch [2690/3237], Loss: 0.12015482038259506\n",
      "Epoch [5/10], Batch [2700/3237], Loss: 0.21794599294662476\n",
      "Epoch [5/10], Batch [2710/3237], Loss: 0.1644495725631714\n",
      "Epoch [5/10], Batch [2720/3237], Loss: 0.17043177783489227\n",
      "Epoch [5/10], Batch [2730/3237], Loss: 0.1679254174232483\n",
      "Epoch [5/10], Batch [2740/3237], Loss: 0.11354482173919678\n",
      "Epoch [5/10], Batch [2750/3237], Loss: 0.07762128859758377\n",
      "Epoch [5/10], Batch [2760/3237], Loss: 0.13695943355560303\n",
      "Epoch [5/10], Batch [2770/3237], Loss: 0.15462332963943481\n",
      "Epoch [5/10], Batch [2780/3237], Loss: 0.36461374163627625\n",
      "Epoch [5/10], Batch [2790/3237], Loss: 0.13811062276363373\n",
      "Epoch [5/10], Batch [2800/3237], Loss: 0.2173013985157013\n",
      "Epoch [5/10], Batch [2810/3237], Loss: 0.1415703445672989\n",
      "Epoch [5/10], Batch [2820/3237], Loss: 0.12493089586496353\n",
      "Epoch [5/10], Batch [2830/3237], Loss: 0.07869965583086014\n",
      "Epoch [5/10], Batch [2840/3237], Loss: 0.13818518817424774\n",
      "Epoch [5/10], Batch [2850/3237], Loss: 0.08635520935058594\n",
      "Epoch [5/10], Batch [2860/3237], Loss: 0.09805312752723694\n",
      "Epoch [5/10], Batch [2870/3237], Loss: 0.061562590301036835\n",
      "Epoch [5/10], Batch [2880/3237], Loss: 0.1637749820947647\n",
      "Epoch [5/10], Batch [2890/3237], Loss: 0.1851610392332077\n",
      "Epoch [5/10], Batch [2900/3237], Loss: 0.09313096851110458\n",
      "Epoch [5/10], Batch [2910/3237], Loss: 0.16335058212280273\n",
      "Epoch [5/10], Batch [2920/3237], Loss: 0.10016366839408875\n",
      "Epoch [5/10], Batch [2930/3237], Loss: 0.08206652104854584\n",
      "Epoch [5/10], Batch [2940/3237], Loss: 0.14052538573741913\n",
      "Epoch [5/10], Batch [2950/3237], Loss: 0.20027850568294525\n",
      "Epoch [5/10], Batch [2960/3237], Loss: 0.07908166944980621\n",
      "Epoch [5/10], Batch [2970/3237], Loss: 0.1737133413553238\n",
      "Epoch [5/10], Batch [2980/3237], Loss: 0.179564967751503\n",
      "Epoch [5/10], Batch [2990/3237], Loss: 0.07772664725780487\n",
      "Epoch [5/10], Batch [3000/3237], Loss: 0.12867599725723267\n",
      "Epoch [5/10], Batch [3010/3237], Loss: 0.06344752758741379\n",
      "Epoch [5/10], Batch [3020/3237], Loss: 0.2979270815849304\n",
      "Epoch [5/10], Batch [3030/3237], Loss: 0.15573211014270782\n",
      "Epoch [5/10], Batch [3040/3237], Loss: 0.12118946015834808\n",
      "Epoch [5/10], Batch [3050/3237], Loss: 0.19300591945648193\n",
      "Epoch [5/10], Batch [3060/3237], Loss: 0.22521990537643433\n",
      "Epoch [5/10], Batch [3070/3237], Loss: 0.11072961241006851\n",
      "Epoch [5/10], Batch [3080/3237], Loss: 0.11163109540939331\n",
      "Epoch [5/10], Batch [3090/3237], Loss: 0.1538579761981964\n",
      "Epoch [5/10], Batch [3100/3237], Loss: 0.13259848952293396\n",
      "Epoch [5/10], Batch [3110/3237], Loss: 0.24928191304206848\n",
      "Epoch [5/10], Batch [3120/3237], Loss: 0.12163705378770828\n",
      "Epoch [5/10], Batch [3130/3237], Loss: 0.1642567366361618\n",
      "Epoch [5/10], Batch [3140/3237], Loss: 0.5138065218925476\n",
      "Epoch [5/10], Batch [3150/3237], Loss: 0.06119067221879959\n",
      "Epoch [5/10], Batch [3160/3237], Loss: 0.13531054556369781\n",
      "Epoch [5/10], Batch [3170/3237], Loss: 0.10391294211149216\n",
      "Epoch [5/10], Batch [3180/3237], Loss: 0.07653198391199112\n",
      "Epoch [5/10], Batch [3190/3237], Loss: 0.15587711334228516\n",
      "Epoch [5/10], Batch [3200/3237], Loss: 0.14328131079673767\n",
      "Epoch [5/10], Batch [3210/3237], Loss: 0.05300304293632507\n",
      "Epoch [5/10], Batch [3220/3237], Loss: 0.09822122007608414\n",
      "Epoch [5/10], Batch [3230/3237], Loss: 0.19350600242614746\n",
      "Epoch 5, Average Train Loss: 0.1738, Average Val Loss: 0.1930\n",
      "Saved new best model at epoch 5 with Val Loss: 0.1930\n",
      "Epoch [6/10], Batch [10/3237], Loss: 0.1300991177558899\n",
      "Epoch [6/10], Batch [20/3237], Loss: 0.16359597444534302\n",
      "Epoch [6/10], Batch [30/3237], Loss: 0.11697278916835785\n",
      "Epoch [6/10], Batch [40/3237], Loss: 0.25346437096595764\n",
      "Epoch [6/10], Batch [50/3237], Loss: 0.16333504021167755\n",
      "Epoch [6/10], Batch [60/3237], Loss: 0.13656458258628845\n",
      "Epoch [6/10], Batch [70/3237], Loss: 0.11741666495800018\n",
      "Epoch [6/10], Batch [80/3237], Loss: 0.19233757257461548\n",
      "Epoch [6/10], Batch [90/3237], Loss: 0.12722288072109222\n",
      "Epoch [6/10], Batch [100/3237], Loss: 0.27249884605407715\n",
      "Epoch [6/10], Batch [110/3237], Loss: 0.10163233429193497\n",
      "Epoch [6/10], Batch [120/3237], Loss: 0.1406444013118744\n",
      "Epoch [6/10], Batch [130/3237], Loss: 0.10956529527902603\n",
      "Epoch [6/10], Batch [140/3237], Loss: 0.1938340961933136\n",
      "Epoch [6/10], Batch [150/3237], Loss: 0.13056093454360962\n",
      "Epoch [6/10], Batch [160/3237], Loss: 0.18583373725414276\n",
      "Epoch [6/10], Batch [170/3237], Loss: 0.17106179893016815\n",
      "Epoch [6/10], Batch [180/3237], Loss: 0.16885381937026978\n",
      "Epoch [6/10], Batch [190/3237], Loss: 0.10316076874732971\n",
      "Epoch [6/10], Batch [200/3237], Loss: 0.08529414981603622\n",
      "Epoch [6/10], Batch [210/3237], Loss: 0.09444316476583481\n",
      "Epoch [6/10], Batch [220/3237], Loss: 0.08589117228984833\n",
      "Epoch [6/10], Batch [230/3237], Loss: 0.19900724291801453\n",
      "Epoch [6/10], Batch [240/3237], Loss: 0.284482479095459\n",
      "Epoch [6/10], Batch [250/3237], Loss: 0.2500230669975281\n",
      "Epoch [6/10], Batch [260/3237], Loss: 0.06398091465234756\n",
      "Epoch [6/10], Batch [270/3237], Loss: 0.07857321202754974\n",
      "Epoch [6/10], Batch [280/3237], Loss: 0.09155026078224182\n",
      "Epoch [6/10], Batch [290/3237], Loss: 0.19463084638118744\n",
      "Epoch [6/10], Batch [300/3237], Loss: 0.08321806788444519\n",
      "Epoch [6/10], Batch [310/3237], Loss: 0.1659432053565979\n",
      "Epoch [6/10], Batch [320/3237], Loss: 0.1494521200656891\n",
      "Epoch [6/10], Batch [330/3237], Loss: 0.19951292872428894\n",
      "Epoch [6/10], Batch [340/3237], Loss: 0.08925248682498932\n",
      "Epoch [6/10], Batch [350/3237], Loss: 0.12724953889846802\n",
      "Epoch [6/10], Batch [360/3237], Loss: 0.1598154753446579\n",
      "Epoch [6/10], Batch [370/3237], Loss: 0.07578425109386444\n",
      "Epoch [6/10], Batch [380/3237], Loss: 0.13822881877422333\n",
      "Epoch [6/10], Batch [390/3237], Loss: 0.12382236123085022\n",
      "Epoch [6/10], Batch [400/3237], Loss: 0.10611370205879211\n",
      "Epoch [6/10], Batch [410/3237], Loss: 0.3581562042236328\n",
      "Epoch [6/10], Batch [420/3237], Loss: 0.14239008724689484\n",
      "Epoch [6/10], Batch [430/3237], Loss: 0.0893586054444313\n",
      "Epoch [6/10], Batch [440/3237], Loss: 0.10117530822753906\n",
      "Epoch [6/10], Batch [450/3237], Loss: 0.23464632034301758\n",
      "Epoch [6/10], Batch [460/3237], Loss: 0.23094260692596436\n",
      "Epoch [6/10], Batch [470/3237], Loss: 0.20423904061317444\n",
      "Epoch [6/10], Batch [480/3237], Loss: 0.15372419357299805\n",
      "Epoch [6/10], Batch [490/3237], Loss: 0.3841152787208557\n",
      "Epoch [6/10], Batch [500/3237], Loss: 0.1344379037618637\n",
      "Epoch [6/10], Batch [510/3237], Loss: 0.15417739748954773\n",
      "Epoch [6/10], Batch [520/3237], Loss: 0.3835929334163666\n",
      "Epoch [6/10], Batch [530/3237], Loss: 0.11621379852294922\n",
      "Epoch [6/10], Batch [540/3237], Loss: 0.1332000195980072\n",
      "Epoch [6/10], Batch [550/3237], Loss: 0.08779305219650269\n",
      "Epoch [6/10], Batch [560/3237], Loss: 0.05225526541471481\n",
      "Epoch [6/10], Batch [570/3237], Loss: 0.11409822106361389\n",
      "Epoch [6/10], Batch [580/3237], Loss: 0.12703464925289154\n",
      "Epoch [6/10], Batch [590/3237], Loss: 0.19075781106948853\n",
      "Epoch [6/10], Batch [600/3237], Loss: 0.14604495465755463\n",
      "Epoch [6/10], Batch [610/3237], Loss: 0.23385390639305115\n",
      "Epoch [6/10], Batch [620/3237], Loss: 0.08399105072021484\n",
      "Epoch [6/10], Batch [630/3237], Loss: 0.1179543063044548\n",
      "Epoch [6/10], Batch [640/3237], Loss: 0.16790321469306946\n",
      "Epoch [6/10], Batch [650/3237], Loss: 0.046966370195150375\n",
      "Epoch [6/10], Batch [660/3237], Loss: 0.18939238786697388\n",
      "Epoch [6/10], Batch [670/3237], Loss: 0.11273550987243652\n",
      "Epoch [6/10], Batch [680/3237], Loss: 0.09735945612192154\n",
      "Epoch [6/10], Batch [690/3237], Loss: 0.18837113678455353\n",
      "Epoch [6/10], Batch [700/3237], Loss: 0.20872338116168976\n",
      "Epoch [6/10], Batch [710/3237], Loss: 0.1601625382900238\n",
      "Epoch [6/10], Batch [720/3237], Loss: 0.08634205907583237\n",
      "Epoch [6/10], Batch [730/3237], Loss: 0.11249574273824692\n",
      "Epoch [6/10], Batch [740/3237], Loss: 0.20315974950790405\n",
      "Epoch [6/10], Batch [750/3237], Loss: 0.0929623395204544\n",
      "Epoch [6/10], Batch [760/3237], Loss: 0.3838907480239868\n",
      "Epoch [6/10], Batch [770/3237], Loss: 0.17439864575862885\n",
      "Epoch [6/10], Batch [780/3237], Loss: 0.08486276119947433\n",
      "Epoch [6/10], Batch [790/3237], Loss: 0.1761728972196579\n",
      "Epoch [6/10], Batch [800/3237], Loss: 0.07895320653915405\n",
      "Epoch [6/10], Batch [810/3237], Loss: 0.16214056313037872\n",
      "Epoch [6/10], Batch [820/3237], Loss: 0.14888814091682434\n",
      "Epoch [6/10], Batch [830/3237], Loss: 0.08326835185289383\n",
      "Epoch [6/10], Batch [840/3237], Loss: 0.28240835666656494\n",
      "Epoch [6/10], Batch [850/3237], Loss: 0.26840007305145264\n",
      "Epoch [6/10], Batch [860/3237], Loss: 0.15373623371124268\n",
      "Epoch [6/10], Batch [870/3237], Loss: 0.1421644687652588\n",
      "Epoch [6/10], Batch [880/3237], Loss: 0.18470832705497742\n",
      "Epoch [6/10], Batch [890/3237], Loss: 0.14941105246543884\n",
      "Epoch [6/10], Batch [900/3237], Loss: 0.159383624792099\n",
      "Epoch [6/10], Batch [910/3237], Loss: 0.3247816860675812\n",
      "Epoch [6/10], Batch [920/3237], Loss: 0.08675336837768555\n",
      "Epoch [6/10], Batch [930/3237], Loss: 0.2920965254306793\n",
      "Epoch [6/10], Batch [940/3237], Loss: 0.06684950739145279\n",
      "Epoch [6/10], Batch [950/3237], Loss: 0.40922579169273376\n",
      "Epoch [6/10], Batch [960/3237], Loss: 0.12554992735385895\n",
      "Epoch [6/10], Batch [970/3237], Loss: 0.2683657109737396\n",
      "Epoch [6/10], Batch [980/3237], Loss: 0.12915988266468048\n",
      "Epoch [6/10], Batch [990/3237], Loss: 0.10986366122961044\n",
      "Epoch [6/10], Batch [1000/3237], Loss: 0.0726056918501854\n",
      "Epoch [6/10], Batch [1010/3237], Loss: 0.2896898686885834\n",
      "Epoch [6/10], Batch [1020/3237], Loss: 0.09638415277004242\n",
      "Epoch [6/10], Batch [1030/3237], Loss: 0.06803659349679947\n",
      "Epoch [6/10], Batch [1040/3237], Loss: 0.1296098828315735\n",
      "Epoch [6/10], Batch [1050/3237], Loss: 0.1810026466846466\n",
      "Epoch [6/10], Batch [1060/3237], Loss: 0.16150589287281036\n",
      "Epoch [6/10], Batch [1070/3237], Loss: 0.5579772591590881\n",
      "Epoch [6/10], Batch [1080/3237], Loss: 0.148634672164917\n",
      "Epoch [6/10], Batch [1090/3237], Loss: 0.09072007983922958\n",
      "Epoch [6/10], Batch [1100/3237], Loss: 0.11518946290016174\n",
      "Epoch [6/10], Batch [1110/3237], Loss: 0.3484194874763489\n",
      "Epoch [6/10], Batch [1120/3237], Loss: 0.35609301924705505\n",
      "Epoch [6/10], Batch [1130/3237], Loss: 0.20695678889751434\n",
      "Epoch [6/10], Batch [1140/3237], Loss: 0.08611941337585449\n",
      "Epoch [6/10], Batch [1150/3237], Loss: 0.396962434053421\n",
      "Epoch [6/10], Batch [1160/3237], Loss: 0.06153915449976921\n",
      "Epoch [6/10], Batch [1170/3237], Loss: 0.0783601924777031\n",
      "Epoch [6/10], Batch [1180/3237], Loss: 0.11147203296422958\n",
      "Epoch [6/10], Batch [1190/3237], Loss: 0.06606455147266388\n",
      "Epoch [6/10], Batch [1200/3237], Loss: 0.19990214705467224\n",
      "Epoch [6/10], Batch [1210/3237], Loss: 0.42053908109664917\n",
      "Epoch [6/10], Batch [1220/3237], Loss: 0.08679676055908203\n",
      "Epoch [6/10], Batch [1230/3237], Loss: 0.0802571028470993\n",
      "Epoch [6/10], Batch [1240/3237], Loss: 0.15131425857543945\n",
      "Epoch [6/10], Batch [1250/3237], Loss: 0.10426781326532364\n",
      "Epoch [6/10], Batch [1260/3237], Loss: 0.17267604172229767\n",
      "Epoch [6/10], Batch [1270/3237], Loss: 0.4615307152271271\n",
      "Epoch [6/10], Batch [1280/3237], Loss: 0.1582665592432022\n",
      "Epoch [6/10], Batch [1290/3237], Loss: 0.09318456798791885\n",
      "Epoch [6/10], Batch [1300/3237], Loss: 0.11519524455070496\n",
      "Epoch [6/10], Batch [1310/3237], Loss: 0.12950833141803741\n",
      "Epoch [6/10], Batch [1320/3237], Loss: 0.07203429192304611\n",
      "Epoch [6/10], Batch [1330/3237], Loss: 0.23213844001293182\n",
      "Epoch [6/10], Batch [1340/3237], Loss: 0.1598529815673828\n",
      "Epoch [6/10], Batch [1350/3237], Loss: 0.22350837290287018\n",
      "Epoch [6/10], Batch [1360/3237], Loss: 0.2093620002269745\n",
      "Epoch [6/10], Batch [1370/3237], Loss: 0.10031089186668396\n",
      "Epoch [6/10], Batch [1380/3237], Loss: 0.21165522933006287\n",
      "Epoch [6/10], Batch [1390/3237], Loss: 0.23728816211223602\n",
      "Epoch [6/10], Batch [1400/3237], Loss: 0.0962277203798294\n",
      "Epoch [6/10], Batch [1410/3237], Loss: 0.13731838762760162\n",
      "Epoch [6/10], Batch [1420/3237], Loss: 0.08673074841499329\n",
      "Epoch [6/10], Batch [1430/3237], Loss: 0.146240696310997\n",
      "Epoch [6/10], Batch [1440/3237], Loss: 0.18197417259216309\n",
      "Epoch [6/10], Batch [1450/3237], Loss: 0.06328960508108139\n",
      "Epoch [6/10], Batch [1460/3237], Loss: 0.41215482354164124\n",
      "Epoch [6/10], Batch [1470/3237], Loss: 0.22024810314178467\n",
      "Epoch [6/10], Batch [1480/3237], Loss: 0.10813551396131516\n",
      "Epoch [6/10], Batch [1490/3237], Loss: 0.25180286169052124\n",
      "Epoch [6/10], Batch [1500/3237], Loss: 0.21569357812404633\n",
      "Epoch [6/10], Batch [1510/3237], Loss: 0.10168901830911636\n",
      "Epoch [6/10], Batch [1520/3237], Loss: 0.1500014215707779\n",
      "Epoch [6/10], Batch [1530/3237], Loss: 0.17984233796596527\n",
      "Epoch [6/10], Batch [1540/3237], Loss: 0.1956966519355774\n",
      "Epoch [6/10], Batch [1550/3237], Loss: 0.14042548835277557\n",
      "Epoch [6/10], Batch [1560/3237], Loss: 0.26710012555122375\n",
      "Epoch [6/10], Batch [1570/3237], Loss: 0.12572193145751953\n",
      "Epoch [6/10], Batch [1580/3237], Loss: 0.11741907149553299\n",
      "Epoch [6/10], Batch [1590/3237], Loss: 0.27551212906837463\n",
      "Epoch [6/10], Batch [1600/3237], Loss: 0.1282157152891159\n",
      "Epoch [6/10], Batch [1610/3237], Loss: 0.09711024910211563\n",
      "Epoch [6/10], Batch [1620/3237], Loss: 0.14333131909370422\n",
      "Epoch [6/10], Batch [1630/3237], Loss: 0.11000023782253265\n",
      "Epoch [6/10], Batch [1640/3237], Loss: 0.22822169959545135\n",
      "Epoch [6/10], Batch [1650/3237], Loss: 0.05988465994596481\n",
      "Epoch [6/10], Batch [1660/3237], Loss: 0.12252168357372284\n",
      "Epoch [6/10], Batch [1670/3237], Loss: 0.09423289448022842\n",
      "Epoch [6/10], Batch [1680/3237], Loss: 0.13164439797401428\n",
      "Epoch [6/10], Batch [1690/3237], Loss: 0.182707279920578\n",
      "Epoch [6/10], Batch [1700/3237], Loss: 0.10659083724021912\n",
      "Epoch [6/10], Batch [1710/3237], Loss: 0.110897958278656\n",
      "Epoch [6/10], Batch [1720/3237], Loss: 0.22014302015304565\n",
      "Epoch [6/10], Batch [1730/3237], Loss: 0.0984322801232338\n",
      "Epoch [6/10], Batch [1740/3237], Loss: 0.09274047613143921\n",
      "Epoch [6/10], Batch [1750/3237], Loss: 0.15843619406223297\n",
      "Epoch [6/10], Batch [1760/3237], Loss: 0.06713679432868958\n",
      "Epoch [6/10], Batch [1770/3237], Loss: 0.10682456940412521\n",
      "Epoch [6/10], Batch [1780/3237], Loss: 0.10511571913957596\n",
      "Epoch [6/10], Batch [1790/3237], Loss: 0.12311612814664841\n",
      "Epoch [6/10], Batch [1800/3237], Loss: 0.046168092638254166\n",
      "Epoch [6/10], Batch [1810/3237], Loss: 0.20982763171195984\n",
      "Epoch [6/10], Batch [1820/3237], Loss: 0.3173254728317261\n",
      "Epoch [6/10], Batch [1830/3237], Loss: 0.18126334249973297\n",
      "Epoch [6/10], Batch [1840/3237], Loss: 0.13046802580356598\n",
      "Epoch [6/10], Batch [1850/3237], Loss: 0.1362069696187973\n",
      "Epoch [6/10], Batch [1860/3237], Loss: 0.1202714815735817\n",
      "Epoch [6/10], Batch [1870/3237], Loss: 0.23916088044643402\n",
      "Epoch [6/10], Batch [1880/3237], Loss: 0.1926015019416809\n",
      "Epoch [6/10], Batch [1890/3237], Loss: 0.11494334787130356\n",
      "Epoch [6/10], Batch [1900/3237], Loss: 0.14858227968215942\n",
      "Epoch [6/10], Batch [1910/3237], Loss: 0.18430407345294952\n",
      "Epoch [6/10], Batch [1920/3237], Loss: 0.19886469841003418\n",
      "Epoch [6/10], Batch [1930/3237], Loss: 0.21921908855438232\n",
      "Epoch [6/10], Batch [1940/3237], Loss: 0.25260141491889954\n",
      "Epoch [6/10], Batch [1950/3237], Loss: 0.14844927191734314\n",
      "Epoch [6/10], Batch [1960/3237], Loss: 0.16388486325740814\n",
      "Epoch [6/10], Batch [1970/3237], Loss: 0.11320331692695618\n",
      "Epoch [6/10], Batch [1980/3237], Loss: 0.18622229993343353\n",
      "Epoch [6/10], Batch [1990/3237], Loss: 0.23731690645217896\n",
      "Epoch [6/10], Batch [2000/3237], Loss: 0.11027129739522934\n",
      "Epoch [6/10], Batch [2010/3237], Loss: 0.09647159278392792\n",
      "Epoch [6/10], Batch [2020/3237], Loss: 0.09713838249444962\n",
      "Epoch [6/10], Batch [2030/3237], Loss: 0.07461652904748917\n",
      "Epoch [6/10], Batch [2040/3237], Loss: 0.12897399067878723\n",
      "Epoch [6/10], Batch [2050/3237], Loss: 0.08155211061239243\n",
      "Epoch [6/10], Batch [2060/3237], Loss: 0.10988698899745941\n",
      "Epoch [6/10], Batch [2070/3237], Loss: 0.16767194867134094\n",
      "Epoch [6/10], Batch [2080/3237], Loss: 0.14381369948387146\n",
      "Epoch [6/10], Batch [2090/3237], Loss: 0.0827147588133812\n",
      "Epoch [6/10], Batch [2100/3237], Loss: 0.1291697472333908\n",
      "Epoch [6/10], Batch [2110/3237], Loss: 0.20937643945217133\n",
      "Epoch [6/10], Batch [2120/3237], Loss: 0.1446724683046341\n",
      "Epoch [6/10], Batch [2130/3237], Loss: 0.11162059754133224\n",
      "Epoch [6/10], Batch [2140/3237], Loss: 0.11409942060709\n",
      "Epoch [6/10], Batch [2150/3237], Loss: 0.3255513310432434\n",
      "Epoch [6/10], Batch [2160/3237], Loss: 0.17661266028881073\n",
      "Epoch [6/10], Batch [2170/3237], Loss: 0.1156739890575409\n",
      "Epoch [6/10], Batch [2180/3237], Loss: 0.2837466895580292\n",
      "Epoch [6/10], Batch [2190/3237], Loss: 0.15388979017734528\n",
      "Epoch [6/10], Batch [2200/3237], Loss: 0.25708112120628357\n",
      "Epoch [6/10], Batch [2210/3237], Loss: 0.10751946270465851\n",
      "Epoch [6/10], Batch [2220/3237], Loss: 0.10405942797660828\n",
      "Epoch [6/10], Batch [2230/3237], Loss: 0.34718450903892517\n",
      "Epoch [6/10], Batch [2240/3237], Loss: 0.18170592188835144\n",
      "Epoch [6/10], Batch [2250/3237], Loss: 0.13926447927951813\n",
      "Epoch [6/10], Batch [2260/3237], Loss: 0.25668784976005554\n",
      "Epoch [6/10], Batch [2270/3237], Loss: 0.08003947883844376\n",
      "Epoch [6/10], Batch [2280/3237], Loss: 0.16163752973079681\n",
      "Epoch [6/10], Batch [2290/3237], Loss: 0.22335734963417053\n",
      "Epoch [6/10], Batch [2300/3237], Loss: 0.12026412785053253\n",
      "Epoch [6/10], Batch [2310/3237], Loss: 0.15193960070610046\n",
      "Epoch [6/10], Batch [2320/3237], Loss: 0.16185353696346283\n",
      "Epoch [6/10], Batch [2330/3237], Loss: 0.11031856387853622\n",
      "Epoch [6/10], Batch [2340/3237], Loss: 0.1804322898387909\n",
      "Epoch [6/10], Batch [2350/3237], Loss: 0.06423593312501907\n",
      "Epoch [6/10], Batch [2360/3237], Loss: 0.09179528802633286\n",
      "Epoch [6/10], Batch [2370/3237], Loss: 0.10588022321462631\n",
      "Epoch [6/10], Batch [2380/3237], Loss: 0.20172050595283508\n",
      "Epoch [6/10], Batch [2390/3237], Loss: 0.06815805286169052\n",
      "Epoch [6/10], Batch [2400/3237], Loss: 0.04402273893356323\n",
      "Epoch [6/10], Batch [2410/3237], Loss: 0.14070314168930054\n",
      "Epoch [6/10], Batch [2420/3237], Loss: 0.12384700775146484\n",
      "Epoch [6/10], Batch [2430/3237], Loss: 0.11000461131334305\n",
      "Epoch [6/10], Batch [2440/3237], Loss: 0.3910457491874695\n",
      "Epoch [6/10], Batch [2450/3237], Loss: 0.10093038529157639\n",
      "Epoch [6/10], Batch [2460/3237], Loss: 0.05614316463470459\n",
      "Epoch [6/10], Batch [2470/3237], Loss: 0.2866976857185364\n",
      "Epoch [6/10], Batch [2480/3237], Loss: 0.24796251952648163\n",
      "Epoch [6/10], Batch [2490/3237], Loss: 0.17604534327983856\n",
      "Epoch [6/10], Batch [2500/3237], Loss: 0.29698848724365234\n",
      "Epoch [6/10], Batch [2510/3237], Loss: 0.06912888586521149\n",
      "Epoch [6/10], Batch [2520/3237], Loss: 0.1477649211883545\n",
      "Epoch [6/10], Batch [2530/3237], Loss: 0.21878820657730103\n",
      "Epoch [6/10], Batch [2540/3237], Loss: 0.26344889402389526\n",
      "Epoch [6/10], Batch [2550/3237], Loss: 0.1709216684103012\n",
      "Epoch [6/10], Batch [2560/3237], Loss: 0.18131235241889954\n",
      "Epoch [6/10], Batch [2570/3237], Loss: 0.07486037909984589\n",
      "Epoch [6/10], Batch [2580/3237], Loss: 0.10444758832454681\n",
      "Epoch [6/10], Batch [2590/3237], Loss: 0.30125927925109863\n",
      "Epoch [6/10], Batch [2600/3237], Loss: 0.26703718304634094\n",
      "Epoch [6/10], Batch [2610/3237], Loss: 0.1016964539885521\n",
      "Epoch [6/10], Batch [2620/3237], Loss: 0.10399062931537628\n",
      "Epoch [6/10], Batch [2630/3237], Loss: 0.18570908904075623\n",
      "Epoch [6/10], Batch [2640/3237], Loss: 0.04296223446726799\n",
      "Epoch [6/10], Batch [2650/3237], Loss: 0.1705774962902069\n",
      "Epoch [6/10], Batch [2660/3237], Loss: 0.12516194581985474\n",
      "Epoch [6/10], Batch [2670/3237], Loss: 0.13527223467826843\n",
      "Epoch [6/10], Batch [2680/3237], Loss: 0.1391414850950241\n",
      "Epoch [6/10], Batch [2690/3237], Loss: 0.35707351565361023\n",
      "Epoch [6/10], Batch [2700/3237], Loss: 0.2756427228450775\n",
      "Epoch [6/10], Batch [2710/3237], Loss: 0.17157891392707825\n",
      "Epoch [6/10], Batch [2720/3237], Loss: 0.13740481436252594\n",
      "Epoch [6/10], Batch [2730/3237], Loss: 0.167835995554924\n",
      "Epoch [6/10], Batch [2740/3237], Loss: 0.25460001826286316\n",
      "Epoch [6/10], Batch [2750/3237], Loss: 0.12511780858039856\n",
      "Epoch [6/10], Batch [2760/3237], Loss: 0.08609934896230698\n",
      "Epoch [6/10], Batch [2770/3237], Loss: 0.16377319395542145\n",
      "Epoch [6/10], Batch [2780/3237], Loss: 0.1331053078174591\n",
      "Epoch [6/10], Batch [2790/3237], Loss: 0.09147869050502777\n",
      "Epoch [6/10], Batch [2800/3237], Loss: 0.15024277567863464\n",
      "Epoch [6/10], Batch [2810/3237], Loss: 0.12684963643550873\n",
      "Epoch [6/10], Batch [2820/3237], Loss: 0.13050763309001923\n",
      "Epoch [6/10], Batch [2830/3237], Loss: 0.1323622167110443\n",
      "Epoch [6/10], Batch [2840/3237], Loss: 0.12931424379348755\n",
      "Epoch [6/10], Batch [2850/3237], Loss: 0.06507941335439682\n",
      "Epoch [6/10], Batch [2860/3237], Loss: 0.17976856231689453\n",
      "Epoch [6/10], Batch [2870/3237], Loss: 0.06789150834083557\n",
      "Epoch [6/10], Batch [2880/3237], Loss: 0.24254748225212097\n",
      "Epoch [6/10], Batch [2890/3237], Loss: 0.16242453455924988\n",
      "Epoch [6/10], Batch [2900/3237], Loss: 0.17531569302082062\n",
      "Epoch [6/10], Batch [2910/3237], Loss: 0.08635604381561279\n",
      "Epoch [6/10], Batch [2920/3237], Loss: 0.1494794636964798\n",
      "Epoch [6/10], Batch [2930/3237], Loss: 0.06909427791833878\n",
      "Epoch [6/10], Batch [2940/3237], Loss: 0.09979592263698578\n",
      "Epoch [6/10], Batch [2950/3237], Loss: 0.06232937052845955\n",
      "Epoch [6/10], Batch [2960/3237], Loss: 0.10269704461097717\n",
      "Epoch [6/10], Batch [2970/3237], Loss: 0.15240298211574554\n",
      "Epoch [6/10], Batch [2980/3237], Loss: 0.0887598991394043\n",
      "Epoch [6/10], Batch [2990/3237], Loss: 0.30586105585098267\n",
      "Epoch [6/10], Batch [3000/3237], Loss: 0.11299382895231247\n",
      "Epoch [6/10], Batch [3010/3237], Loss: 0.17701011896133423\n",
      "Epoch [6/10], Batch [3020/3237], Loss: 0.14608636498451233\n",
      "Epoch [6/10], Batch [3030/3237], Loss: 0.10816533863544464\n",
      "Epoch [6/10], Batch [3040/3237], Loss: 0.1112481951713562\n",
      "Epoch [6/10], Batch [3050/3237], Loss: 0.32777419686317444\n",
      "Epoch [6/10], Batch [3060/3237], Loss: 0.25040575861930847\n",
      "Epoch [6/10], Batch [3070/3237], Loss: 0.18451140820980072\n",
      "Epoch [6/10], Batch [3080/3237], Loss: 0.2938787639141083\n",
      "Epoch [6/10], Batch [3090/3237], Loss: 0.43394264578819275\n",
      "Epoch [6/10], Batch [3100/3237], Loss: 0.2577221691608429\n",
      "Epoch [6/10], Batch [3110/3237], Loss: 0.12020693719387054\n",
      "Epoch [6/10], Batch [3120/3237], Loss: 0.16672170162200928\n",
      "Epoch [6/10], Batch [3130/3237], Loss: 0.16290250420570374\n",
      "Epoch [6/10], Batch [3140/3237], Loss: 0.14502570033073425\n",
      "Epoch [6/10], Batch [3150/3237], Loss: 0.10770919919013977\n",
      "Epoch [6/10], Batch [3160/3237], Loss: 0.17318803071975708\n",
      "Epoch [6/10], Batch [3170/3237], Loss: 0.11193244904279709\n",
      "Epoch [6/10], Batch [3180/3237], Loss: 0.17379014194011688\n",
      "Epoch [6/10], Batch [3190/3237], Loss: 0.0813814252614975\n",
      "Epoch [6/10], Batch [3200/3237], Loss: 0.16510754823684692\n",
      "Epoch [6/10], Batch [3210/3237], Loss: 0.07663989812135696\n",
      "Epoch [6/10], Batch [3220/3237], Loss: 0.1823616772890091\n",
      "Epoch [6/10], Batch [3230/3237], Loss: 0.21691998839378357\n",
      "Epoch 6, Average Train Loss: 0.1632, Average Val Loss: 0.1851\n",
      "Saved new best model at epoch 6 with Val Loss: 0.1851\n",
      "Epoch [7/10], Batch [10/3237], Loss: 0.09757125377655029\n",
      "Epoch [7/10], Batch [20/3237], Loss: 0.24761179089546204\n",
      "Epoch [7/10], Batch [30/3237], Loss: 0.17914491891860962\n",
      "Epoch [7/10], Batch [40/3237], Loss: 0.21666759252548218\n",
      "Epoch [7/10], Batch [50/3237], Loss: 0.17311477661132812\n",
      "Epoch [7/10], Batch [60/3237], Loss: 0.11308804899454117\n",
      "Epoch [7/10], Batch [70/3237], Loss: 0.10178083181381226\n",
      "Epoch [7/10], Batch [80/3237], Loss: 0.17661906778812408\n",
      "Epoch [7/10], Batch [90/3237], Loss: 0.06720547378063202\n",
      "Epoch [7/10], Batch [100/3237], Loss: 0.09312275052070618\n",
      "Epoch [7/10], Batch [110/3237], Loss: 0.06739199161529541\n",
      "Epoch [7/10], Batch [120/3237], Loss: 0.08430209755897522\n",
      "Epoch [7/10], Batch [130/3237], Loss: 0.06940463185310364\n",
      "Epoch [7/10], Batch [140/3237], Loss: 0.156962051987648\n",
      "Epoch [7/10], Batch [150/3237], Loss: 0.32173264026641846\n",
      "Epoch [7/10], Batch [160/3237], Loss: 0.17022746801376343\n",
      "Epoch [7/10], Batch [170/3237], Loss: 0.17204086482524872\n",
      "Epoch [7/10], Batch [180/3237], Loss: 0.16066356003284454\n",
      "Epoch [7/10], Batch [190/3237], Loss: 0.1978570967912674\n",
      "Epoch [7/10], Batch [200/3237], Loss: 0.1417449563741684\n",
      "Epoch [7/10], Batch [210/3237], Loss: 0.17320704460144043\n",
      "Epoch [7/10], Batch [220/3237], Loss: 0.06852064281702042\n",
      "Epoch [7/10], Batch [230/3237], Loss: 0.568086564540863\n",
      "Epoch [7/10], Batch [240/3237], Loss: 0.11144240945577621\n",
      "Epoch [7/10], Batch [250/3237], Loss: 0.11162751913070679\n",
      "Epoch [7/10], Batch [260/3237], Loss: 0.295841783285141\n",
      "Epoch [7/10], Batch [270/3237], Loss: 0.2409496009349823\n",
      "Epoch [7/10], Batch [280/3237], Loss: 0.3222816288471222\n",
      "Epoch [7/10], Batch [290/3237], Loss: 0.27024516463279724\n",
      "Epoch [7/10], Batch [300/3237], Loss: 0.13254764676094055\n",
      "Epoch [7/10], Batch [310/3237], Loss: 0.06425119191408157\n",
      "Epoch [7/10], Batch [320/3237], Loss: 0.30647072196006775\n",
      "Epoch [7/10], Batch [330/3237], Loss: 0.13809193670749664\n",
      "Epoch [7/10], Batch [340/3237], Loss: 0.268324077129364\n",
      "Epoch [7/10], Batch [350/3237], Loss: 0.12144041061401367\n",
      "Epoch [7/10], Batch [360/3237], Loss: 0.07274871319532394\n",
      "Epoch [7/10], Batch [370/3237], Loss: 0.1767507940530777\n",
      "Epoch [7/10], Batch [380/3237], Loss: 0.3251878321170807\n",
      "Epoch [7/10], Batch [390/3237], Loss: 0.06629065424203873\n",
      "Epoch [7/10], Batch [400/3237], Loss: 0.1974559724330902\n",
      "Epoch [7/10], Batch [410/3237], Loss: 0.15102897584438324\n",
      "Epoch [7/10], Batch [420/3237], Loss: 0.20100119709968567\n",
      "Epoch [7/10], Batch [430/3237], Loss: 0.31652018427848816\n",
      "Epoch [7/10], Batch [440/3237], Loss: 0.1727581024169922\n",
      "Epoch [7/10], Batch [450/3237], Loss: 0.07846828550100327\n",
      "Epoch [7/10], Batch [460/3237], Loss: 0.09500818699598312\n",
      "Epoch [7/10], Batch [470/3237], Loss: 0.08300846815109253\n",
      "Epoch [7/10], Batch [480/3237], Loss: 0.20718564093112946\n",
      "Epoch [7/10], Batch [490/3237], Loss: 0.087567038834095\n",
      "Epoch [7/10], Batch [500/3237], Loss: 0.14999167621135712\n",
      "Epoch [7/10], Batch [510/3237], Loss: 0.09263662248849869\n",
      "Epoch [7/10], Batch [520/3237], Loss: 0.10266526788473129\n",
      "Epoch [7/10], Batch [530/3237], Loss: 0.12386395782232285\n",
      "Epoch [7/10], Batch [540/3237], Loss: 0.07182386517524719\n",
      "Epoch [7/10], Batch [550/3237], Loss: 0.08448359370231628\n",
      "Epoch [7/10], Batch [560/3237], Loss: 0.18127013742923737\n",
      "Epoch [7/10], Batch [570/3237], Loss: 0.0953742042183876\n",
      "Epoch [7/10], Batch [580/3237], Loss: 0.08277080208063126\n",
      "Epoch [7/10], Batch [590/3237], Loss: 0.13231542706489563\n",
      "Epoch [7/10], Batch [600/3237], Loss: 0.179317444562912\n",
      "Epoch [7/10], Batch [610/3237], Loss: 0.26193466782569885\n",
      "Epoch [7/10], Batch [620/3237], Loss: 0.17850489914417267\n",
      "Epoch [7/10], Batch [630/3237], Loss: 0.1515904814004898\n",
      "Epoch [7/10], Batch [640/3237], Loss: 0.1071360632777214\n",
      "Epoch [7/10], Batch [650/3237], Loss: 0.10434835404157639\n",
      "Epoch [7/10], Batch [660/3237], Loss: 0.10470965504646301\n",
      "Epoch [7/10], Batch [670/3237], Loss: 0.10881467908620834\n",
      "Epoch [7/10], Batch [680/3237], Loss: 0.11652988195419312\n",
      "Epoch [7/10], Batch [690/3237], Loss: 0.09309465438127518\n",
      "Epoch [7/10], Batch [700/3237], Loss: 0.21283027529716492\n",
      "Epoch [7/10], Batch [710/3237], Loss: 0.08054983615875244\n",
      "Epoch [7/10], Batch [720/3237], Loss: 0.06051880866289139\n",
      "Epoch [7/10], Batch [730/3237], Loss: 0.14516900479793549\n",
      "Epoch [7/10], Batch [740/3237], Loss: 0.1399199515581131\n",
      "Epoch [7/10], Batch [750/3237], Loss: 0.12246709316968918\n",
      "Epoch [7/10], Batch [760/3237], Loss: 0.23037470877170563\n",
      "Epoch [7/10], Batch [770/3237], Loss: 0.07329390943050385\n",
      "Epoch [7/10], Batch [780/3237], Loss: 0.11011111736297607\n",
      "Epoch [7/10], Batch [790/3237], Loss: 0.17282924056053162\n",
      "Epoch [7/10], Batch [800/3237], Loss: 0.09667080640792847\n",
      "Epoch [7/10], Batch [810/3237], Loss: 0.12284422665834427\n",
      "Epoch [7/10], Batch [820/3237], Loss: 0.12339773029088974\n",
      "Epoch [7/10], Batch [830/3237], Loss: 0.13073216378688812\n",
      "Epoch [7/10], Batch [840/3237], Loss: 0.19017238914966583\n",
      "Epoch [7/10], Batch [850/3237], Loss: 0.12272621691226959\n",
      "Epoch [7/10], Batch [860/3237], Loss: 0.15297819674015045\n",
      "Epoch [7/10], Batch [870/3237], Loss: 0.03920124098658562\n",
      "Epoch [7/10], Batch [880/3237], Loss: 0.1128716841340065\n",
      "Epoch [7/10], Batch [890/3237], Loss: 0.10158256441354752\n",
      "Epoch [7/10], Batch [900/3237], Loss: 0.08181191980838776\n",
      "Epoch [7/10], Batch [910/3237], Loss: 0.1580927073955536\n",
      "Epoch [7/10], Batch [920/3237], Loss: 0.13970589637756348\n",
      "Epoch [7/10], Batch [930/3237], Loss: 0.13642413914203644\n",
      "Epoch [7/10], Batch [940/3237], Loss: 0.16067180037498474\n",
      "Epoch [7/10], Batch [950/3237], Loss: 0.19809988141059875\n",
      "Epoch [7/10], Batch [960/3237], Loss: 0.14531546831130981\n",
      "Epoch [7/10], Batch [970/3237], Loss: 0.1576729118824005\n",
      "Epoch [7/10], Batch [980/3237], Loss: 0.23220767080783844\n",
      "Epoch [7/10], Batch [990/3237], Loss: 0.22242233157157898\n",
      "Epoch [7/10], Batch [1000/3237], Loss: 0.16492274403572083\n",
      "Epoch [7/10], Batch [1010/3237], Loss: 0.2126302868127823\n",
      "Epoch [7/10], Batch [1020/3237], Loss: 0.15857109427452087\n",
      "Epoch [7/10], Batch [1030/3237], Loss: 0.13047346472740173\n",
      "Epoch [7/10], Batch [1040/3237], Loss: 0.12516427040100098\n",
      "Epoch [7/10], Batch [1050/3237], Loss: 0.08321048319339752\n",
      "Epoch [7/10], Batch [1060/3237], Loss: 0.06879586726427078\n",
      "Epoch [7/10], Batch [1070/3237], Loss: 0.14551740884780884\n",
      "Epoch [7/10], Batch [1080/3237], Loss: 0.2701253592967987\n",
      "Epoch [7/10], Batch [1090/3237], Loss: 0.17131167650222778\n",
      "Epoch [7/10], Batch [1100/3237], Loss: 0.1703769862651825\n",
      "Epoch [7/10], Batch [1110/3237], Loss: 0.19304125010967255\n",
      "Epoch [7/10], Batch [1120/3237], Loss: 0.12885312736034393\n",
      "Epoch [7/10], Batch [1130/3237], Loss: 0.09246686846017838\n",
      "Epoch [7/10], Batch [1140/3237], Loss: 0.1115485355257988\n",
      "Epoch [7/10], Batch [1150/3237], Loss: 0.11072094738483429\n",
      "Epoch [7/10], Batch [1160/3237], Loss: 0.23309069871902466\n",
      "Epoch [7/10], Batch [1170/3237], Loss: 0.09573045372962952\n",
      "Epoch [7/10], Batch [1180/3237], Loss: 0.07819440960884094\n",
      "Epoch [7/10], Batch [1190/3237], Loss: 0.15489308536052704\n",
      "Epoch [7/10], Batch [1200/3237], Loss: 0.12590576708316803\n",
      "Epoch [7/10], Batch [1210/3237], Loss: 0.11724930256605148\n",
      "Epoch [7/10], Batch [1220/3237], Loss: 0.31849783658981323\n",
      "Epoch [7/10], Batch [1230/3237], Loss: 0.19895832240581512\n",
      "Epoch [7/10], Batch [1240/3237], Loss: 0.1898839920759201\n",
      "Epoch [7/10], Batch [1250/3237], Loss: 0.08558137714862823\n",
      "Epoch [7/10], Batch [1260/3237], Loss: 0.1498177945613861\n",
      "Epoch [7/10], Batch [1270/3237], Loss: 0.11207745969295502\n",
      "Epoch [7/10], Batch [1280/3237], Loss: 0.08364273607730865\n",
      "Epoch [7/10], Batch [1290/3237], Loss: 0.14757658541202545\n",
      "Epoch [7/10], Batch [1300/3237], Loss: 0.13736383616924286\n",
      "Epoch [7/10], Batch [1310/3237], Loss: 0.21124553680419922\n",
      "Epoch [7/10], Batch [1320/3237], Loss: 0.14336784183979034\n",
      "Epoch [7/10], Batch [1330/3237], Loss: 0.15712836384773254\n",
      "Epoch [7/10], Batch [1340/3237], Loss: 0.16112086176872253\n",
      "Epoch [7/10], Batch [1350/3237], Loss: 0.24126707017421722\n",
      "Epoch [7/10], Batch [1360/3237], Loss: 0.1793607920408249\n",
      "Epoch [7/10], Batch [1370/3237], Loss: 0.1258832961320877\n",
      "Epoch [7/10], Batch [1380/3237], Loss: 0.18240699172019958\n",
      "Epoch [7/10], Batch [1390/3237], Loss: 0.12758579850196838\n",
      "Epoch [7/10], Batch [1400/3237], Loss: 0.19704574346542358\n",
      "Epoch [7/10], Batch [1410/3237], Loss: 0.3257548213005066\n",
      "Epoch [7/10], Batch [1420/3237], Loss: 0.10694606602191925\n",
      "Epoch [7/10], Batch [1430/3237], Loss: 0.15306632220745087\n",
      "Epoch [7/10], Batch [1440/3237], Loss: 0.18791364133358002\n",
      "Epoch [7/10], Batch [1450/3237], Loss: 0.16676755249500275\n",
      "Epoch [7/10], Batch [1460/3237], Loss: 0.1215779110789299\n",
      "Epoch [7/10], Batch [1470/3237], Loss: 0.09299671649932861\n",
      "Epoch [7/10], Batch [1480/3237], Loss: 0.045858852565288544\n",
      "Epoch [7/10], Batch [1490/3237], Loss: 0.12456919252872467\n",
      "Epoch [7/10], Batch [1500/3237], Loss: 0.19246666133403778\n",
      "Epoch [7/10], Batch [1510/3237], Loss: 0.11839592456817627\n",
      "Epoch [7/10], Batch [1520/3237], Loss: 0.1477564573287964\n",
      "Epoch [7/10], Batch [1530/3237], Loss: 0.14615954458713531\n",
      "Epoch [7/10], Batch [1540/3237], Loss: 0.20348387956619263\n",
      "Epoch [7/10], Batch [1550/3237], Loss: 0.3144787549972534\n",
      "Epoch [7/10], Batch [1560/3237], Loss: 0.05639318376779556\n",
      "Epoch [7/10], Batch [1570/3237], Loss: 0.13918925821781158\n",
      "Epoch [7/10], Batch [1580/3237], Loss: 0.17504149675369263\n",
      "Epoch [7/10], Batch [1590/3237], Loss: 0.12509557604789734\n",
      "Epoch [7/10], Batch [1600/3237], Loss: 0.04993269965052605\n",
      "Epoch [7/10], Batch [1610/3237], Loss: 0.144881471991539\n",
      "Epoch [7/10], Batch [1620/3237], Loss: 0.14835867285728455\n",
      "Epoch [7/10], Batch [1630/3237], Loss: 0.08702994883060455\n",
      "Epoch [7/10], Batch [1640/3237], Loss: 0.18109500408172607\n",
      "Epoch [7/10], Batch [1650/3237], Loss: 0.09446107596158981\n",
      "Epoch [7/10], Batch [1660/3237], Loss: 0.09319949895143509\n",
      "Epoch [7/10], Batch [1670/3237], Loss: 0.09281645715236664\n",
      "Epoch [7/10], Batch [1680/3237], Loss: 0.12273260205984116\n",
      "Epoch [7/10], Batch [1690/3237], Loss: 0.13188478350639343\n",
      "Epoch [7/10], Batch [1700/3237], Loss: 0.07091883569955826\n",
      "Epoch [7/10], Batch [1710/3237], Loss: 0.14988455176353455\n",
      "Epoch [7/10], Batch [1720/3237], Loss: 0.11733534187078476\n",
      "Epoch [7/10], Batch [1730/3237], Loss: 0.09864173829555511\n",
      "Epoch [7/10], Batch [1740/3237], Loss: 0.0976904109120369\n",
      "Epoch [7/10], Batch [1750/3237], Loss: 0.13873285055160522\n",
      "Epoch [7/10], Batch [1760/3237], Loss: 0.17700621485710144\n",
      "Epoch [7/10], Batch [1770/3237], Loss: 0.08717942982912064\n",
      "Epoch [7/10], Batch [1780/3237], Loss: 0.0915796086192131\n",
      "Epoch [7/10], Batch [1790/3237], Loss: 0.09625762701034546\n",
      "Epoch [7/10], Batch [1800/3237], Loss: 0.1896010935306549\n",
      "Epoch [7/10], Batch [1810/3237], Loss: 0.13524393737316132\n",
      "Epoch [7/10], Batch [1820/3237], Loss: 0.05424077436327934\n",
      "Epoch [7/10], Batch [1830/3237], Loss: 0.16453948616981506\n",
      "Epoch [7/10], Batch [1840/3237], Loss: 0.1075994148850441\n",
      "Epoch [7/10], Batch [1850/3237], Loss: 0.18923301994800568\n",
      "Epoch [7/10], Batch [1860/3237], Loss: 0.15589511394500732\n",
      "Epoch [7/10], Batch [1870/3237], Loss: 0.1774367094039917\n",
      "Epoch [7/10], Batch [1880/3237], Loss: 0.14539863169193268\n",
      "Epoch [7/10], Batch [1890/3237], Loss: 0.12014205753803253\n",
      "Epoch [7/10], Batch [1900/3237], Loss: 0.1048702597618103\n",
      "Epoch [7/10], Batch [1910/3237], Loss: 0.2630126178264618\n",
      "Epoch [7/10], Batch [1920/3237], Loss: 0.0889853835105896\n",
      "Epoch [7/10], Batch [1930/3237], Loss: 0.27535295486450195\n",
      "Epoch [7/10], Batch [1940/3237], Loss: 0.06307294964790344\n",
      "Epoch [7/10], Batch [1950/3237], Loss: 0.15856267511844635\n",
      "Epoch [7/10], Batch [1960/3237], Loss: 0.2945500910282135\n",
      "Epoch [7/10], Batch [1970/3237], Loss: 0.10820072889328003\n",
      "Epoch [7/10], Batch [1980/3237], Loss: 0.10637205094099045\n",
      "Epoch [7/10], Batch [1990/3237], Loss: 0.14517834782600403\n",
      "Epoch [7/10], Batch [2000/3237], Loss: 0.1882765144109726\n",
      "Epoch [7/10], Batch [2010/3237], Loss: 0.1439971625804901\n",
      "Epoch [7/10], Batch [2020/3237], Loss: 0.0922614112496376\n",
      "Epoch [7/10], Batch [2030/3237], Loss: 0.09867530316114426\n",
      "Epoch [7/10], Batch [2040/3237], Loss: 0.1337192952632904\n",
      "Epoch [7/10], Batch [2050/3237], Loss: 0.11465757340192795\n",
      "Epoch [7/10], Batch [2060/3237], Loss: 0.28811269998550415\n",
      "Epoch [7/10], Batch [2070/3237], Loss: 0.11530568450689316\n",
      "Epoch [7/10], Batch [2080/3237], Loss: 0.06084216386079788\n",
      "Epoch [7/10], Batch [2090/3237], Loss: 0.08230619132518768\n",
      "Epoch [7/10], Batch [2100/3237], Loss: 0.19153518974781036\n",
      "Epoch [7/10], Batch [2110/3237], Loss: 0.1822635978460312\n",
      "Epoch [7/10], Batch [2120/3237], Loss: 0.06331156194210052\n",
      "Epoch [7/10], Batch [2130/3237], Loss: 0.1739245504140854\n",
      "Epoch [7/10], Batch [2140/3237], Loss: 0.1368616670370102\n",
      "Epoch [7/10], Batch [2150/3237], Loss: 0.13202530145645142\n",
      "Epoch [7/10], Batch [2160/3237], Loss: 0.05643853917717934\n",
      "Epoch [7/10], Batch [2170/3237], Loss: 0.07136687636375427\n",
      "Epoch [7/10], Batch [2180/3237], Loss: 0.044533804059028625\n",
      "Epoch [7/10], Batch [2190/3237], Loss: 0.1566561460494995\n",
      "Epoch [7/10], Batch [2200/3237], Loss: 0.05461946874856949\n",
      "Epoch [7/10], Batch [2210/3237], Loss: 0.21643075346946716\n",
      "Epoch [7/10], Batch [2220/3237], Loss: 0.28121212124824524\n",
      "Epoch [7/10], Batch [2230/3237], Loss: 0.18796226382255554\n",
      "Epoch [7/10], Batch [2240/3237], Loss: 0.06350884586572647\n",
      "Epoch [7/10], Batch [2250/3237], Loss: 0.06609444320201874\n",
      "Epoch [7/10], Batch [2260/3237], Loss: 0.12115287780761719\n",
      "Epoch [7/10], Batch [2270/3237], Loss: 0.10055208951234818\n",
      "Epoch [7/10], Batch [2280/3237], Loss: 0.12171008437871933\n",
      "Epoch [7/10], Batch [2290/3237], Loss: 0.11735145002603531\n",
      "Epoch [7/10], Batch [2300/3237], Loss: 0.0762544721364975\n",
      "Epoch [7/10], Batch [2310/3237], Loss: 0.1331334263086319\n",
      "Epoch [7/10], Batch [2320/3237], Loss: 0.1662018895149231\n",
      "Epoch [7/10], Batch [2330/3237], Loss: 0.1527058333158493\n",
      "Epoch [7/10], Batch [2340/3237], Loss: 0.14189648628234863\n",
      "Epoch [7/10], Batch [2350/3237], Loss: 0.12356286495923996\n",
      "Epoch [7/10], Batch [2360/3237], Loss: 0.05774417892098427\n",
      "Epoch [7/10], Batch [2370/3237], Loss: 0.14072951674461365\n",
      "Epoch [7/10], Batch [2380/3237], Loss: 0.09114014357328415\n",
      "Epoch [7/10], Batch [2390/3237], Loss: 0.08729033917188644\n",
      "Epoch [7/10], Batch [2400/3237], Loss: 0.16714435815811157\n",
      "Epoch [7/10], Batch [2410/3237], Loss: 0.1154593750834465\n",
      "Epoch [7/10], Batch [2420/3237], Loss: 0.12047212570905685\n",
      "Epoch [7/10], Batch [2430/3237], Loss: 0.037641339004039764\n",
      "Epoch [7/10], Batch [2440/3237], Loss: 0.14304082095623016\n",
      "Epoch [7/10], Batch [2450/3237], Loss: 0.14087559282779694\n",
      "Epoch [7/10], Batch [2460/3237], Loss: 0.17191706597805023\n",
      "Epoch [7/10], Batch [2470/3237], Loss: 0.271576464176178\n",
      "Epoch [7/10], Batch [2480/3237], Loss: 0.08344217389822006\n",
      "Epoch [7/10], Batch [2490/3237], Loss: 0.09599475562572479\n",
      "Epoch [7/10], Batch [2500/3237], Loss: 0.11323274672031403\n",
      "Epoch [7/10], Batch [2510/3237], Loss: 0.09803649038076401\n",
      "Epoch [7/10], Batch [2520/3237], Loss: 0.1450813114643097\n",
      "Epoch [7/10], Batch [2530/3237], Loss: 0.20097243785858154\n",
      "Epoch [7/10], Batch [2540/3237], Loss: 0.24000297486782074\n",
      "Epoch [7/10], Batch [2550/3237], Loss: 0.10173829644918442\n",
      "Epoch [7/10], Batch [2560/3237], Loss: 0.23856334388256073\n",
      "Epoch [7/10], Batch [2570/3237], Loss: 0.17105385661125183\n",
      "Epoch [7/10], Batch [2580/3237], Loss: 0.07207652181386948\n",
      "Epoch [7/10], Batch [2590/3237], Loss: 0.21239185333251953\n",
      "Epoch [7/10], Batch [2600/3237], Loss: 0.2362058162689209\n",
      "Epoch [7/10], Batch [2610/3237], Loss: 0.11838217824697495\n",
      "Epoch [7/10], Batch [2620/3237], Loss: 0.20338858664035797\n",
      "Epoch [7/10], Batch [2630/3237], Loss: 0.09695611894130707\n",
      "Epoch [7/10], Batch [2640/3237], Loss: 0.18387286365032196\n",
      "Epoch [7/10], Batch [2650/3237], Loss: 0.18183298408985138\n",
      "Epoch [7/10], Batch [2660/3237], Loss: 0.1797112226486206\n",
      "Epoch [7/10], Batch [2670/3237], Loss: 0.1334860622882843\n",
      "Epoch [7/10], Batch [2680/3237], Loss: 0.10110163688659668\n",
      "Epoch [7/10], Batch [2690/3237], Loss: 0.0825771912932396\n",
      "Epoch [7/10], Batch [2700/3237], Loss: 0.13428527116775513\n",
      "Epoch [7/10], Batch [2710/3237], Loss: 0.08158323913812637\n",
      "Epoch [7/10], Batch [2720/3237], Loss: 0.17953628301620483\n",
      "Epoch [7/10], Batch [2730/3237], Loss: 0.21328982710838318\n",
      "Epoch [7/10], Batch [2740/3237], Loss: 0.08440620452165604\n",
      "Epoch [7/10], Batch [2750/3237], Loss: 0.12888629734516144\n",
      "Epoch [7/10], Batch [2760/3237], Loss: 0.035867635160684586\n",
      "Epoch [7/10], Batch [2770/3237], Loss: 0.14162477850914001\n",
      "Epoch [7/10], Batch [2780/3237], Loss: 0.18206627666950226\n",
      "Epoch [7/10], Batch [2790/3237], Loss: 0.1391008049249649\n",
      "Epoch [7/10], Batch [2800/3237], Loss: 0.16525816917419434\n",
      "Epoch [7/10], Batch [2810/3237], Loss: 0.06760768592357635\n",
      "Epoch [7/10], Batch [2820/3237], Loss: 0.09877096116542816\n",
      "Epoch [7/10], Batch [2830/3237], Loss: 0.2797068655490875\n",
      "Epoch [7/10], Batch [2840/3237], Loss: 0.24541045725345612\n",
      "Epoch [7/10], Batch [2850/3237], Loss: 0.2755937874317169\n",
      "Epoch [7/10], Batch [2860/3237], Loss: 0.11298739165067673\n",
      "Epoch [7/10], Batch [2870/3237], Loss: 0.4014066159725189\n",
      "Epoch [7/10], Batch [2880/3237], Loss: 0.2226281762123108\n",
      "Epoch [7/10], Batch [2890/3237], Loss: 0.13600517809391022\n",
      "Epoch [7/10], Batch [2900/3237], Loss: 0.1802826225757599\n",
      "Epoch [7/10], Batch [2910/3237], Loss: 0.09111254662275314\n",
      "Epoch [7/10], Batch [2920/3237], Loss: 0.06271464377641678\n",
      "Epoch [7/10], Batch [2930/3237], Loss: 0.21309202909469604\n",
      "Epoch [7/10], Batch [2940/3237], Loss: 0.16030092537403107\n",
      "Epoch [7/10], Batch [2950/3237], Loss: 0.07706595957279205\n",
      "Epoch [7/10], Batch [2960/3237], Loss: 0.18282321095466614\n",
      "Epoch [7/10], Batch [2970/3237], Loss: 0.09171745181083679\n",
      "Epoch [7/10], Batch [2980/3237], Loss: 0.11019284278154373\n",
      "Epoch [7/10], Batch [2990/3237], Loss: 0.2178139090538025\n",
      "Epoch [7/10], Batch [3000/3237], Loss: 0.20573699474334717\n",
      "Epoch [7/10], Batch [3010/3237], Loss: 0.08820565789937973\n",
      "Epoch [7/10], Batch [3020/3237], Loss: 0.2146337330341339\n",
      "Epoch [7/10], Batch [3030/3237], Loss: 0.1765240877866745\n",
      "Epoch [7/10], Batch [3040/3237], Loss: 0.09346465766429901\n",
      "Epoch [7/10], Batch [3050/3237], Loss: 0.10272977501153946\n",
      "Epoch [7/10], Batch [3060/3237], Loss: 0.1067998856306076\n",
      "Epoch [7/10], Batch [3070/3237], Loss: 0.30861696600914\n",
      "Epoch [7/10], Batch [3080/3237], Loss: 0.15386849641799927\n",
      "Epoch [7/10], Batch [3090/3237], Loss: 0.12649652361869812\n",
      "Epoch [7/10], Batch [3100/3237], Loss: 0.10542178153991699\n",
      "Epoch [7/10], Batch [3110/3237], Loss: 0.2453579157590866\n",
      "Epoch [7/10], Batch [3120/3237], Loss: 0.2143852412700653\n",
      "Epoch [7/10], Batch [3130/3237], Loss: 0.06638513505458832\n",
      "Epoch [7/10], Batch [3140/3237], Loss: 0.08912788331508636\n",
      "Epoch [7/10], Batch [3150/3237], Loss: 0.13931068778038025\n",
      "Epoch [7/10], Batch [3160/3237], Loss: 0.44567564129829407\n",
      "Epoch [7/10], Batch [3170/3237], Loss: 0.05864990875124931\n",
      "Epoch [7/10], Batch [3180/3237], Loss: 0.16352781653404236\n",
      "Epoch [7/10], Batch [3190/3237], Loss: 0.08005895465612411\n",
      "Epoch [7/10], Batch [3200/3237], Loss: 0.1852038949728012\n",
      "Epoch [7/10], Batch [3210/3237], Loss: 0.218383327126503\n",
      "Epoch [7/10], Batch [3220/3237], Loss: 0.24820634722709656\n",
      "Epoch [7/10], Batch [3230/3237], Loss: 0.23273421823978424\n",
      "Epoch 7, Average Train Loss: 0.1556, Average Val Loss: 0.1760\n",
      "Saved new best model at epoch 7 with Val Loss: 0.1760\n",
      "Epoch [8/10], Batch [10/3237], Loss: 0.2463115006685257\n",
      "Epoch [8/10], Batch [20/3237], Loss: 0.10885734111070633\n",
      "Epoch [8/10], Batch [30/3237], Loss: 0.15096336603164673\n",
      "Epoch [8/10], Batch [40/3237], Loss: 0.14568175375461578\n",
      "Epoch [8/10], Batch [50/3237], Loss: 0.36133894324302673\n",
      "Epoch [8/10], Batch [60/3237], Loss: 0.225753054022789\n",
      "Epoch [8/10], Batch [70/3237], Loss: 0.13659948110580444\n",
      "Epoch [8/10], Batch [80/3237], Loss: 0.19339217245578766\n",
      "Epoch [8/10], Batch [90/3237], Loss: 0.15031054615974426\n",
      "Epoch [8/10], Batch [100/3237], Loss: 0.07648202031850815\n",
      "Epoch [8/10], Batch [110/3237], Loss: 0.14045080542564392\n",
      "Epoch [8/10], Batch [120/3237], Loss: 0.1276368945837021\n",
      "Epoch [8/10], Batch [130/3237], Loss: 0.07760917395353317\n",
      "Epoch [8/10], Batch [140/3237], Loss: 0.07824645191431046\n",
      "Epoch [8/10], Batch [150/3237], Loss: 0.10540200024843216\n",
      "Epoch [8/10], Batch [160/3237], Loss: 0.1395920217037201\n",
      "Epoch [8/10], Batch [170/3237], Loss: 0.09697145968675613\n",
      "Epoch [8/10], Batch [180/3237], Loss: 0.11661039292812347\n",
      "Epoch [8/10], Batch [190/3237], Loss: 0.10732351988554001\n",
      "Epoch [8/10], Batch [200/3237], Loss: 0.1367121785879135\n",
      "Epoch [8/10], Batch [210/3237], Loss: 0.1825518012046814\n",
      "Epoch [8/10], Batch [220/3237], Loss: 0.225498229265213\n",
      "Epoch [8/10], Batch [230/3237], Loss: 0.18497306108474731\n",
      "Epoch [8/10], Batch [240/3237], Loss: 0.18019665777683258\n",
      "Epoch [8/10], Batch [250/3237], Loss: 0.07980722934007645\n",
      "Epoch [8/10], Batch [260/3237], Loss: 0.07826601713895798\n",
      "Epoch [8/10], Batch [270/3237], Loss: 0.10338984429836273\n",
      "Epoch [8/10], Batch [280/3237], Loss: 0.12626193463802338\n",
      "Epoch [8/10], Batch [290/3237], Loss: 0.21144677698612213\n",
      "Epoch [8/10], Batch [300/3237], Loss: 0.12511926889419556\n",
      "Epoch [8/10], Batch [310/3237], Loss: 0.13669554889202118\n",
      "Epoch [8/10], Batch [320/3237], Loss: 0.16444365680217743\n",
      "Epoch [8/10], Batch [330/3237], Loss: 0.08367441594600677\n",
      "Epoch [8/10], Batch [340/3237], Loss: 0.13668876886367798\n",
      "Epoch [8/10], Batch [350/3237], Loss: 0.1666697859764099\n",
      "Epoch [8/10], Batch [360/3237], Loss: 0.12296468764543533\n",
      "Epoch [8/10], Batch [370/3237], Loss: 0.06959247589111328\n",
      "Epoch [8/10], Batch [380/3237], Loss: 0.2687171399593353\n",
      "Epoch [8/10], Batch [390/3237], Loss: 0.07475830614566803\n",
      "Epoch [8/10], Batch [400/3237], Loss: 0.08788047730922699\n",
      "Epoch [8/10], Batch [410/3237], Loss: 0.08955127745866776\n",
      "Epoch [8/10], Batch [420/3237], Loss: 0.10678169876337051\n",
      "Epoch [8/10], Batch [430/3237], Loss: 0.10338890552520752\n",
      "Epoch [8/10], Batch [440/3237], Loss: 0.14129818975925446\n",
      "Epoch [8/10], Batch [450/3237], Loss: 0.14784325659275055\n",
      "Epoch [8/10], Batch [460/3237], Loss: 0.0796055942773819\n",
      "Epoch [8/10], Batch [470/3237], Loss: 0.12448158860206604\n",
      "Epoch [8/10], Batch [480/3237], Loss: 0.2122020721435547\n",
      "Epoch [8/10], Batch [490/3237], Loss: 0.13091503083705902\n",
      "Epoch [8/10], Batch [500/3237], Loss: 0.10255437344312668\n",
      "Epoch [8/10], Batch [510/3237], Loss: 0.21549905836582184\n",
      "Epoch [8/10], Batch [520/3237], Loss: 0.12343887239694595\n",
      "Epoch [8/10], Batch [530/3237], Loss: 0.058523572981357574\n",
      "Epoch [8/10], Batch [540/3237], Loss: 0.2398093342781067\n",
      "Epoch [8/10], Batch [550/3237], Loss: 0.07788217067718506\n",
      "Epoch [8/10], Batch [560/3237], Loss: 0.09085625410079956\n",
      "Epoch [8/10], Batch [570/3237], Loss: 0.09818925708532333\n",
      "Epoch [8/10], Batch [580/3237], Loss: 0.07410784810781479\n",
      "Epoch [8/10], Batch [590/3237], Loss: 0.08190389722585678\n",
      "Epoch [8/10], Batch [600/3237], Loss: 0.334717333316803\n",
      "Epoch [8/10], Batch [610/3237], Loss: 0.09990501403808594\n",
      "Epoch [8/10], Batch [620/3237], Loss: 0.21582773327827454\n",
      "Epoch [8/10], Batch [630/3237], Loss: 0.19709493219852448\n",
      "Epoch [8/10], Batch [640/3237], Loss: 0.11679847538471222\n",
      "Epoch [8/10], Batch [650/3237], Loss: 0.09019952267408371\n",
      "Epoch [8/10], Batch [660/3237], Loss: 0.1258278340101242\n",
      "Epoch [8/10], Batch [670/3237], Loss: 0.47485169768333435\n",
      "Epoch [8/10], Batch [680/3237], Loss: 0.1552957147359848\n",
      "Epoch [8/10], Batch [690/3237], Loss: 0.2624049186706543\n",
      "Epoch [8/10], Batch [700/3237], Loss: 0.14929832518100739\n",
      "Epoch [8/10], Batch [710/3237], Loss: 0.10517390817403793\n",
      "Epoch [8/10], Batch [720/3237], Loss: 0.24147744476795197\n",
      "Epoch [8/10], Batch [730/3237], Loss: 0.0509420782327652\n",
      "Epoch [8/10], Batch [740/3237], Loss: 0.1010482981801033\n",
      "Epoch [8/10], Batch [750/3237], Loss: 0.09944343566894531\n",
      "Epoch [8/10], Batch [760/3237], Loss: 0.07641906291246414\n",
      "Epoch [8/10], Batch [770/3237], Loss: 0.18711669743061066\n",
      "Epoch [8/10], Batch [780/3237], Loss: 0.1610160917043686\n",
      "Epoch [8/10], Batch [790/3237], Loss: 0.2142566591501236\n",
      "Epoch [8/10], Batch [800/3237], Loss: 0.051624029874801636\n",
      "Epoch [8/10], Batch [810/3237], Loss: 0.10784568637609482\n",
      "Epoch [8/10], Batch [820/3237], Loss: 0.12214170396327972\n",
      "Epoch [8/10], Batch [830/3237], Loss: 0.061393603682518005\n",
      "Epoch [8/10], Batch [840/3237], Loss: 0.08697805553674698\n",
      "Epoch [8/10], Batch [850/3237], Loss: 0.1766083538532257\n",
      "Epoch [8/10], Batch [860/3237], Loss: 0.1591009497642517\n",
      "Epoch [8/10], Batch [870/3237], Loss: 0.1847355216741562\n",
      "Epoch [8/10], Batch [880/3237], Loss: 0.13750512897968292\n",
      "Epoch [8/10], Batch [890/3237], Loss: 0.32053709030151367\n",
      "Epoch [8/10], Batch [900/3237], Loss: 0.07626844197511673\n",
      "Epoch [8/10], Batch [910/3237], Loss: 0.16031654179096222\n",
      "Epoch [8/10], Batch [920/3237], Loss: 0.17988501489162445\n",
      "Epoch [8/10], Batch [930/3237], Loss: 0.1310088187456131\n",
      "Epoch [8/10], Batch [940/3237], Loss: 0.09922154992818832\n",
      "Epoch [8/10], Batch [950/3237], Loss: 0.269910603761673\n",
      "Epoch [8/10], Batch [960/3237], Loss: 0.11036952584981918\n",
      "Epoch [8/10], Batch [970/3237], Loss: 0.0583227202296257\n",
      "Epoch [8/10], Batch [980/3237], Loss: 0.1942877471446991\n",
      "Epoch [8/10], Batch [990/3237], Loss: 0.18369221687316895\n",
      "Epoch [8/10], Batch [1000/3237], Loss: 0.08026902377605438\n",
      "Epoch [8/10], Batch [1010/3237], Loss: 0.13216279447078705\n",
      "Epoch [8/10], Batch [1020/3237], Loss: 0.171360045671463\n",
      "Epoch [8/10], Batch [1030/3237], Loss: 0.15320099890232086\n",
      "Epoch [8/10], Batch [1040/3237], Loss: 0.2734511196613312\n",
      "Epoch [8/10], Batch [1050/3237], Loss: 0.1767667531967163\n",
      "Epoch [8/10], Batch [1060/3237], Loss: 0.1421356350183487\n",
      "Epoch [8/10], Batch [1070/3237], Loss: 0.4248703420162201\n",
      "Epoch [8/10], Batch [1080/3237], Loss: 0.18139216303825378\n",
      "Epoch [8/10], Batch [1090/3237], Loss: 0.16176116466522217\n",
      "Epoch [8/10], Batch [1100/3237], Loss: 0.10917872935533524\n",
      "Epoch [8/10], Batch [1110/3237], Loss: 0.05588972568511963\n",
      "Epoch [8/10], Batch [1120/3237], Loss: 0.10724740475416183\n",
      "Epoch [8/10], Batch [1130/3237], Loss: 0.1900082677602768\n",
      "Epoch [8/10], Batch [1140/3237], Loss: 0.23176929354667664\n",
      "Epoch [8/10], Batch [1150/3237], Loss: 0.1592882126569748\n",
      "Epoch [8/10], Batch [1160/3237], Loss: 0.1263483613729477\n",
      "Epoch [8/10], Batch [1170/3237], Loss: 0.09870711714029312\n",
      "Epoch [8/10], Batch [1180/3237], Loss: 0.19315719604492188\n",
      "Epoch [8/10], Batch [1190/3237], Loss: 0.07133930921554565\n",
      "Epoch [8/10], Batch [1200/3237], Loss: 0.0651516392827034\n",
      "Epoch [8/10], Batch [1210/3237], Loss: 0.19475817680358887\n",
      "Epoch [8/10], Batch [1220/3237], Loss: 0.11692855507135391\n",
      "Epoch [8/10], Batch [1230/3237], Loss: 0.10245207697153091\n",
      "Epoch [8/10], Batch [1240/3237], Loss: 0.07059802860021591\n",
      "Epoch [8/10], Batch [1250/3237], Loss: 0.18920259177684784\n",
      "Epoch [8/10], Batch [1260/3237], Loss: 0.07527770847082138\n",
      "Epoch [8/10], Batch [1270/3237], Loss: 0.06582701951265335\n",
      "Epoch [8/10], Batch [1280/3237], Loss: 0.07970186322927475\n",
      "Epoch [8/10], Batch [1290/3237], Loss: 0.14323218166828156\n",
      "Epoch [8/10], Batch [1300/3237], Loss: 0.1710090935230255\n",
      "Epoch [8/10], Batch [1310/3237], Loss: 0.071089006960392\n",
      "Epoch [8/10], Batch [1320/3237], Loss: 0.09234459698200226\n",
      "Epoch [8/10], Batch [1330/3237], Loss: 0.1269897073507309\n",
      "Epoch [8/10], Batch [1340/3237], Loss: 0.06517587602138519\n",
      "Epoch [8/10], Batch [1350/3237], Loss: 0.06521777808666229\n",
      "Epoch [8/10], Batch [1360/3237], Loss: 0.15539760887622833\n",
      "Epoch [8/10], Batch [1370/3237], Loss: 0.19399063289165497\n",
      "Epoch [8/10], Batch [1380/3237], Loss: 0.051033999770879745\n",
      "Epoch [8/10], Batch [1390/3237], Loss: 0.08604465425014496\n",
      "Epoch [8/10], Batch [1400/3237], Loss: 0.16674095392227173\n",
      "Epoch [8/10], Batch [1410/3237], Loss: 0.1436598300933838\n",
      "Epoch [8/10], Batch [1420/3237], Loss: 0.2560521066188812\n",
      "Epoch [8/10], Batch [1430/3237], Loss: 0.17753951251506805\n",
      "Epoch [8/10], Batch [1440/3237], Loss: 0.10785911232233047\n",
      "Epoch [8/10], Batch [1450/3237], Loss: 0.25551271438598633\n",
      "Epoch [8/10], Batch [1460/3237], Loss: 0.1747698187828064\n",
      "Epoch [8/10], Batch [1470/3237], Loss: 0.11203517764806747\n",
      "Epoch [8/10], Batch [1480/3237], Loss: 0.18074952065944672\n",
      "Epoch [8/10], Batch [1490/3237], Loss: 0.10248658061027527\n",
      "Epoch [8/10], Batch [1500/3237], Loss: 0.11584354192018509\n",
      "Epoch [8/10], Batch [1510/3237], Loss: 0.09855005145072937\n",
      "Epoch [8/10], Batch [1520/3237], Loss: 0.1360342800617218\n",
      "Epoch [8/10], Batch [1530/3237], Loss: 0.0773451179265976\n",
      "Epoch [8/10], Batch [1540/3237], Loss: 0.05297773703932762\n",
      "Epoch [8/10], Batch [1550/3237], Loss: 0.2023487091064453\n",
      "Epoch [8/10], Batch [1560/3237], Loss: 0.11041945219039917\n",
      "Epoch [8/10], Batch [1570/3237], Loss: 0.22184477746486664\n",
      "Epoch [8/10], Batch [1580/3237], Loss: 0.4108201563358307\n",
      "Epoch [8/10], Batch [1590/3237], Loss: 0.12955538928508759\n",
      "Epoch [8/10], Batch [1600/3237], Loss: 0.23028506338596344\n",
      "Epoch [8/10], Batch [1610/3237], Loss: 0.10965091735124588\n",
      "Epoch [8/10], Batch [1620/3237], Loss: 0.057283658534288406\n",
      "Epoch [8/10], Batch [1630/3237], Loss: 0.03933844342827797\n",
      "Epoch [8/10], Batch [1640/3237], Loss: 0.30124321579933167\n",
      "Epoch [8/10], Batch [1650/3237], Loss: 0.16089588403701782\n",
      "Epoch [8/10], Batch [1660/3237], Loss: 0.0729324147105217\n",
      "Epoch [8/10], Batch [1670/3237], Loss: 0.3242890238761902\n",
      "Epoch [8/10], Batch [1680/3237], Loss: 0.08920951932668686\n",
      "Epoch [8/10], Batch [1690/3237], Loss: 0.11186186224222183\n",
      "Epoch [8/10], Batch [1700/3237], Loss: 0.11137905716896057\n",
      "Epoch [8/10], Batch [1710/3237], Loss: 0.14293178915977478\n",
      "Epoch [8/10], Batch [1720/3237], Loss: 0.2038310021162033\n",
      "Epoch [8/10], Batch [1730/3237], Loss: 0.1726733297109604\n",
      "Epoch [8/10], Batch [1740/3237], Loss: 0.16531920433044434\n",
      "Epoch [8/10], Batch [1750/3237], Loss: 0.31917786598205566\n",
      "Epoch [8/10], Batch [1760/3237], Loss: 0.2795846462249756\n",
      "Epoch [8/10], Batch [1770/3237], Loss: 0.09306857734918594\n",
      "Epoch [8/10], Batch [1780/3237], Loss: 0.31999093294143677\n",
      "Epoch [8/10], Batch [1790/3237], Loss: 0.25430595874786377\n",
      "Epoch [8/10], Batch [1800/3237], Loss: 0.06580428779125214\n",
      "Epoch [8/10], Batch [1810/3237], Loss: 0.2250909060239792\n",
      "Epoch [8/10], Batch [1820/3237], Loss: 0.06464570015668869\n",
      "Epoch [8/10], Batch [1830/3237], Loss: 0.2148616760969162\n",
      "Epoch [8/10], Batch [1840/3237], Loss: 0.09681638330221176\n",
      "Epoch [8/10], Batch [1850/3237], Loss: 0.07896647602319717\n",
      "Epoch [8/10], Batch [1860/3237], Loss: 0.12696173787117004\n",
      "Epoch [8/10], Batch [1870/3237], Loss: 0.20576174557209015\n",
      "Epoch [8/10], Batch [1880/3237], Loss: 0.16624392569065094\n",
      "Epoch [8/10], Batch [1890/3237], Loss: 0.13022039830684662\n",
      "Epoch [8/10], Batch [1900/3237], Loss: 0.12144828587770462\n",
      "Epoch [8/10], Batch [1910/3237], Loss: 0.062235277146101\n",
      "Epoch [8/10], Batch [1920/3237], Loss: 0.1403150111436844\n",
      "Epoch [8/10], Batch [1930/3237], Loss: 0.2259959727525711\n",
      "Epoch [8/10], Batch [1940/3237], Loss: 0.17214389145374298\n",
      "Epoch [8/10], Batch [1950/3237], Loss: 0.11667941510677338\n",
      "Epoch [8/10], Batch [1960/3237], Loss: 0.12373737245798111\n",
      "Epoch [8/10], Batch [1970/3237], Loss: 0.09034412354230881\n",
      "Epoch [8/10], Batch [1980/3237], Loss: 0.0463956817984581\n",
      "Epoch [8/10], Batch [1990/3237], Loss: 0.17317470908164978\n",
      "Epoch [8/10], Batch [2000/3237], Loss: 0.06731211394071579\n",
      "Epoch [8/10], Batch [2010/3237], Loss: 0.17463043332099915\n",
      "Epoch [8/10], Batch [2020/3237], Loss: 0.13451755046844482\n",
      "Epoch [8/10], Batch [2030/3237], Loss: 0.26907190680503845\n",
      "Epoch [8/10], Batch [2040/3237], Loss: 0.3226028382778168\n",
      "Epoch [8/10], Batch [2050/3237], Loss: 0.13278833031654358\n",
      "Epoch [8/10], Batch [2060/3237], Loss: 0.09085287898778915\n",
      "Epoch [8/10], Batch [2070/3237], Loss: 0.06801880896091461\n",
      "Epoch [8/10], Batch [2080/3237], Loss: 0.19673234224319458\n",
      "Epoch [8/10], Batch [2090/3237], Loss: 0.15861569344997406\n",
      "Epoch [8/10], Batch [2100/3237], Loss: 0.10334386676549911\n",
      "Epoch [8/10], Batch [2110/3237], Loss: 0.07885602116584778\n",
      "Epoch [8/10], Batch [2120/3237], Loss: 0.12344271689653397\n",
      "Epoch [8/10], Batch [2130/3237], Loss: 0.06606584042310715\n",
      "Epoch [8/10], Batch [2140/3237], Loss: 0.11407972872257233\n",
      "Epoch [8/10], Batch [2150/3237], Loss: 0.17972128093242645\n",
      "Epoch [8/10], Batch [2160/3237], Loss: 0.17980347573757172\n",
      "Epoch [8/10], Batch [2170/3237], Loss: 0.1579887568950653\n",
      "Epoch [8/10], Batch [2180/3237], Loss: 0.2645554840564728\n",
      "Epoch [8/10], Batch [2190/3237], Loss: 0.22124917805194855\n",
      "Epoch [8/10], Batch [2200/3237], Loss: 0.06517045199871063\n",
      "Epoch [8/10], Batch [2210/3237], Loss: 0.20669348537921906\n",
      "Epoch [8/10], Batch [2220/3237], Loss: 0.09220341593027115\n",
      "Epoch [8/10], Batch [2230/3237], Loss: 0.1583629548549652\n",
      "Epoch [8/10], Batch [2240/3237], Loss: 0.1167287677526474\n",
      "Epoch [8/10], Batch [2250/3237], Loss: 0.1998065561056137\n",
      "Epoch [8/10], Batch [2260/3237], Loss: 0.11182291060686111\n",
      "Epoch [8/10], Batch [2270/3237], Loss: 0.09271693974733353\n",
      "Epoch [8/10], Batch [2280/3237], Loss: 0.14722371101379395\n",
      "Epoch [8/10], Batch [2290/3237], Loss: 0.07045643031597137\n",
      "Epoch [8/10], Batch [2300/3237], Loss: 0.11029097437858582\n",
      "Epoch [8/10], Batch [2310/3237], Loss: 0.18206000328063965\n",
      "Epoch [8/10], Batch [2320/3237], Loss: 0.09723568707704544\n",
      "Epoch [8/10], Batch [2330/3237], Loss: 0.2427840381860733\n",
      "Epoch [8/10], Batch [2340/3237], Loss: 0.025803379714488983\n",
      "Epoch [8/10], Batch [2350/3237], Loss: 0.18720503151416779\n",
      "Epoch [8/10], Batch [2360/3237], Loss: 0.15738947689533234\n",
      "Epoch [8/10], Batch [2370/3237], Loss: 0.17407695949077606\n",
      "Epoch [8/10], Batch [2380/3237], Loss: 0.12762819230556488\n",
      "Epoch [8/10], Batch [2390/3237], Loss: 0.06108299270272255\n",
      "Epoch [8/10], Batch [2400/3237], Loss: 0.0767003744840622\n",
      "Epoch [8/10], Batch [2410/3237], Loss: 0.08958600461483002\n",
      "Epoch [8/10], Batch [2420/3237], Loss: 0.1750224083662033\n",
      "Epoch [8/10], Batch [2430/3237], Loss: 0.37576496601104736\n",
      "Epoch [8/10], Batch [2440/3237], Loss: 0.16473142802715302\n",
      "Epoch [8/10], Batch [2450/3237], Loss: 0.2645892798900604\n",
      "Epoch [8/10], Batch [2460/3237], Loss: 0.22504714131355286\n",
      "Epoch [8/10], Batch [2470/3237], Loss: 0.22838221490383148\n",
      "Epoch [8/10], Batch [2480/3237], Loss: 0.197686567902565\n",
      "Epoch [8/10], Batch [2490/3237], Loss: 0.13762372732162476\n",
      "Epoch [8/10], Batch [2500/3237], Loss: 0.19197127223014832\n",
      "Epoch [8/10], Batch [2510/3237], Loss: 0.18334728479385376\n",
      "Epoch [8/10], Batch [2520/3237], Loss: 0.25079870223999023\n",
      "Epoch [8/10], Batch [2530/3237], Loss: 0.12135730683803558\n",
      "Epoch [8/10], Batch [2540/3237], Loss: 0.23132666945457458\n",
      "Epoch [8/10], Batch [2550/3237], Loss: 0.15520033240318298\n",
      "Epoch [8/10], Batch [2560/3237], Loss: 0.19857555627822876\n",
      "Epoch [8/10], Batch [2570/3237], Loss: 0.1505342572927475\n",
      "Epoch [8/10], Batch [2580/3237], Loss: 0.261601060628891\n",
      "Epoch [8/10], Batch [2590/3237], Loss: 0.05735688656568527\n",
      "Epoch [8/10], Batch [2600/3237], Loss: 0.3433445692062378\n",
      "Epoch [8/10], Batch [2610/3237], Loss: 0.09012680500745773\n",
      "Epoch [8/10], Batch [2620/3237], Loss: 0.11003007739782333\n",
      "Epoch [8/10], Batch [2630/3237], Loss: 0.14275895059108734\n",
      "Epoch [8/10], Batch [2640/3237], Loss: 0.21413539350032806\n",
      "Epoch [8/10], Batch [2650/3237], Loss: 0.1371782422065735\n",
      "Epoch [8/10], Batch [2660/3237], Loss: 0.08871269226074219\n",
      "Epoch [8/10], Batch [2670/3237], Loss: 0.2011340707540512\n",
      "Epoch [8/10], Batch [2680/3237], Loss: 0.09696491062641144\n",
      "Epoch [8/10], Batch [2690/3237], Loss: 0.12781922519207\n",
      "Epoch [8/10], Batch [2700/3237], Loss: 0.09648501873016357\n",
      "Epoch [8/10], Batch [2710/3237], Loss: 0.1064283549785614\n",
      "Epoch [8/10], Batch [2720/3237], Loss: 0.14462997019290924\n",
      "Epoch [8/10], Batch [2730/3237], Loss: 0.10417960584163666\n",
      "Epoch [8/10], Batch [2740/3237], Loss: 0.1478494256734848\n",
      "Epoch [8/10], Batch [2750/3237], Loss: 0.08028998225927353\n",
      "Epoch [8/10], Batch [2760/3237], Loss: 0.06868811696767807\n",
      "Epoch [8/10], Batch [2770/3237], Loss: 0.15370655059814453\n",
      "Epoch [8/10], Batch [2780/3237], Loss: 0.13789458572864532\n",
      "Epoch [8/10], Batch [2790/3237], Loss: 0.2357126623392105\n",
      "Epoch [8/10], Batch [2800/3237], Loss: 0.1752919703722\n",
      "Epoch [8/10], Batch [2810/3237], Loss: 0.1724153608083725\n",
      "Epoch [8/10], Batch [2820/3237], Loss: 0.21621079742908478\n",
      "Epoch [8/10], Batch [2830/3237], Loss: 0.15788404643535614\n",
      "Epoch [8/10], Batch [2840/3237], Loss: 0.2189202606678009\n",
      "Epoch [8/10], Batch [2850/3237], Loss: 0.2623628079891205\n",
      "Epoch [8/10], Batch [2860/3237], Loss: 0.0685686320066452\n",
      "Epoch [8/10], Batch [2870/3237], Loss: 0.1069171205163002\n",
      "Epoch [8/10], Batch [2880/3237], Loss: 0.06612640619277954\n",
      "Epoch [8/10], Batch [2890/3237], Loss: 0.08691373467445374\n",
      "Epoch [8/10], Batch [2900/3237], Loss: 0.08873723447322845\n",
      "Epoch [8/10], Batch [2910/3237], Loss: 0.16509123146533966\n",
      "Epoch [8/10], Batch [2920/3237], Loss: 0.1948552131652832\n",
      "Epoch [8/10], Batch [2930/3237], Loss: 0.11438415199518204\n",
      "Epoch [8/10], Batch [2940/3237], Loss: 0.2627686560153961\n",
      "Epoch [8/10], Batch [2950/3237], Loss: 0.1595616489648819\n",
      "Epoch [8/10], Batch [2960/3237], Loss: 0.08416654169559479\n",
      "Epoch [8/10], Batch [2970/3237], Loss: 0.1500234454870224\n",
      "Epoch [8/10], Batch [2980/3237], Loss: 0.13632294535636902\n",
      "Epoch [8/10], Batch [2990/3237], Loss: 0.07526825368404388\n",
      "Epoch [8/10], Batch [3000/3237], Loss: 0.22378532588481903\n",
      "Epoch [8/10], Batch [3010/3237], Loss: 0.25250008702278137\n",
      "Epoch [8/10], Batch [3020/3237], Loss: 0.11429675668478012\n",
      "Epoch [8/10], Batch [3030/3237], Loss: 0.2873861491680145\n",
      "Epoch [8/10], Batch [3040/3237], Loss: 0.08379991352558136\n",
      "Epoch [8/10], Batch [3050/3237], Loss: 0.17540858685970306\n",
      "Epoch [8/10], Batch [3060/3237], Loss: 0.09222610294818878\n",
      "Epoch [8/10], Batch [3070/3237], Loss: 0.14157448709011078\n",
      "Epoch [8/10], Batch [3080/3237], Loss: 0.09955727308988571\n",
      "Epoch [8/10], Batch [3090/3237], Loss: 0.1151500791311264\n",
      "Epoch [8/10], Batch [3100/3237], Loss: 0.5290092825889587\n",
      "Epoch [8/10], Batch [3110/3237], Loss: 0.17408207058906555\n",
      "Epoch [8/10], Batch [3120/3237], Loss: 0.17289964854717255\n",
      "Epoch [8/10], Batch [3130/3237], Loss: 0.08722451329231262\n",
      "Epoch [8/10], Batch [3140/3237], Loss: 0.09365001320838928\n",
      "Epoch [8/10], Batch [3150/3237], Loss: 0.0873715728521347\n",
      "Epoch [8/10], Batch [3160/3237], Loss: 0.17065578699111938\n",
      "Epoch [8/10], Batch [3170/3237], Loss: 0.17156913876533508\n",
      "Epoch [8/10], Batch [3180/3237], Loss: 0.24248729646205902\n",
      "Epoch [8/10], Batch [3190/3237], Loss: 0.14475208520889282\n",
      "Epoch [8/10], Batch [3200/3237], Loss: 0.19536525011062622\n",
      "Epoch [8/10], Batch [3210/3237], Loss: 0.11382985860109329\n",
      "Epoch [8/10], Batch [3220/3237], Loss: 0.116861991584301\n",
      "Epoch [8/10], Batch [3230/3237], Loss: 0.06816402077674866\n",
      "Epoch 8, Average Train Loss: 0.1473, Average Val Loss: 0.1695\n",
      "Saved new best model at epoch 8 with Val Loss: 0.1695\n",
      "Epoch [9/10], Batch [10/3237], Loss: 0.06275904178619385\n",
      "Epoch [9/10], Batch [20/3237], Loss: 0.10230208933353424\n",
      "Epoch [9/10], Batch [30/3237], Loss: 0.14769607782363892\n",
      "Epoch [9/10], Batch [40/3237], Loss: 0.3173685669898987\n",
      "Epoch [9/10], Batch [50/3237], Loss: 0.09192224591970444\n",
      "Epoch [9/10], Batch [60/3237], Loss: 0.18850532174110413\n",
      "Epoch [9/10], Batch [70/3237], Loss: 0.17580276727676392\n",
      "Epoch [9/10], Batch [80/3237], Loss: 0.0747794583439827\n",
      "Epoch [9/10], Batch [90/3237], Loss: 0.13432660698890686\n",
      "Epoch [9/10], Batch [100/3237], Loss: 0.13082261383533478\n",
      "Epoch [9/10], Batch [110/3237], Loss: 0.2804129421710968\n",
      "Epoch [9/10], Batch [120/3237], Loss: 0.09219010919332504\n",
      "Epoch [9/10], Batch [130/3237], Loss: 0.11815974861383438\n",
      "Epoch [9/10], Batch [140/3237], Loss: 0.12343097478151321\n",
      "Epoch [9/10], Batch [150/3237], Loss: 0.2042725682258606\n",
      "Epoch [9/10], Batch [160/3237], Loss: 0.15525200963020325\n",
      "Epoch [9/10], Batch [170/3237], Loss: 0.14684274792671204\n",
      "Epoch [9/10], Batch [180/3237], Loss: 0.06767145544290543\n",
      "Epoch [9/10], Batch [190/3237], Loss: 0.16900552809238434\n",
      "Epoch [9/10], Batch [200/3237], Loss: 0.16643047332763672\n",
      "Epoch [9/10], Batch [210/3237], Loss: 0.08673616498708725\n",
      "Epoch [9/10], Batch [220/3237], Loss: 0.15177170932292938\n",
      "Epoch [9/10], Batch [230/3237], Loss: 0.08405838161706924\n",
      "Epoch [9/10], Batch [240/3237], Loss: 0.1212121918797493\n",
      "Epoch [9/10], Batch [250/3237], Loss: 0.0794505923986435\n",
      "Epoch [9/10], Batch [260/3237], Loss: 0.16035328805446625\n",
      "Epoch [9/10], Batch [270/3237], Loss: 0.11631786078214645\n",
      "Epoch [9/10], Batch [280/3237], Loss: 0.14061817526817322\n",
      "Epoch [9/10], Batch [290/3237], Loss: 0.19472388923168182\n",
      "Epoch [9/10], Batch [300/3237], Loss: 0.09603290259838104\n",
      "Epoch [9/10], Batch [310/3237], Loss: 0.06628509610891342\n",
      "Epoch [9/10], Batch [320/3237], Loss: 0.21862009167671204\n",
      "Epoch [9/10], Batch [330/3237], Loss: 0.19547328352928162\n",
      "Epoch [9/10], Batch [340/3237], Loss: 0.10297319293022156\n",
      "Epoch [9/10], Batch [350/3237], Loss: 0.08402907103300095\n",
      "Epoch [9/10], Batch [360/3237], Loss: 0.15491563081741333\n",
      "Epoch [9/10], Batch [370/3237], Loss: 0.2930181324481964\n",
      "Epoch [9/10], Batch [380/3237], Loss: 0.3140508532524109\n",
      "Epoch [9/10], Batch [390/3237], Loss: 0.16596569120883942\n",
      "Epoch [9/10], Batch [400/3237], Loss: 0.20531710982322693\n",
      "Epoch [9/10], Batch [410/3237], Loss: 0.39756882190704346\n",
      "Epoch [9/10], Batch [420/3237], Loss: 0.19740009307861328\n",
      "Epoch [9/10], Batch [430/3237], Loss: 0.1033054068684578\n",
      "Epoch [9/10], Batch [440/3237], Loss: 0.2902829349040985\n",
      "Epoch [9/10], Batch [450/3237], Loss: 0.12637457251548767\n",
      "Epoch [9/10], Batch [460/3237], Loss: 0.12263034284114838\n",
      "Epoch [9/10], Batch [470/3237], Loss: 0.09962209314107895\n",
      "Epoch [9/10], Batch [480/3237], Loss: 0.16191211342811584\n",
      "Epoch [9/10], Batch [490/3237], Loss: 0.07396124303340912\n",
      "Epoch [9/10], Batch [500/3237], Loss: 0.11901777237653732\n",
      "Epoch [9/10], Batch [510/3237], Loss: 0.1639186441898346\n",
      "Epoch [9/10], Batch [520/3237], Loss: 0.08058244735002518\n",
      "Epoch [9/10], Batch [530/3237], Loss: 0.11400210857391357\n",
      "Epoch [9/10], Batch [540/3237], Loss: 0.17336265742778778\n",
      "Epoch [9/10], Batch [550/3237], Loss: 0.13141900300979614\n",
      "Epoch [9/10], Batch [560/3237], Loss: 0.055257391184568405\n",
      "Epoch [9/10], Batch [570/3237], Loss: 0.1486232727766037\n",
      "Epoch [9/10], Batch [580/3237], Loss: 0.20898161828517914\n",
      "Epoch [9/10], Batch [590/3237], Loss: 0.13160738348960876\n",
      "Epoch [9/10], Batch [600/3237], Loss: 0.12180012464523315\n",
      "Epoch [9/10], Batch [610/3237], Loss: 0.1262076199054718\n",
      "Epoch [9/10], Batch [620/3237], Loss: 0.18183448910713196\n",
      "Epoch [9/10], Batch [630/3237], Loss: 0.11946649104356766\n",
      "Epoch [9/10], Batch [640/3237], Loss: 0.1276458203792572\n",
      "Epoch [9/10], Batch [650/3237], Loss: 0.1834069788455963\n",
      "Epoch [9/10], Batch [660/3237], Loss: 0.2734878361225128\n",
      "Epoch [9/10], Batch [670/3237], Loss: 0.1726091206073761\n",
      "Epoch [9/10], Batch [680/3237], Loss: 0.14637354016304016\n",
      "Epoch [9/10], Batch [690/3237], Loss: 0.18442405760288239\n",
      "Epoch [9/10], Batch [700/3237], Loss: 0.06230858713388443\n",
      "Epoch [9/10], Batch [710/3237], Loss: 0.2156623750925064\n",
      "Epoch [9/10], Batch [720/3237], Loss: 0.10347241908311844\n",
      "Epoch [9/10], Batch [730/3237], Loss: 0.16705550253391266\n",
      "Epoch [9/10], Batch [740/3237], Loss: 0.06692898273468018\n",
      "Epoch [9/10], Batch [750/3237], Loss: 0.18087002635002136\n",
      "Epoch [9/10], Batch [760/3237], Loss: 0.05659480765461922\n",
      "Epoch [9/10], Batch [770/3237], Loss: 0.14083953201770782\n",
      "Epoch [9/10], Batch [780/3237], Loss: 0.1251961886882782\n",
      "Epoch [9/10], Batch [790/3237], Loss: 0.12661834061145782\n",
      "Epoch [9/10], Batch [800/3237], Loss: 0.24823687970638275\n",
      "Epoch [9/10], Batch [810/3237], Loss: 0.2032555490732193\n",
      "Epoch [9/10], Batch [820/3237], Loss: 0.13029828667640686\n",
      "Epoch [9/10], Batch [830/3237], Loss: 0.2619315981864929\n",
      "Epoch [9/10], Batch [840/3237], Loss: 0.24271182715892792\n",
      "Epoch [9/10], Batch [850/3237], Loss: 0.07225403934717178\n",
      "Epoch [9/10], Batch [860/3237], Loss: 0.15227766335010529\n",
      "Epoch [9/10], Batch [870/3237], Loss: 0.06890539824962616\n",
      "Epoch [9/10], Batch [880/3237], Loss: 0.13889409601688385\n",
      "Epoch [9/10], Batch [890/3237], Loss: 0.04851063713431358\n",
      "Epoch [9/10], Batch [900/3237], Loss: 0.09439855068922043\n",
      "Epoch [9/10], Batch [910/3237], Loss: 0.10415712743997574\n",
      "Epoch [9/10], Batch [920/3237], Loss: 0.17791812121868134\n",
      "Epoch [9/10], Batch [930/3237], Loss: 0.16081318259239197\n",
      "Epoch [9/10], Batch [940/3237], Loss: 0.07332301139831543\n",
      "Epoch [9/10], Batch [950/3237], Loss: 0.1565992385149002\n",
      "Epoch [9/10], Batch [960/3237], Loss: 0.08986598998308182\n",
      "Epoch [9/10], Batch [970/3237], Loss: 0.06515369564294815\n",
      "Epoch [9/10], Batch [980/3237], Loss: 0.042209919542074203\n",
      "Epoch [9/10], Batch [990/3237], Loss: 0.12594549357891083\n",
      "Epoch [9/10], Batch [1000/3237], Loss: 0.10669930279254913\n",
      "Epoch [9/10], Batch [1010/3237], Loss: 0.05640116706490517\n",
      "Epoch [9/10], Batch [1020/3237], Loss: 0.17870227992534637\n",
      "Epoch [9/10], Batch [1030/3237], Loss: 0.07443192601203918\n",
      "Epoch [9/10], Batch [1040/3237], Loss: 0.20661580562591553\n",
      "Epoch [9/10], Batch [1050/3237], Loss: 0.053140535950660706\n",
      "Epoch [9/10], Batch [1060/3237], Loss: 0.09195815026760101\n",
      "Epoch [9/10], Batch [1070/3237], Loss: 0.07804004848003387\n",
      "Epoch [9/10], Batch [1080/3237], Loss: 0.09076517075300217\n",
      "Epoch [9/10], Batch [1090/3237], Loss: 0.07571034878492355\n",
      "Epoch [9/10], Batch [1100/3237], Loss: 0.10514913499355316\n",
      "Epoch [9/10], Batch [1110/3237], Loss: 0.07920455932617188\n",
      "Epoch [9/10], Batch [1120/3237], Loss: 0.4899747371673584\n",
      "Epoch [9/10], Batch [1130/3237], Loss: 0.182433620095253\n",
      "Epoch [9/10], Batch [1140/3237], Loss: 0.11521987617015839\n",
      "Epoch [9/10], Batch [1150/3237], Loss: 0.17771251499652863\n",
      "Epoch [9/10], Batch [1160/3237], Loss: 0.03060724213719368\n",
      "Epoch [9/10], Batch [1170/3237], Loss: 0.06371713429689407\n",
      "Epoch [9/10], Batch [1180/3237], Loss: 0.08035305887460709\n",
      "Epoch [9/10], Batch [1190/3237], Loss: 0.09163371473550797\n",
      "Epoch [9/10], Batch [1200/3237], Loss: 0.09782061725854874\n",
      "Epoch [9/10], Batch [1210/3237], Loss: 0.12045221030712128\n",
      "Epoch [9/10], Batch [1220/3237], Loss: 0.08874674886465073\n",
      "Epoch [9/10], Batch [1230/3237], Loss: 0.13010504841804504\n",
      "Epoch [9/10], Batch [1240/3237], Loss: 0.30682384967803955\n",
      "Epoch [9/10], Batch [1250/3237], Loss: 0.16850246489048004\n",
      "Epoch [9/10], Batch [1260/3237], Loss: 0.14562061429023743\n",
      "Epoch [9/10], Batch [1270/3237], Loss: 0.12594559788703918\n",
      "Epoch [9/10], Batch [1280/3237], Loss: 0.16775164008140564\n",
      "Epoch [9/10], Batch [1290/3237], Loss: 0.17964346706867218\n",
      "Epoch [9/10], Batch [1300/3237], Loss: 0.11461371183395386\n",
      "Epoch [9/10], Batch [1310/3237], Loss: 0.09000260382890701\n",
      "Epoch [9/10], Batch [1320/3237], Loss: 0.05281631276011467\n",
      "Epoch [9/10], Batch [1330/3237], Loss: 0.10655863583087921\n",
      "Epoch [9/10], Batch [1340/3237], Loss: 0.2851335108280182\n",
      "Epoch [9/10], Batch [1350/3237], Loss: 0.24627338349819183\n",
      "Epoch [9/10], Batch [1360/3237], Loss: 0.21851924061775208\n",
      "Epoch [9/10], Batch [1370/3237], Loss: 0.0701666846871376\n",
      "Epoch [9/10], Batch [1380/3237], Loss: 0.06245601549744606\n",
      "Epoch [9/10], Batch [1390/3237], Loss: 0.06543322652578354\n",
      "Epoch [9/10], Batch [1400/3237], Loss: 0.23682697117328644\n",
      "Epoch [9/10], Batch [1410/3237], Loss: 0.18127138912677765\n",
      "Epoch [9/10], Batch [1420/3237], Loss: 0.12145908176898956\n",
      "Epoch [9/10], Batch [1430/3237], Loss: 0.12157294899225235\n",
      "Epoch [9/10], Batch [1440/3237], Loss: 0.17390751838684082\n",
      "Epoch [9/10], Batch [1450/3237], Loss: 0.19007574021816254\n",
      "Epoch [9/10], Batch [1460/3237], Loss: 0.09926105290651321\n",
      "Epoch [9/10], Batch [1470/3237], Loss: 0.17074380815029144\n",
      "Epoch [9/10], Batch [1480/3237], Loss: 0.2928769588470459\n",
      "Epoch [9/10], Batch [1490/3237], Loss: 0.06729669123888016\n",
      "Epoch [9/10], Batch [1500/3237], Loss: 0.127984881401062\n",
      "Epoch [9/10], Batch [1510/3237], Loss: 0.2226584553718567\n",
      "Epoch [9/10], Batch [1520/3237], Loss: 0.12566572427749634\n",
      "Epoch [9/10], Batch [1530/3237], Loss: 0.13010917603969574\n",
      "Epoch [9/10], Batch [1540/3237], Loss: 0.3151516318321228\n",
      "Epoch [9/10], Batch [1550/3237], Loss: 0.10131797194480896\n",
      "Epoch [9/10], Batch [1560/3237], Loss: 0.10690885037183762\n",
      "Epoch [9/10], Batch [1570/3237], Loss: 0.05179081857204437\n",
      "Epoch [9/10], Batch [1580/3237], Loss: 0.10266111046075821\n",
      "Epoch [9/10], Batch [1590/3237], Loss: 0.04233941063284874\n",
      "Epoch [9/10], Batch [1600/3237], Loss: 0.15091925859451294\n",
      "Epoch [9/10], Batch [1610/3237], Loss: 0.1485929787158966\n",
      "Epoch [9/10], Batch [1620/3237], Loss: 0.1048930361866951\n",
      "Epoch [9/10], Batch [1630/3237], Loss: 0.21105262637138367\n",
      "Epoch [9/10], Batch [1640/3237], Loss: 0.08180148899555206\n",
      "Epoch [9/10], Batch [1650/3237], Loss: 0.05350670590996742\n",
      "Epoch [9/10], Batch [1660/3237], Loss: 0.14991341531276703\n",
      "Epoch [9/10], Batch [1670/3237], Loss: 0.07521440833806992\n",
      "Epoch [9/10], Batch [1680/3237], Loss: 0.12594600021839142\n",
      "Epoch [9/10], Batch [1690/3237], Loss: 0.15605348348617554\n",
      "Epoch [9/10], Batch [1700/3237], Loss: 0.11437083780765533\n",
      "Epoch [9/10], Batch [1710/3237], Loss: 0.11145129054784775\n",
      "Epoch [9/10], Batch [1720/3237], Loss: 0.20499885082244873\n",
      "Epoch [9/10], Batch [1730/3237], Loss: 0.06634263694286346\n",
      "Epoch [9/10], Batch [1740/3237], Loss: 0.07827241718769073\n",
      "Epoch [9/10], Batch [1750/3237], Loss: 0.07577496021986008\n",
      "Epoch [9/10], Batch [1760/3237], Loss: 0.09714355319738388\n",
      "Epoch [9/10], Batch [1770/3237], Loss: 0.21272611618041992\n",
      "Epoch [9/10], Batch [1780/3237], Loss: 0.17083464562892914\n",
      "Epoch [9/10], Batch [1790/3237], Loss: 0.14450562000274658\n",
      "Epoch [9/10], Batch [1800/3237], Loss: 0.12861688435077667\n",
      "Epoch [9/10], Batch [1810/3237], Loss: 0.19750705361366272\n",
      "Epoch [9/10], Batch [1820/3237], Loss: 0.0844956561923027\n",
      "Epoch [9/10], Batch [1830/3237], Loss: 0.05848545953631401\n",
      "Epoch [9/10], Batch [1840/3237], Loss: 0.07375478744506836\n",
      "Epoch [9/10], Batch [1850/3237], Loss: 0.07482350617647171\n",
      "Epoch [9/10], Batch [1860/3237], Loss: 0.1127389669418335\n",
      "Epoch [9/10], Batch [1870/3237], Loss: 0.19617673754692078\n",
      "Epoch [9/10], Batch [1880/3237], Loss: 0.06890153139829636\n",
      "Epoch [9/10], Batch [1890/3237], Loss: 0.1792697161436081\n",
      "Epoch [9/10], Batch [1900/3237], Loss: 0.08591002225875854\n",
      "Epoch [9/10], Batch [1910/3237], Loss: 0.11105020344257355\n",
      "Epoch [9/10], Batch [1920/3237], Loss: 0.29509052634239197\n",
      "Epoch [9/10], Batch [1930/3237], Loss: 0.10378788411617279\n",
      "Epoch [9/10], Batch [1940/3237], Loss: 0.2712235152721405\n",
      "Epoch [9/10], Batch [1950/3237], Loss: 0.06533438712358475\n",
      "Epoch [9/10], Batch [1960/3237], Loss: 0.11572311073541641\n",
      "Epoch [9/10], Batch [1970/3237], Loss: 0.24145175516605377\n",
      "Epoch [9/10], Batch [1980/3237], Loss: 0.07550488412380219\n",
      "Epoch [9/10], Batch [1990/3237], Loss: 0.2277946025133133\n",
      "Epoch [9/10], Batch [2000/3237], Loss: 0.12612320482730865\n",
      "Epoch [9/10], Batch [2010/3237], Loss: 0.03932144492864609\n",
      "Epoch [9/10], Batch [2020/3237], Loss: 0.13206742703914642\n",
      "Epoch [9/10], Batch [2030/3237], Loss: 0.07766449451446533\n",
      "Epoch [9/10], Batch [2040/3237], Loss: 0.08838804066181183\n",
      "Epoch [9/10], Batch [2050/3237], Loss: 0.09183486551046371\n",
      "Epoch [9/10], Batch [2060/3237], Loss: 0.07722283899784088\n",
      "Epoch [9/10], Batch [2070/3237], Loss: 0.20484565198421478\n",
      "Epoch [9/10], Batch [2080/3237], Loss: 0.23201435804367065\n",
      "Epoch [9/10], Batch [2090/3237], Loss: 0.07889892905950546\n",
      "Epoch [9/10], Batch [2100/3237], Loss: 0.15513794124126434\n",
      "Epoch [9/10], Batch [2110/3237], Loss: 0.12004287540912628\n",
      "Epoch [9/10], Batch [2120/3237], Loss: 0.11431732773780823\n",
      "Epoch [9/10], Batch [2130/3237], Loss: 0.05455857142806053\n",
      "Epoch [9/10], Batch [2140/3237], Loss: 0.11542771756649017\n",
      "Epoch [9/10], Batch [2150/3237], Loss: 0.10771708935499191\n",
      "Epoch [9/10], Batch [2160/3237], Loss: 0.2241937518119812\n",
      "Epoch [9/10], Batch [2170/3237], Loss: 0.15585178136825562\n",
      "Epoch [9/10], Batch [2180/3237], Loss: 0.09778732061386108\n",
      "Epoch [9/10], Batch [2190/3237], Loss: 0.2582010328769684\n",
      "Epoch [9/10], Batch [2200/3237], Loss: 0.10093572735786438\n",
      "Epoch [9/10], Batch [2210/3237], Loss: 0.09453313797712326\n",
      "Epoch [9/10], Batch [2220/3237], Loss: 0.1027902364730835\n",
      "Epoch [9/10], Batch [2230/3237], Loss: 0.1819392293691635\n",
      "Epoch [9/10], Batch [2240/3237], Loss: 0.199144184589386\n",
      "Epoch [9/10], Batch [2250/3237], Loss: 0.0537121556699276\n",
      "Epoch [9/10], Batch [2260/3237], Loss: 0.15004558861255646\n",
      "Epoch [9/10], Batch [2270/3237], Loss: 0.1636444628238678\n",
      "Epoch [9/10], Batch [2280/3237], Loss: 0.11052809655666351\n",
      "Epoch [9/10], Batch [2290/3237], Loss: 0.09570024162530899\n",
      "Epoch [9/10], Batch [2300/3237], Loss: 0.24518710374832153\n",
      "Epoch [9/10], Batch [2310/3237], Loss: 0.06476019322872162\n",
      "Epoch [9/10], Batch [2320/3237], Loss: 0.0772704929113388\n",
      "Epoch [9/10], Batch [2330/3237], Loss: 0.09173277020454407\n",
      "Epoch [9/10], Batch [2340/3237], Loss: 0.13050535321235657\n",
      "Epoch [9/10], Batch [2350/3237], Loss: 0.10813827067613602\n",
      "Epoch [9/10], Batch [2360/3237], Loss: 0.11296378821134567\n",
      "Epoch [9/10], Batch [2370/3237], Loss: 0.12182940542697906\n",
      "Epoch [9/10], Batch [2380/3237], Loss: 0.09661252796649933\n",
      "Epoch [9/10], Batch [2390/3237], Loss: 0.10917919129133224\n",
      "Epoch [9/10], Batch [2400/3237], Loss: 0.11120150238275528\n",
      "Epoch [9/10], Batch [2410/3237], Loss: 0.08063261210918427\n",
      "Epoch [9/10], Batch [2420/3237], Loss: 0.2105671763420105\n",
      "Epoch [9/10], Batch [2430/3237], Loss: 0.21780318021774292\n",
      "Epoch [9/10], Batch [2440/3237], Loss: 0.08370008319616318\n",
      "Epoch [9/10], Batch [2450/3237], Loss: 0.2841736972332001\n",
      "Epoch [9/10], Batch [2460/3237], Loss: 0.059706754982471466\n",
      "Epoch [9/10], Batch [2470/3237], Loss: 0.058096688240766525\n",
      "Epoch [9/10], Batch [2480/3237], Loss: 0.12222839891910553\n",
      "Epoch [9/10], Batch [2490/3237], Loss: 0.1300373077392578\n",
      "Epoch [9/10], Batch [2500/3237], Loss: 0.31317493319511414\n",
      "Epoch [9/10], Batch [2510/3237], Loss: 0.04200464487075806\n",
      "Epoch [9/10], Batch [2520/3237], Loss: 0.2525350749492645\n",
      "Epoch [9/10], Batch [2530/3237], Loss: 0.11958304792642593\n",
      "Epoch [9/10], Batch [2540/3237], Loss: 0.11956784874200821\n",
      "Epoch [9/10], Batch [2550/3237], Loss: 0.454986035823822\n",
      "Epoch [9/10], Batch [2560/3237], Loss: 0.07417413592338562\n",
      "Epoch [9/10], Batch [2570/3237], Loss: 0.1861647218465805\n",
      "Epoch [9/10], Batch [2580/3237], Loss: 0.03961016237735748\n",
      "Epoch [9/10], Batch [2590/3237], Loss: 0.2425864338874817\n",
      "Epoch [9/10], Batch [2600/3237], Loss: 0.1590810865163803\n",
      "Epoch [9/10], Batch [2610/3237], Loss: 0.12697967886924744\n",
      "Epoch [9/10], Batch [2620/3237], Loss: 0.14970795810222626\n",
      "Epoch [9/10], Batch [2630/3237], Loss: 0.12539531290531158\n",
      "Epoch [9/10], Batch [2640/3237], Loss: 0.15021857619285583\n",
      "Epoch [9/10], Batch [2650/3237], Loss: 0.08609257638454437\n",
      "Epoch [9/10], Batch [2660/3237], Loss: 0.10178370028734207\n",
      "Epoch [9/10], Batch [2670/3237], Loss: 0.13752219080924988\n",
      "Epoch [9/10], Batch [2680/3237], Loss: 0.17382903397083282\n",
      "Epoch [9/10], Batch [2690/3237], Loss: 0.07981236279010773\n",
      "Epoch [9/10], Batch [2700/3237], Loss: 0.17927497625350952\n",
      "Epoch [9/10], Batch [2710/3237], Loss: 0.0787430927157402\n",
      "Epoch [9/10], Batch [2720/3237], Loss: 0.36274638772010803\n",
      "Epoch [9/10], Batch [2730/3237], Loss: 0.07678871601819992\n",
      "Epoch [9/10], Batch [2740/3237], Loss: 0.16851888597011566\n",
      "Epoch [9/10], Batch [2750/3237], Loss: 0.13067975640296936\n",
      "Epoch [9/10], Batch [2760/3237], Loss: 0.20177432894706726\n",
      "Epoch [9/10], Batch [2770/3237], Loss: 0.09717883914709091\n",
      "Epoch [9/10], Batch [2780/3237], Loss: 0.07928714156150818\n",
      "Epoch [9/10], Batch [2790/3237], Loss: 0.06531249731779099\n",
      "Epoch [9/10], Batch [2800/3237], Loss: 0.07647619396448135\n",
      "Epoch [9/10], Batch [2810/3237], Loss: 0.10402272641658783\n",
      "Epoch [9/10], Batch [2820/3237], Loss: 0.08555787801742554\n",
      "Epoch [9/10], Batch [2830/3237], Loss: 0.12174852192401886\n",
      "Epoch [9/10], Batch [2840/3237], Loss: 0.14040425419807434\n",
      "Epoch [9/10], Batch [2850/3237], Loss: 0.20368725061416626\n",
      "Epoch [9/10], Batch [2860/3237], Loss: 0.04803274944424629\n",
      "Epoch [9/10], Batch [2870/3237], Loss: 0.10719369351863861\n",
      "Epoch [9/10], Batch [2880/3237], Loss: 0.1676715761423111\n",
      "Epoch [9/10], Batch [2890/3237], Loss: 0.04140082374215126\n",
      "Epoch [9/10], Batch [2900/3237], Loss: 0.06455214321613312\n",
      "Epoch [9/10], Batch [2910/3237], Loss: 0.2824800908565521\n",
      "Epoch [9/10], Batch [2920/3237], Loss: 0.09946676343679428\n",
      "Epoch [9/10], Batch [2930/3237], Loss: 0.19554397463798523\n",
      "Epoch [9/10], Batch [2940/3237], Loss: 0.07968029379844666\n",
      "Epoch [9/10], Batch [2950/3237], Loss: 0.14039912819862366\n",
      "Epoch [9/10], Batch [2960/3237], Loss: 0.10900825262069702\n",
      "Epoch [9/10], Batch [2970/3237], Loss: 0.10723628848791122\n",
      "Epoch [9/10], Batch [2980/3237], Loss: 0.3783923387527466\n",
      "Epoch [9/10], Batch [2990/3237], Loss: 0.08923351764678955\n",
      "Epoch [9/10], Batch [3000/3237], Loss: 0.1375284045934677\n",
      "Epoch [9/10], Batch [3010/3237], Loss: 0.0827760174870491\n",
      "Epoch [9/10], Batch [3020/3237], Loss: 0.2525259852409363\n",
      "Epoch [9/10], Batch [3030/3237], Loss: 0.09174800664186478\n",
      "Epoch [9/10], Batch [3040/3237], Loss: 0.07480496913194656\n",
      "Epoch [9/10], Batch [3050/3237], Loss: 0.09761613607406616\n",
      "Epoch [9/10], Batch [3060/3237], Loss: 0.14442752301692963\n",
      "Epoch [9/10], Batch [3070/3237], Loss: 0.15933941304683685\n",
      "Epoch [9/10], Batch [3080/3237], Loss: 0.06813547760248184\n",
      "Epoch [9/10], Batch [3090/3237], Loss: 0.2420923113822937\n",
      "Epoch [9/10], Batch [3100/3237], Loss: 0.13731607794761658\n",
      "Epoch [9/10], Batch [3110/3237], Loss: 0.12891247868537903\n",
      "Epoch [9/10], Batch [3120/3237], Loss: 0.14906024932861328\n",
      "Epoch [9/10], Batch [3130/3237], Loss: 0.28988754749298096\n",
      "Epoch [9/10], Batch [3140/3237], Loss: 0.15216432511806488\n",
      "Epoch [9/10], Batch [3150/3237], Loss: 0.17714987695217133\n",
      "Epoch [9/10], Batch [3160/3237], Loss: 0.13118277490139008\n",
      "Epoch [9/10], Batch [3170/3237], Loss: 0.11926134675741196\n",
      "Epoch [9/10], Batch [3180/3237], Loss: 0.057863831520080566\n",
      "Epoch [9/10], Batch [3190/3237], Loss: 0.2508750855922699\n",
      "Epoch [9/10], Batch [3200/3237], Loss: 0.08075112104415894\n",
      "Epoch [9/10], Batch [3210/3237], Loss: 0.055305082350969315\n",
      "Epoch [9/10], Batch [3220/3237], Loss: 0.06936124712228775\n",
      "Epoch [9/10], Batch [3230/3237], Loss: 0.09642987698316574\n",
      "Epoch 9, Average Train Loss: 0.1406, Average Val Loss: 0.1696\n",
      "Epoch [10/10], Batch [10/3237], Loss: 0.12878094613552094\n",
      "Epoch [10/10], Batch [20/3237], Loss: 0.12882466614246368\n",
      "Epoch [10/10], Batch [30/3237], Loss: 0.10610583424568176\n",
      "Epoch [10/10], Batch [40/3237], Loss: 0.13217543065547943\n",
      "Epoch [10/10], Batch [50/3237], Loss: 0.09497997164726257\n",
      "Epoch [10/10], Batch [60/3237], Loss: 0.07478106021881104\n",
      "Epoch [10/10], Batch [70/3237], Loss: 0.046356551349163055\n",
      "Epoch [10/10], Batch [80/3237], Loss: 0.21136906743049622\n",
      "Epoch [10/10], Batch [90/3237], Loss: 0.13045991957187653\n",
      "Epoch [10/10], Batch [100/3237], Loss: 0.06033619865775108\n",
      "Epoch [10/10], Batch [110/3237], Loss: 0.08236728608608246\n",
      "Epoch [10/10], Batch [120/3237], Loss: 0.1935151070356369\n",
      "Epoch [10/10], Batch [130/3237], Loss: 0.15337766706943512\n",
      "Epoch [10/10], Batch [140/3237], Loss: 0.10711392760276794\n",
      "Epoch [10/10], Batch [150/3237], Loss: 0.09369533509016037\n",
      "Epoch [10/10], Batch [160/3237], Loss: 0.11765270680189133\n",
      "Epoch [10/10], Batch [170/3237], Loss: 0.15752927958965302\n",
      "Epoch [10/10], Batch [180/3237], Loss: 0.14988017082214355\n",
      "Epoch [10/10], Batch [190/3237], Loss: 0.17673686146736145\n",
      "Epoch [10/10], Batch [200/3237], Loss: 0.09335236996412277\n",
      "Epoch [10/10], Batch [210/3237], Loss: 0.12799029052257538\n",
      "Epoch [10/10], Batch [220/3237], Loss: 0.0840374082326889\n",
      "Epoch [10/10], Batch [230/3237], Loss: 0.09798935800790787\n",
      "Epoch [10/10], Batch [240/3237], Loss: 0.12009266763925552\n",
      "Epoch [10/10], Batch [250/3237], Loss: 0.18153491616249084\n",
      "Epoch [10/10], Batch [260/3237], Loss: 0.19121932983398438\n",
      "Epoch [10/10], Batch [270/3237], Loss: 0.06207887828350067\n",
      "Epoch [10/10], Batch [280/3237], Loss: 0.15403804183006287\n",
      "Epoch [10/10], Batch [290/3237], Loss: 0.17016936838626862\n",
      "Epoch [10/10], Batch [300/3237], Loss: 0.14806421101093292\n",
      "Epoch [10/10], Batch [310/3237], Loss: 0.066848985850811\n",
      "Epoch [10/10], Batch [320/3237], Loss: 0.1772277057170868\n",
      "Epoch [10/10], Batch [330/3237], Loss: 0.1356634944677353\n",
      "Epoch [10/10], Batch [340/3237], Loss: 0.1954531967639923\n",
      "Epoch [10/10], Batch [350/3237], Loss: 0.06322502344846725\n",
      "Epoch [10/10], Batch [360/3237], Loss: 0.11736281216144562\n",
      "Epoch [10/10], Batch [370/3237], Loss: 0.18710891902446747\n",
      "Epoch [10/10], Batch [380/3237], Loss: 0.07441144436597824\n",
      "Epoch [10/10], Batch [390/3237], Loss: 0.133372500538826\n",
      "Epoch [10/10], Batch [400/3237], Loss: 0.17966991662979126\n",
      "Epoch [10/10], Batch [410/3237], Loss: 0.05657017603516579\n",
      "Epoch [10/10], Batch [420/3237], Loss: 0.14666612446308136\n",
      "Epoch [10/10], Batch [430/3237], Loss: 0.08908708393573761\n",
      "Epoch [10/10], Batch [440/3237], Loss: 0.07937578856945038\n",
      "Epoch [10/10], Batch [450/3237], Loss: 0.09764780849218369\n",
      "Epoch [10/10], Batch [460/3237], Loss: 0.06472711265087128\n",
      "Epoch [10/10], Batch [470/3237], Loss: 0.1726551353931427\n",
      "Epoch [10/10], Batch [480/3237], Loss: 0.053191956132650375\n",
      "Epoch [10/10], Batch [490/3237], Loss: 0.11155752092599869\n",
      "Epoch [10/10], Batch [500/3237], Loss: 0.09662605822086334\n",
      "Epoch [10/10], Batch [510/3237], Loss: 0.15091858804225922\n",
      "Epoch [10/10], Batch [520/3237], Loss: 0.0797717273235321\n",
      "Epoch [10/10], Batch [530/3237], Loss: 0.17225804924964905\n",
      "Epoch [10/10], Batch [540/3237], Loss: 0.21637217700481415\n",
      "Epoch [10/10], Batch [550/3237], Loss: 0.09092947095632553\n",
      "Epoch [10/10], Batch [560/3237], Loss: 0.14478576183319092\n",
      "Epoch [10/10], Batch [570/3237], Loss: 0.12534339725971222\n",
      "Epoch [10/10], Batch [580/3237], Loss: 0.10955492407083511\n",
      "Epoch [10/10], Batch [590/3237], Loss: 0.11619455367326736\n",
      "Epoch [10/10], Batch [600/3237], Loss: 0.27801233530044556\n",
      "Epoch [10/10], Batch [610/3237], Loss: 0.08995898067951202\n",
      "Epoch [10/10], Batch [620/3237], Loss: 0.06378304958343506\n",
      "Epoch [10/10], Batch [630/3237], Loss: 0.09223631024360657\n",
      "Epoch [10/10], Batch [640/3237], Loss: 0.07308216392993927\n",
      "Epoch [10/10], Batch [650/3237], Loss: 0.1485372930765152\n",
      "Epoch [10/10], Batch [660/3237], Loss: 0.10792282968759537\n",
      "Epoch [10/10], Batch [670/3237], Loss: 0.055651500821113586\n",
      "Epoch [10/10], Batch [680/3237], Loss: 0.06753425300121307\n",
      "Epoch [10/10], Batch [690/3237], Loss: 0.053771454840898514\n",
      "Epoch [10/10], Batch [700/3237], Loss: 0.12362794578075409\n",
      "Epoch [10/10], Batch [710/3237], Loss: 0.18003930151462555\n",
      "Epoch [10/10], Batch [720/3237], Loss: 0.06407836824655533\n",
      "Epoch [10/10], Batch [730/3237], Loss: 0.15511569380760193\n",
      "Epoch [10/10], Batch [740/3237], Loss: 0.1269758939743042\n",
      "Epoch [10/10], Batch [750/3237], Loss: 0.05743396282196045\n",
      "Epoch [10/10], Batch [760/3237], Loss: 0.04128648713231087\n",
      "Epoch [10/10], Batch [770/3237], Loss: 0.10277530550956726\n",
      "Epoch [10/10], Batch [780/3237], Loss: 0.10845467448234558\n",
      "Epoch [10/10], Batch [790/3237], Loss: 0.1880892962217331\n",
      "Epoch [10/10], Batch [800/3237], Loss: 0.0830061286687851\n",
      "Epoch [10/10], Batch [810/3237], Loss: 0.25398704409599304\n",
      "Epoch [10/10], Batch [820/3237], Loss: 0.06631961464881897\n",
      "Epoch [10/10], Batch [830/3237], Loss: 0.11262785643339157\n",
      "Epoch [10/10], Batch [840/3237], Loss: 0.14565183222293854\n",
      "Epoch [10/10], Batch [850/3237], Loss: 0.1941748410463333\n",
      "Epoch [10/10], Batch [860/3237], Loss: 0.14512239396572113\n",
      "Epoch [10/10], Batch [870/3237], Loss: 0.08204331994056702\n",
      "Epoch [10/10], Batch [880/3237], Loss: 0.052549343556165695\n",
      "Epoch [10/10], Batch [890/3237], Loss: 0.11857744306325912\n",
      "Epoch [10/10], Batch [900/3237], Loss: 0.17551957070827484\n",
      "Epoch [10/10], Batch [910/3237], Loss: 0.06500020623207092\n",
      "Epoch [10/10], Batch [920/3237], Loss: 0.08116430044174194\n",
      "Epoch [10/10], Batch [930/3237], Loss: 0.1224011704325676\n",
      "Epoch [10/10], Batch [940/3237], Loss: 0.09394339472055435\n",
      "Epoch [10/10], Batch [950/3237], Loss: 0.13122953474521637\n",
      "Epoch [10/10], Batch [960/3237], Loss: 0.11604253202676773\n",
      "Epoch [10/10], Batch [970/3237], Loss: 0.11166747659444809\n",
      "Epoch [10/10], Batch [980/3237], Loss: 0.07017343491315842\n",
      "Epoch [10/10], Batch [990/3237], Loss: 0.2785971462726593\n",
      "Epoch [10/10], Batch [1000/3237], Loss: 0.06768553704023361\n",
      "Epoch [10/10], Batch [1010/3237], Loss: 0.1376366764307022\n",
      "Epoch [10/10], Batch [1020/3237], Loss: 0.14969439804553986\n",
      "Epoch [10/10], Batch [1030/3237], Loss: 0.09108060598373413\n",
      "Epoch [10/10], Batch [1040/3237], Loss: 0.09892603009939194\n",
      "Epoch [10/10], Batch [1050/3237], Loss: 0.05578409880399704\n",
      "Epoch [10/10], Batch [1060/3237], Loss: 0.16256557404994965\n",
      "Epoch [10/10], Batch [1070/3237], Loss: 0.10149192810058594\n",
      "Epoch [10/10], Batch [1080/3237], Loss: 0.12999650835990906\n",
      "Epoch [10/10], Batch [1090/3237], Loss: 0.10655900835990906\n",
      "Epoch [10/10], Batch [1100/3237], Loss: 0.1997731477022171\n",
      "Epoch [10/10], Batch [1110/3237], Loss: 0.16455714404582977\n",
      "Epoch [10/10], Batch [1120/3237], Loss: 0.1224939152598381\n",
      "Epoch [10/10], Batch [1130/3237], Loss: 0.1419391930103302\n",
      "Epoch [10/10], Batch [1140/3237], Loss: 0.09138164669275284\n",
      "Epoch [10/10], Batch [1150/3237], Loss: 0.10036806017160416\n",
      "Epoch [10/10], Batch [1160/3237], Loss: 0.2647400498390198\n",
      "Epoch [10/10], Batch [1170/3237], Loss: 0.11541286110877991\n",
      "Epoch [10/10], Batch [1180/3237], Loss: 0.048354022204875946\n",
      "Epoch [10/10], Batch [1190/3237], Loss: 0.1195574626326561\n",
      "Epoch [10/10], Batch [1200/3237], Loss: 0.09854018688201904\n",
      "Epoch [10/10], Batch [1210/3237], Loss: 0.1291598230600357\n",
      "Epoch [10/10], Batch [1220/3237], Loss: 0.07125167548656464\n",
      "Epoch [10/10], Batch [1230/3237], Loss: 0.04726570099592209\n",
      "Epoch [10/10], Batch [1240/3237], Loss: 0.13685089349746704\n",
      "Epoch [10/10], Batch [1250/3237], Loss: 0.18084120750427246\n",
      "Epoch [10/10], Batch [1260/3237], Loss: 0.15301023423671722\n",
      "Epoch [10/10], Batch [1270/3237], Loss: 0.16989287734031677\n",
      "Epoch [10/10], Batch [1280/3237], Loss: 0.07035095244646072\n",
      "Epoch [10/10], Batch [1290/3237], Loss: 0.08988016098737717\n",
      "Epoch [10/10], Batch [1300/3237], Loss: 0.14473384618759155\n",
      "Epoch [10/10], Batch [1310/3237], Loss: 0.0492137148976326\n",
      "Epoch [10/10], Batch [1320/3237], Loss: 0.12712548673152924\n",
      "Epoch [10/10], Batch [1330/3237], Loss: 0.15505743026733398\n",
      "Epoch [10/10], Batch [1340/3237], Loss: 0.04162119701504707\n",
      "Epoch [10/10], Batch [1350/3237], Loss: 0.3371305763721466\n",
      "Epoch [10/10], Batch [1360/3237], Loss: 0.11740606278181076\n",
      "Epoch [10/10], Batch [1370/3237], Loss: 0.2781639099121094\n",
      "Epoch [10/10], Batch [1380/3237], Loss: 0.11811145395040512\n",
      "Epoch [10/10], Batch [1390/3237], Loss: 0.16957078874111176\n",
      "Epoch [10/10], Batch [1400/3237], Loss: 0.1174364909529686\n",
      "Epoch [10/10], Batch [1410/3237], Loss: 0.21520881354808807\n",
      "Epoch [10/10], Batch [1420/3237], Loss: 0.04754463583230972\n",
      "Epoch [10/10], Batch [1430/3237], Loss: 0.13990916311740875\n",
      "Epoch [10/10], Batch [1440/3237], Loss: 0.22015804052352905\n",
      "Epoch [10/10], Batch [1450/3237], Loss: 0.18759536743164062\n",
      "Epoch [10/10], Batch [1460/3237], Loss: 0.15861102938652039\n",
      "Epoch [10/10], Batch [1470/3237], Loss: 0.15416283905506134\n",
      "Epoch [10/10], Batch [1480/3237], Loss: 0.17647893726825714\n",
      "Epoch [10/10], Batch [1490/3237], Loss: 0.08571989834308624\n",
      "Epoch [10/10], Batch [1500/3237], Loss: 0.17559586465358734\n",
      "Epoch [10/10], Batch [1510/3237], Loss: 0.11063630878925323\n",
      "Epoch [10/10], Batch [1520/3237], Loss: 0.1043793261051178\n",
      "Epoch [10/10], Batch [1530/3237], Loss: 0.1024809256196022\n",
      "Epoch [10/10], Batch [1540/3237], Loss: 0.08652480691671371\n",
      "Epoch [10/10], Batch [1550/3237], Loss: 0.06655029952526093\n",
      "Epoch [10/10], Batch [1560/3237], Loss: 0.1134878620505333\n",
      "Epoch [10/10], Batch [1570/3237], Loss: 0.10512157529592514\n",
      "Epoch [10/10], Batch [1580/3237], Loss: 0.21727986633777618\n",
      "Epoch [10/10], Batch [1590/3237], Loss: 0.19232261180877686\n",
      "Epoch [10/10], Batch [1600/3237], Loss: 0.04039613530039787\n",
      "Epoch [10/10], Batch [1610/3237], Loss: 0.11297237128019333\n",
      "Epoch [10/10], Batch [1620/3237], Loss: 0.20458637177944183\n",
      "Epoch [10/10], Batch [1630/3237], Loss: 0.061419013887643814\n",
      "Epoch [10/10], Batch [1640/3237], Loss: 0.056677158921957016\n",
      "Epoch [10/10], Batch [1650/3237], Loss: 0.0857028216123581\n",
      "Epoch [10/10], Batch [1660/3237], Loss: 0.1813824474811554\n",
      "Epoch [10/10], Batch [1670/3237], Loss: 0.06789802759885788\n",
      "Epoch [10/10], Batch [1680/3237], Loss: 0.35592153668403625\n",
      "Epoch [10/10], Batch [1690/3237], Loss: 0.23974870145320892\n",
      "Epoch [10/10], Batch [1700/3237], Loss: 0.04439161345362663\n",
      "Epoch [10/10], Batch [1710/3237], Loss: 0.2838830053806305\n",
      "Epoch [10/10], Batch [1720/3237], Loss: 0.15606094896793365\n",
      "Epoch [10/10], Batch [1730/3237], Loss: 0.08682489395141602\n",
      "Epoch [10/10], Batch [1740/3237], Loss: 0.14573055505752563\n",
      "Epoch [10/10], Batch [1750/3237], Loss: 0.031568534672260284\n",
      "Epoch [10/10], Batch [1760/3237], Loss: 0.09632909297943115\n",
      "Epoch [10/10], Batch [1770/3237], Loss: 0.08449378609657288\n",
      "Epoch [10/10], Batch [1780/3237], Loss: 0.05022076889872551\n",
      "Epoch [10/10], Batch [1790/3237], Loss: 0.15234598517417908\n",
      "Epoch [10/10], Batch [1800/3237], Loss: 0.08261797577142715\n",
      "Epoch [10/10], Batch [1810/3237], Loss: 0.07349765300750732\n",
      "Epoch [10/10], Batch [1820/3237], Loss: 0.09533393383026123\n",
      "Epoch [10/10], Batch [1830/3237], Loss: 0.08145450055599213\n",
      "Epoch [10/10], Batch [1840/3237], Loss: 0.08230138570070267\n",
      "Epoch [10/10], Batch [1850/3237], Loss: 0.12383802235126495\n",
      "Epoch [10/10], Batch [1860/3237], Loss: 0.08941733837127686\n",
      "Epoch [10/10], Batch [1870/3237], Loss: 0.12750187516212463\n",
      "Epoch [10/10], Batch [1880/3237], Loss: 0.08042431622743607\n",
      "Epoch [10/10], Batch [1890/3237], Loss: 0.042817458510398865\n",
      "Epoch [10/10], Batch [1900/3237], Loss: 0.15662571787834167\n",
      "Epoch [10/10], Batch [1910/3237], Loss: 0.054054975509643555\n",
      "Epoch [10/10], Batch [1920/3237], Loss: 0.17324768006801605\n",
      "Epoch [10/10], Batch [1930/3237], Loss: 0.05447181686758995\n",
      "Epoch [10/10], Batch [1940/3237], Loss: 0.13356193900108337\n",
      "Epoch [10/10], Batch [1950/3237], Loss: 0.3315099775791168\n",
      "Epoch [10/10], Batch [1960/3237], Loss: 0.19018729031085968\n",
      "Epoch [10/10], Batch [1970/3237], Loss: 0.09218740463256836\n",
      "Epoch [10/10], Batch [1980/3237], Loss: 0.13880424201488495\n",
      "Epoch [10/10], Batch [1990/3237], Loss: 0.12251489609479904\n",
      "Epoch [10/10], Batch [2000/3237], Loss: 0.1547400802373886\n",
      "Epoch [10/10], Batch [2010/3237], Loss: 0.3703475594520569\n",
      "Epoch [10/10], Batch [2020/3237], Loss: 0.12093834578990936\n",
      "Epoch [10/10], Batch [2030/3237], Loss: 0.2352522611618042\n",
      "Epoch [10/10], Batch [2040/3237], Loss: 0.14287069439888\n",
      "Epoch [10/10], Batch [2050/3237], Loss: 0.12855938076972961\n",
      "Epoch [10/10], Batch [2060/3237], Loss: 0.08250968903303146\n",
      "Epoch [10/10], Batch [2070/3237], Loss: 0.06130560114979744\n",
      "Epoch [10/10], Batch [2080/3237], Loss: 0.16645975410938263\n",
      "Epoch [10/10], Batch [2090/3237], Loss: 0.12149715423583984\n",
      "Epoch [10/10], Batch [2100/3237], Loss: 0.14861518144607544\n",
      "Epoch [10/10], Batch [2110/3237], Loss: 0.109405517578125\n",
      "Epoch [10/10], Batch [2120/3237], Loss: 0.1248445138335228\n",
      "Epoch [10/10], Batch [2130/3237], Loss: 0.24602362513542175\n",
      "Epoch [10/10], Batch [2140/3237], Loss: 0.24272990226745605\n",
      "Epoch [10/10], Batch [2150/3237], Loss: 0.09641686826944351\n",
      "Epoch [10/10], Batch [2160/3237], Loss: 0.09424421191215515\n",
      "Epoch [10/10], Batch [2170/3237], Loss: 0.15762893855571747\n",
      "Epoch [10/10], Batch [2180/3237], Loss: 0.2094837874174118\n",
      "Epoch [10/10], Batch [2190/3237], Loss: 0.08866427093744278\n",
      "Epoch [10/10], Batch [2200/3237], Loss: 0.07600909471511841\n",
      "Epoch [10/10], Batch [2210/3237], Loss: 0.0821371078491211\n",
      "Epoch [10/10], Batch [2220/3237], Loss: 0.057509858161211014\n",
      "Epoch [10/10], Batch [2230/3237], Loss: 0.11508695036172867\n",
      "Epoch [10/10], Batch [2240/3237], Loss: 0.068655826151371\n",
      "Epoch [10/10], Batch [2250/3237], Loss: 0.06742450594902039\n",
      "Epoch [10/10], Batch [2260/3237], Loss: 0.12489073723554611\n",
      "Epoch [10/10], Batch [2270/3237], Loss: 0.22092948853969574\n",
      "Epoch [10/10], Batch [2280/3237], Loss: 0.1822415441274643\n",
      "Epoch [10/10], Batch [2290/3237], Loss: 0.04488413780927658\n",
      "Epoch [10/10], Batch [2300/3237], Loss: 0.3056331276893616\n",
      "Epoch [10/10], Batch [2310/3237], Loss: 0.06960385292768478\n",
      "Epoch [10/10], Batch [2320/3237], Loss: 0.13793492317199707\n",
      "Epoch [10/10], Batch [2330/3237], Loss: 0.22900502383708954\n",
      "Epoch [10/10], Batch [2340/3237], Loss: 0.26301729679107666\n",
      "Epoch [10/10], Batch [2350/3237], Loss: 0.11810857802629471\n",
      "Epoch [10/10], Batch [2360/3237], Loss: 0.1305817812681198\n",
      "Epoch [10/10], Batch [2370/3237], Loss: 0.13374826312065125\n",
      "Epoch [10/10], Batch [2380/3237], Loss: 0.07888159900903702\n",
      "Epoch [10/10], Batch [2390/3237], Loss: 0.140323668718338\n",
      "Epoch [10/10], Batch [2400/3237], Loss: 0.13695316016674042\n",
      "Epoch [10/10], Batch [2410/3237], Loss: 0.15136221051216125\n",
      "Epoch [10/10], Batch [2420/3237], Loss: 0.1483391523361206\n",
      "Epoch [10/10], Batch [2430/3237], Loss: 0.044620636850595474\n",
      "Epoch [10/10], Batch [2440/3237], Loss: 0.2602526545524597\n",
      "Epoch [10/10], Batch [2450/3237], Loss: 0.1899917721748352\n",
      "Epoch [10/10], Batch [2460/3237], Loss: 0.11186599731445312\n",
      "Epoch [10/10], Batch [2470/3237], Loss: 0.060137342661619186\n",
      "Epoch [10/10], Batch [2480/3237], Loss: 0.13405534625053406\n",
      "Epoch [10/10], Batch [2490/3237], Loss: 0.1149570569396019\n",
      "Epoch [10/10], Batch [2500/3237], Loss: 0.14876143634319305\n",
      "Epoch [10/10], Batch [2510/3237], Loss: 0.046751320362091064\n",
      "Epoch [10/10], Batch [2520/3237], Loss: 0.10416364669799805\n",
      "Epoch [10/10], Batch [2530/3237], Loss: 0.07933956384658813\n",
      "Epoch [10/10], Batch [2540/3237], Loss: 0.14533397555351257\n",
      "Epoch [10/10], Batch [2550/3237], Loss: 0.10484704375267029\n",
      "Epoch [10/10], Batch [2560/3237], Loss: 0.1874765157699585\n",
      "Epoch [10/10], Batch [2570/3237], Loss: 0.19391509890556335\n",
      "Epoch [10/10], Batch [2580/3237], Loss: 0.07190851867198944\n",
      "Epoch [10/10], Batch [2590/3237], Loss: 0.21153034269809723\n",
      "Epoch [10/10], Batch [2600/3237], Loss: 0.10087711364030838\n",
      "Epoch [10/10], Batch [2610/3237], Loss: 0.2518588900566101\n",
      "Epoch [10/10], Batch [2620/3237], Loss: 0.15649080276489258\n",
      "Epoch [10/10], Batch [2630/3237], Loss: 0.3846624791622162\n",
      "Epoch [10/10], Batch [2640/3237], Loss: 0.09064044803380966\n",
      "Epoch [10/10], Batch [2650/3237], Loss: 0.08615721762180328\n",
      "Epoch [10/10], Batch [2660/3237], Loss: 0.1392063945531845\n",
      "Epoch [10/10], Batch [2670/3237], Loss: 0.04534359276294708\n",
      "Epoch [10/10], Batch [2680/3237], Loss: 0.05140010640025139\n",
      "Epoch [10/10], Batch [2690/3237], Loss: 0.2508845925331116\n",
      "Epoch [10/10], Batch [2700/3237], Loss: 0.07292022556066513\n",
      "Epoch [10/10], Batch [2710/3237], Loss: 0.20708629488945007\n",
      "Epoch [10/10], Batch [2720/3237], Loss: 0.10467708110809326\n",
      "Epoch [10/10], Batch [2730/3237], Loss: 0.1749534010887146\n",
      "Epoch [10/10], Batch [2740/3237], Loss: 0.14744266867637634\n",
      "Epoch [10/10], Batch [2750/3237], Loss: 0.17112840712070465\n",
      "Epoch [10/10], Batch [2760/3237], Loss: 0.08585931360721588\n",
      "Epoch [10/10], Batch [2770/3237], Loss: 0.10408271849155426\n",
      "Epoch [10/10], Batch [2780/3237], Loss: 0.08109897375106812\n",
      "Epoch [10/10], Batch [2790/3237], Loss: 0.10780245810747147\n",
      "Epoch [10/10], Batch [2800/3237], Loss: 0.2294621616601944\n",
      "Epoch [10/10], Batch [2810/3237], Loss: 0.08482201397418976\n",
      "Epoch [10/10], Batch [2820/3237], Loss: 0.2339056134223938\n",
      "Epoch [10/10], Batch [2830/3237], Loss: 0.08052907884120941\n",
      "Epoch [10/10], Batch [2840/3237], Loss: 0.10691571980714798\n",
      "Epoch [10/10], Batch [2850/3237], Loss: 0.14949685335159302\n",
      "Epoch [10/10], Batch [2860/3237], Loss: 0.1325107216835022\n",
      "Epoch [10/10], Batch [2870/3237], Loss: 0.22829420864582062\n",
      "Epoch [10/10], Batch [2880/3237], Loss: 0.19389492273330688\n",
      "Epoch [10/10], Batch [2890/3237], Loss: 0.12924300134181976\n",
      "Epoch [10/10], Batch [2900/3237], Loss: 0.06233232468366623\n",
      "Epoch [10/10], Batch [2910/3237], Loss: 0.09840098768472672\n",
      "Epoch [10/10], Batch [2920/3237], Loss: 0.10065242648124695\n",
      "Epoch [10/10], Batch [2930/3237], Loss: 0.3373872935771942\n",
      "Epoch [10/10], Batch [2940/3237], Loss: 0.404916912317276\n",
      "Epoch [10/10], Batch [2950/3237], Loss: 0.13335390388965607\n",
      "Epoch [10/10], Batch [2960/3237], Loss: 0.1365918666124344\n",
      "Epoch [10/10], Batch [2970/3237], Loss: 0.1033811867237091\n",
      "Epoch [10/10], Batch [2980/3237], Loss: 0.1663474440574646\n",
      "Epoch [10/10], Batch [2990/3237], Loss: 0.13172027468681335\n",
      "Epoch [10/10], Batch [3000/3237], Loss: 0.24601051211357117\n",
      "Epoch [10/10], Batch [3010/3237], Loss: 0.10154712200164795\n",
      "Epoch [10/10], Batch [3020/3237], Loss: 0.1951170265674591\n",
      "Epoch [10/10], Batch [3030/3237], Loss: 0.07651172578334808\n",
      "Epoch [10/10], Batch [3040/3237], Loss: 0.03444235771894455\n",
      "Epoch [10/10], Batch [3050/3237], Loss: 0.1809922307729721\n",
      "Epoch [10/10], Batch [3060/3237], Loss: 0.07820148020982742\n",
      "Epoch [10/10], Batch [3070/3237], Loss: 0.20888350903987885\n",
      "Epoch [10/10], Batch [3080/3237], Loss: 0.05806942656636238\n",
      "Epoch [10/10], Batch [3090/3237], Loss: 0.14902959764003754\n",
      "Epoch [10/10], Batch [3100/3237], Loss: 0.07869405299425125\n",
      "Epoch [10/10], Batch [3110/3237], Loss: 0.22278162837028503\n",
      "Epoch [10/10], Batch [3120/3237], Loss: 0.0783781111240387\n",
      "Epoch [10/10], Batch [3130/3237], Loss: 0.15340273082256317\n",
      "Epoch [10/10], Batch [3140/3237], Loss: 0.17956633865833282\n",
      "Epoch [10/10], Batch [3150/3237], Loss: 0.10949470102787018\n",
      "Epoch [10/10], Batch [3160/3237], Loss: 0.2924518883228302\n",
      "Epoch [10/10], Batch [3170/3237], Loss: 0.1379546970129013\n",
      "Epoch [10/10], Batch [3180/3237], Loss: 0.3473142087459564\n",
      "Epoch [10/10], Batch [3190/3237], Loss: 0.14101018011569977\n",
      "Epoch [10/10], Batch [3200/3237], Loss: 0.1415507048368454\n",
      "Epoch [10/10], Batch [3210/3237], Loss: 0.050263307988643646\n",
      "Epoch [10/10], Batch [3220/3237], Loss: 0.26336610317230225\n",
      "Epoch [10/10], Batch [3230/3237], Loss: 0.1431165337562561\n",
      "Epoch 10, Average Train Loss: 0.1339, Average Val Loss: 0.1698\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming you have a directory with images and their corresponding masks\n",
    "images_dir='//home//lic9//prj//test//Dataset//complete_original'\n",
    "masks_dir='//home//lic9//prj//test//Dataset//complete_alphamap'\n",
    "\n",
    "# List image and mask files\n",
    "image_files = sorted(glob.glob(os.path.join(images_dir, \"*.jpg\")))\n",
    "mask_files = sorted(glob.glob(os.path.join(masks_dir, \"*.png\")))\n",
    "\n",
    "# Use this function to filter out unmatched files\n",
    "matched_image_files, matched_mask_files = get_matching_files(images_dir, masks_dir)\n",
    "\n",
    "# Now you can proceed with splitting your data, setting up your dataset and dataloader as before\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(matched_image_files, matched_mask_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = SegmentationDataset(train_images, train_masks, transform=transform)\n",
    "val_dataset = SegmentationDataset(val_images, val_masks, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# 加载和准备模型\n",
    "model = load_pretrained_model().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Call the training function\n",
    "# Note: Uncomment the next line to train the model. Make sure you have defined the paths and the dataset correctly.\n",
    "train_and_evaluate_with_saving(model, train_loader, val_loader, optimizer, criterion, device, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image1 = '/Users/tanqiwen/Desktop/test/Dataset/complete_original/GT04.png'\n",
    "alpha_map = inference_and_get_alphamap(image1, device)  # 获取Alpha Map\n",
    "\n",
    "# 指定Alpha Map的保存路径\n",
    "saved_alpha_map_path = '/Users/tanqiwen/Desktop/test/alpha_map.png'\n",
    "save_alpha_map(alpha_map, saved_alpha_map_path)  # 保存Alpha Map\n",
    "\n",
    "# 然后你可以使用IPython.display来显示保存的图像，如之前所示\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=saved_alpha_map_path))\n",
    "display(Image(filename=image1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
